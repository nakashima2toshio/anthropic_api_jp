{
  "Intro": "OpenAI Agents SDK\nThe\nOpenAI Agents SDK\nenables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents,\nSwarm\n. The Agents SDK has a very small set of primitives:\nAgents\n, which are LLMs equipped with instructions and tools\nHandoffs\n, which allow agents to delegate to other agents for specific tasks\nGuardrails\n, which enable the inputs to agents to be validated\nIn combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in\ntracing\nthat lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.\nWhy use the Agents SDK\nThe SDK has two driving design principles:\nEnough features to be worth using, but few enough primitives to make it quick to learn.\nWorks great out of the box, but you can customize exactly what happens.\nHere are the main features of the SDK:\nAgent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\nPython-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\nHandoffs: A powerful feature to coordinate and delegate between multiple agents.\nGuardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\nFunction tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\nTracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\nInstallation\npip\ninstall\nopenai-agents\nHello world example\nfrom\nagents\nimport\nAgent\n,\nRunner\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You are a helpful assistant\"\n)\nresult\n=\nRunner\n.\nrun_sync\n(\nagent\n,\n\"Write a haiku about recursion in programming.\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n(\nIf running this, ensure you set the\nOPENAI_API_KEY\nenvironment variable\n)\nexport\nOPENAI_API_KEY\n=\nsk-...",
  "Quickstart": "Quickstart\nPrerequisites\nMake sure you've followed the base\nquickstart instructions\nfor the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:\npip\ninstall\n'openai-agents[voice]'\nConcepts\nThe main concept to know about is a\nVoicePipeline\n, which is a 3 step process:\nRun a speech-to-text model to turn audio into text.\nRun your code, which is usually an agentic workflow, to produce a result.\nRun a text-to-speech model to turn the result text back into audio.\ngraph LR\n    %% Input\n    A[\"ðŸŽ¤ Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --> C --> D\n    end\n\n    %% Output\n    E[\"ðŸŽ§ Audio Output\"]\n\n    %% Flow\n    A --> Voice_Pipeline\n    Voice_Pipeline --> E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\nAgents\nFirst, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.\nimport\nasyncio\nimport\nrandom\nfrom\nagents\nimport\n(\nAgent\n,\nfunction_tool\n,\n)\nfrom\nagents.extensions.handoff_prompt\nimport\nprompt_with_handoff_instructions\n@function_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\n\"\"\"Get the weather for a given city.\"\"\"\nprint\n(\nf\n\"[debug] get_weather called with city:\n{\ncity\n}\n\"\n)\nchoices\n=\n[\n\"sunny\"\n,\n\"cloudy\"\n,\n\"rainy\"\n,\n\"snowy\"\n]\nreturn\nf\n\"The weather in\n{\ncity\n}\nis\n{\nrandom\n.\nchoice\n(\nchoices\n)\n}\n.\"\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish\"\n,\nhandoff_description\n=\n\"A spanish speaking agent.\"\n,\ninstructions\n=\nprompt_with_handoff_instructions\n(\n\"You're speaking to a human, so be polite and concise. Speak in Spanish.\"\n,\n),\nmodel\n=\n\"gpt-4o-mini\"\n,\n)\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\nprompt_with_handoff_instructions\n(\n\"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\"\n,\n),\nmodel\n=\n\"gpt-4o-mini\"\n,\nhandoffs\n=\n[\nspanish_agent\n],\ntools\n=\n[\nget_weather\n],\n)\nVoice pipeline\nWe'll set up a simple voice pipeline, using\nSingleAgentVoiceWorkflow\nas the workflow.\nfrom\nagents.voice\nimport\nSingleAgentVoiceWorkflow\n,\nVoicePipeline\npipeline\n=\nVoicePipeline\n(\nworkflow\n=\nSingleAgentVoiceWorkflow\n(\nagent\n))\nRun the pipeline\nimport\nnumpy\nas\nnp\nimport\nsounddevice\nas\nsd\nfrom\nagents.voice\nimport\nAudioInput\n# For simplicity, we'll just create 3 seconds of silence\n# In reality, you'd get microphone data\nbuffer\n=\nnp\n.\nzeros\n(\n24000\n*\n3\n,\ndtype\n=\nnp\n.\nint16\n)\naudio_input\n=\nAudioInput\n(\nbuffer\n=\nbuffer\n)\nresult\n=\nawait\npipeline\n.\nrun\n(\naudio_input\n)\n# Create an audio player using `sounddevice`\nplayer\n=\nsd\n.\nOutputStream\n(\nsamplerate\n=\n24000\n,\nchannels\n=\n1\n,\ndtype\n=\nnp\n.\nint16\n)\nplayer\n.\nstart\n()\n# Play the audio stream as it comes in\nasync\nfor\nevent\nin\nresult\n.\nstream\n():\nif\nevent\n.\ntype\n==\n\"voice_stream_event_audio\"\n:\nplayer\n.\nwrite\n(\nevent\n.\ndata\n)\nPut it all together\nimport\nasyncio\nimport\nrandom\nimport\nnumpy\nas\nnp\nimport\nsounddevice\nas\nsd\nfrom\nagents\nimport\n(\nAgent\n,\nfunction_tool\n,\nset_tracing_disabled\n,\n)\nfrom\nagents.voice\nimport\n(\nAudioInput\n,\nSingleAgentVoiceWorkflow\n,\nVoicePipeline\n,\n)\nfrom\nagents.extensions.handoff_prompt\nimport\nprompt_with_handoff_instructions\n@function_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\n\"\"\"Get the weather for a given city.\"\"\"\nprint\n(\nf\n\"[debug] get_weather called with city:\n{\ncity\n}\n\"\n)\nchoices\n=\n[\n\"sunny\"\n,\n\"cloudy\"\n,\n\"rainy\"\n,\n\"snowy\"\n]\nreturn\nf\n\"The weather in\n{\ncity\n}\nis\n{\nrandom\n.\nchoice\n(\nchoices\n)\n}\n.\"\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish\"\n,\nhandoff_description\n=\n\"A spanish speaking agent.\"\n,\ninstructions\n=\nprompt_with_handoff_instructions\n(\n\"You're speaking to a human, so be polite and concise. Speak in Spanish.\"\n,\n),\nmodel\n=\n\"gpt-4o-mini\"\n,\n)\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\nprompt_with_handoff_instructions\n(\n\"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\"\n,\n),\nmodel\n=\n\"gpt-4o-mini\"\n,\nhandoffs\n=\n[\nspanish_agent\n],\ntools\n=\n[\nget_weather\n],\n)\nasync\ndef\nmain\n():\npipeline\n=\nVoicePipeline\n(\nworkflow\n=\nSingleAgentVoiceWorkflow\n(\nagent\n))\nbuffer\n=\nnp\n.\nzeros\n(\n24000\n*\n3\n,\ndtype\n=\nnp\n.\nint16\n)\naudio_input\n=\nAudioInput\n(\nbuffer\n=\nbuffer\n)\nresult\n=\nawait\npipeline\n.\nrun\n(\naudio_input\n)\n# Create an audio player using `sounddevice`\nplayer\n=\nsd\n.\nOutputStream\n(\nsamplerate\n=\n24000\n,\nchannels\n=\n1\n,\ndtype\n=\nnp\n.\nint16\n)\nplayer\n.\nstart\n()\n# Play the audio stream as it comes in\nasync\nfor\nevent\nin\nresult\n.\nstream\n():\nif\nevent\n.\ntype\n==\n\"voice_stream_event_audio\"\n:\nplayer\n.\nwrite\n(\nevent\n.\ndata\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nIf you run this example, the agent will speak to you! Check out the example in\nexamples/voice/static\nto see a demo where you can speak to the agent yourself.",
  "Examples": "Examples\nCheck out a variety of sample implementations of the SDK in the examples section of the\nrepo\n. The examples are organized into several categories that demonstrate different patterns and capabilities.\nCategories\nagent_patterns\n:\nExamples in this category illustrate common agent design patterns, such as\nDeterministic workflows\nAgents as tools\nParallel agent execution\nbasic\n:\nThese examples showcase foundational capabilities of the SDK, such as\nDynamic system prompts\nStreaming outputs\nLifecycle events\ntool examples\n:\nLearn how to implement OAI hosted tools such as web search and file search,\n   and integrate them into your agents.\nmodel providers\n:\nExplore how to use non-OpenAI models with the SDK.\nhandoffs\n:\nSee practical examples of agent handoffs.\nmcp\n:\nLearn how to build agents with MCP.\ncustomer_service\nand\nresearch_bot\n:\nTwo more built-out examples that illustrate real-world applications\ncustomer_service\n: Example customer service system for an airline.\nresearch_bot\n: Simple deep research clone.\nvoice\n:\nSee examples of voice agents, using our TTS and STT models.",
  "Agents": "Agents\nToolsToFinalOutputFunction\nmodule-attribute\nToolsToFinalOutputFunction\n:\nTypeAlias\n=\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nlist\n[\nFunctionToolResult\n]],\nMaybeAwaitable\n[\nToolsToFinalOutputResult\n],\n]\nA function that takes a run context and a list of tool results, and returns a\nToolsToFinalOutputResult\n.\nToolsToFinalOutputResult\ndataclass\nSource code in\nsrc/agents/agent.py\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n@dataclass\nclass\nToolsToFinalOutputResult\n:\nis_final_output\n:\nbool\n\"\"\"Whether this is the final output. If False, the LLM will run again and receive the tool call\noutput.\n\"\"\"\nfinal_output\n:\nAny\n|\nNone\n=\nNone\n\"\"\"The final output. Can be None if `is_final_output` is False, otherwise must match the\n`output_type` of the agent.\n\"\"\"\nis_final_output\ninstance-attribute\nis_final_output\n:\nbool\nWhether this is the final output. If False, the LLM will run again and receive the tool call\noutput.\nfinal_output\nclass-attribute\ninstance-attribute\nfinal_output\n:\nAny\n|\nNone\n=\nNone\nThe final output. Can be None if\nis_final_output\nis False, otherwise must match the\noutput_type\nof the agent.\nStopAtTools\nBases:\nTypedDict\nSource code in\nsrc/agents/agent.py\n51\n52\n53\nclass\nStopAtTools\n(\nTypedDict\n):\nstop_at_tool_names\n:\nlist\n[\nstr\n]\n\"\"\"A list of tool names, any of which will stop the agent from running further.\"\"\"\nstop_at_tool_names\ninstance-attribute\nstop_at_tool_names\n:\nlist\n[\nstr\n]\nA list of tool names, any of which will stop the agent from running further.\nMCPConfig\nBases:\nTypedDict\nConfiguration for MCP servers.\nSource code in\nsrc/agents/agent.py\n56\n57\n58\n59\n60\n61\n62\nclass\nMCPConfig\n(\nTypedDict\n):\n\"\"\"Configuration for MCP servers.\"\"\"\nconvert_schemas_to_strict\n:\nNotRequired\n[\nbool\n]\n\"\"\"If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a\nbest-effort conversion, so some schemas may not be convertible. Defaults to False.\n\"\"\"\nconvert_schemas_to_strict\ninstance-attribute\nconvert_schemas_to_strict\n:\nNotRequired\n[\nbool\n]\nIf True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a\nbest-effort conversion, so some schemas may not be convertible. Defaults to False.\nAgent\ndataclass\nBases:\nGeneric\n[\nTContext\n]\nAn agent is an AI model configured with instructions, tools, guardrails, handoffs and more.\nWe strongly recommend passing\ninstructions\n, which is the \"system prompt\" for the agent. In\naddition, you can pass\nhandoff_description\n, which is a human-readable description of the\nagent, used when the agent is used inside tools/handoffs.\nAgents are generic on the context type. The context is a (mutable) object you create. It is\npassed to tool functions, handoffs, guardrails, etc.\nSource code in\nsrc/agents/agent.py\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n@dataclass\nclass\nAgent\n(\nGeneric\n[\nTContext\n]):\n\"\"\"An agent is an AI model configured with instructions, tools, guardrails, handoffs and more.\nWe strongly recommend passing `instructions`, which is the \"system prompt\" for the agent. In\naddition, you can pass `handoff_description`, which is a human-readable description of the\nagent, used when the agent is used inside tools/handoffs.\nAgents are generic on the context type. The context is a (mutable) object you create. It is\npassed to tool functions, handoffs, guardrails, etc.\n\"\"\"\nname\n:\nstr\n\"\"\"The name of the agent.\"\"\"\ninstructions\n:\n(\nstr\n|\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nTContext\n]],\nMaybeAwaitable\n[\nstr\n],\n]\n|\nNone\n)\n=\nNone\n\"\"\"The instructions for the agent. Will be used as the \"system prompt\" when this agent is\ninvoked. Describes what the agent should do, and how it responds.\nCan either be a string, or a function that dynamically generates instructions for the agent. If\nyou provide a function, it will be called with the context and the agent instance. It must\nreturn a string.\n\"\"\"\nhandoff_description\n:\nstr\n|\nNone\n=\nNone\n\"\"\"A description of the agent. This is used when the agent is used as a handoff, so that an\nLLM knows what it does and when to invoke it.\n\"\"\"\nhandoffs\n:\nlist\n[\nAgent\n[\nAny\n]\n|\nHandoff\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\n\"\"\"Handoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\nand the agent can choose to delegate to them if relevant. Allows for separation of concerns and\nmodularity.\n\"\"\"\nmodel\n:\nstr\n|\nModel\n|\nNone\n=\nNone\n\"\"\"The model implementation to use when invoking the LLM.\nBy default, if not set, the agent will use the default model configured in\n`openai_provider.DEFAULT_MODEL` (currently \"gpt-4o\").\n\"\"\"\nmodel_settings\n:\nModelSettings\n=\nfield\n(\ndefault_factory\n=\nModelSettings\n)\n\"\"\"Configures model-specific tuning parameters (e.g. temperature, top_p).\n\"\"\"\ntools\n:\nlist\n[\nTool\n]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\n\"\"\"A list of tools that the agent can use.\"\"\"\nmcp_servers\n:\nlist\n[\nMCPServer\n]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\n\"\"\"A list of [Model Context Protocol](https://modelcontextprotocol.io/) servers that\nthe agent can use. Every time the agent runs, it will include tools from these servers in the\nlist of available tools.\nNOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call\n`server.connect()` before passing it to the agent, and `server.cleanup()` when the server is no\nlonger needed.\n\"\"\"\nmcp_config\n:\nMCPConfig\n=\nfield\n(\ndefault_factory\n=\nlambda\n:\nMCPConfig\n())\n\"\"\"Configuration for MCP servers.\"\"\"\ninput_guardrails\n:\nlist\n[\nInputGuardrail\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\n\"\"\"A list of checks that run in parallel to the agent's execution, before generating a\nresponse. Runs only if the agent is the first agent in the chain.\n\"\"\"\noutput_guardrails\n:\nlist\n[\nOutputGuardrail\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\n\"\"\"A list of checks that run on the final output of the agent, after generating a response.\nRuns only if the agent produces a final output.\n\"\"\"\noutput_type\n:\ntype\n[\nAny\n]\n|\nNone\n=\nNone\n\"\"\"The type of the output object. If not provided, the output will be `str`.\"\"\"\nhooks\n:\nAgentHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n\"\"\"A class that receives callbacks on various lifecycle events for this agent.\n\"\"\"\ntool_use_behavior\n:\n(\nLiteral\n[\n\"run_llm_again\"\n,\n\"stop_on_first_tool\"\n]\n|\nStopAtTools\n|\nToolsToFinalOutputFunction\n)\n=\n\"run_llm_again\"\n\"\"\"This lets you configure how tool use is handled.\n- \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results\nand gets to respond.\n- \"stop_on_first_tool\": The output of the first tool call is used as the final output. This\nmeans that the LLM does not process the result of the tool call.\n- A list of tool names: The agent will stop running if any of the tools in the list are called.\nThe final output will be the output of the first matching tool call. The LLM does not\nprocess the result of the tool call.\n- A function: If you pass a function, it will be called with the run context and the list of\ntool results. It must return a `ToolToFinalOutputResult`, which determines whether the tool\ncalls result in a final output.\nNOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,\nweb search, etc are always processed by the LLM.\n\"\"\"\nreset_tool_choice\n:\nbool\n=\nTrue\n\"\"\"Whether to reset the tool choice to the default value after a tool has been called. Defaults\nto True. This ensures that the agent doesn't enter an infinite loop of tool usage.\"\"\"\ndef\nclone\n(\nself\n,\n**\nkwargs\n:\nAny\n)\n->\nAgent\n[\nTContext\n]:\n\"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n```\nnew_agent = agent.clone(instructions=\"New instructions\")\n```\n\"\"\"\nreturn\ndataclasses\n.\nreplace\n(\nself\n,\n**\nkwargs\n)\ndef\nas_tool\n(\nself\n,\ntool_name\n:\nstr\n|\nNone\n,\ntool_description\n:\nstr\n|\nNone\n,\ncustom_output_extractor\n:\nCallable\n[[\nRunResult\n],\nAwaitable\n[\nstr\n]]\n|\nNone\n=\nNone\n,\n)\n->\nTool\n:\n\"\"\"Transform this agent into a tool, callable by other agents.\nThis is different from handoffs in two ways:\n1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\nreceives generated input.\n2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\ncalled as a tool, and the conversation is continued by the original agent.\nArgs:\ntool_name: The name of the tool. If not provided, the agent's name will be used.\ntool_description: The description of the tool, which should indicate what it does and\nwhen to use it.\ncustom_output_extractor: A function that extracts the output from the agent. If not\nprovided, the last message from the agent will be used.\n\"\"\"\n@function_tool\n(\nname_override\n=\ntool_name\nor\n_transforms\n.\ntransform_string_function_style\n(\nself\n.\nname\n),\ndescription_override\n=\ntool_description\nor\n\"\"\n,\n)\nasync\ndef\nrun_agent\n(\ncontext\n:\nRunContextWrapper\n,\ninput\n:\nstr\n)\n->\nstr\n:\nfrom\n.run\nimport\nRunner\noutput\n=\nawait\nRunner\n.\nrun\n(\nstarting_agent\n=\nself\n,\ninput\n=\ninput\n,\ncontext\n=\ncontext\n.\ncontext\n,\n)\nif\ncustom_output_extractor\n:\nreturn\nawait\ncustom_output_extractor\n(\noutput\n)\nreturn\nItemHelpers\n.\ntext_message_outputs\n(\noutput\n.\nnew_items\n)\nreturn\nrun_agent\nasync\ndef\nget_system_prompt\n(\nself\n,\nrun_context\n:\nRunContextWrapper\n[\nTContext\n])\n->\nstr\n|\nNone\n:\n\"\"\"Get the system prompt for the agent.\"\"\"\nif\nisinstance\n(\nself\n.\ninstructions\n,\nstr\n):\nreturn\nself\n.\ninstructions\nelif\ncallable\n(\nself\n.\ninstructions\n):\nif\ninspect\n.\niscoroutinefunction\n(\nself\n.\ninstructions\n):\nreturn\nawait\ncast\n(\nAwaitable\n[\nstr\n],\nself\n.\ninstructions\n(\nrun_context\n,\nself\n))\nelse\n:\nreturn\ncast\n(\nstr\n,\nself\n.\ninstructions\n(\nrun_context\n,\nself\n))\nelif\nself\n.\ninstructions\nis\nnot\nNone\n:\nlogger\n.\nerror\n(\nf\n\"Instructions must be a string or a function, got\n{\nself\n.\ninstructions\n}\n\"\n)\nreturn\nNone\nasync\ndef\nget_mcp_tools\n(\nself\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Fetches the available tools from the MCP servers.\"\"\"\nconvert_schemas_to_strict\n=\nself\n.\nmcp_config\n.\nget\n(\n\"convert_schemas_to_strict\"\n,\nFalse\n)\nreturn\nawait\nMCPUtil\n.\nget_all_function_tools\n(\nself\n.\nmcp_servers\n,\nconvert_schemas_to_strict\n)\nasync\ndef\nget_all_tools\n(\nself\n)\n->\nlist\n[\nTool\n]:\n\"\"\"All agent tools, including MCP tools and function tools.\"\"\"\nmcp_tools\n=\nawait\nself\n.\nget_mcp_tools\n()\nreturn\nmcp_tools\n+\nself\n.\ntools\nname\ninstance-attribute\nname\n:\nstr\nThe name of the agent.\ninstructions\nclass-attribute\ninstance-attribute\ninstructions\n:\n(\nstr\n|\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nTContext\n]],\nMaybeAwaitable\n[\nstr\n],\n]\n|\nNone\n)\n=\nNone\nThe instructions for the agent. Will be used as the \"system prompt\" when this agent is\ninvoked. Describes what the agent should do, and how it responds.\nCan either be a string, or a function that dynamically generates instructions for the agent. If\nyou provide a function, it will be called with the context and the agent instance. It must\nreturn a string.\nhandoff_description\nclass-attribute\ninstance-attribute\nhandoff_description\n:\nstr\n|\nNone\n=\nNone\nA description of the agent. This is used when the agent is used as a handoff, so that an\nLLM knows what it does and when to invoke it.\nhandoffs\nclass-attribute\ninstance-attribute\nhandoffs\n:\nlist\n[\nAgent\n[\nAny\n]\n|\nHandoff\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\nHandoffs are sub-agents that the agent can delegate to. You can provide a list of handoffs,\nand the agent can choose to delegate to them if relevant. Allows for separation of concerns and\nmodularity.\nmodel\nclass-attribute\ninstance-attribute\nmodel\n:\nstr\n|\nModel\n|\nNone\n=\nNone\nThe model implementation to use when invoking the LLM.\nBy default, if not set, the agent will use the default model configured in\nopenai_provider.DEFAULT_MODEL\n(currently \"gpt-4o\").\nmodel_settings\nclass-attribute\ninstance-attribute\nmodel_settings\n:\nModelSettings\n=\nfield\n(\ndefault_factory\n=\nModelSettings\n)\nConfigures model-specific tuning parameters (e.g. temperature, top_p).\ntools\nclass-attribute\ninstance-attribute\ntools\n:\nlist\n[\nTool\n]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\nA list of tools that the agent can use.\nmcp_servers\nclass-attribute\ninstance-attribute\nmcp_servers\n:\nlist\n[\nMCPServer\n]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\nA list of\nModel Context Protocol\nservers that\nthe agent can use. Every time the agent runs, it will include tools from these servers in the\nlist of available tools.\nNOTE: You are expected to manage the lifecycle of these servers. Specifically, you must call\nserver.connect()\nbefore passing it to the agent, and\nserver.cleanup()\nwhen the server is no\nlonger needed.\nmcp_config\nclass-attribute\ninstance-attribute\nmcp_config\n:\nMCPConfig\n=\nfield\n(\ndefault_factory\n=\nlambda\n:\nMCPConfig\n()\n)\nConfiguration for MCP servers.\ninput_guardrails\nclass-attribute\ninstance-attribute\ninput_guardrails\n:\nlist\n[\nInputGuardrail\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\nA list of checks that run in parallel to the agent's execution, before generating a\nresponse. Runs only if the agent is the first agent in the chain.\noutput_guardrails\nclass-attribute\ninstance-attribute\noutput_guardrails\n:\nlist\n[\nOutputGuardrail\n[\nTContext\n]]\n=\nfield\n(\ndefault_factory\n=\nlist\n)\nA list of checks that run on the final output of the agent, after generating a response.\nRuns only if the agent produces a final output.\noutput_type\nclass-attribute\ninstance-attribute\noutput_type\n:\ntype\n[\nAny\n]\n|\nNone\n=\nNone\nThe type of the output object. If not provided, the output will be\nstr\n.\nhooks\nclass-attribute\ninstance-attribute\nhooks\n:\nAgentHooks\n[\nTContext\n]\n|\nNone\n=\nNone\nA class that receives callbacks on various lifecycle events for this agent.\ntool_use_behavior\nclass-attribute\ninstance-attribute\ntool_use_behavior\n:\n(\nLiteral\n[\n\"run_llm_again\"\n,\n\"stop_on_first_tool\"\n]\n|\nStopAtTools\n|\nToolsToFinalOutputFunction\n)\n=\n\"run_llm_again\"\nThis lets you configure how tool use is handled.\n- \"run_llm_again\": The default behavior. Tools are run, and then the LLM receives the results\n    and gets to respond.\n- \"stop_on_first_tool\": The output of the first tool call is used as the final output. This\n    means that the LLM does not process the result of the tool call.\n- A list of tool names: The agent will stop running if any of the tools in the list are called.\n    The final output will be the output of the first matching tool call. The LLM does not\n    process the result of the tool call.\n- A function: If you pass a function, it will be called with the run context and the list of\n  tool results. It must return a\nToolToFinalOutputResult\n, which determines whether the tool\n  calls result in a final output.\nNOTE: This configuration is specific to FunctionTools. Hosted tools, such as file search,\n  web search, etc are always processed by the LLM.\nreset_tool_choice\nclass-attribute\ninstance-attribute\nreset_tool_choice\n:\nbool\n=\nTrue\nWhether to reset the tool choice to the default value after a tool has been called. Defaults\nto True. This ensures that the agent doesn't enter an infinite loop of tool usage.\nclone\nclone\n(\n**\nkwargs\n:\nAny\n)\n->\nAgent\n[\nTContext\n]\nMake a copy of the agent, with the given arguments changed. For example, you could do:\nnew_agent = agent.clone(instructions=\"New instructions\")\nSource code in\nsrc/agents/agent.py\n174\n175\n176\n177\n178\n179\n180\ndef\nclone\n(\nself\n,\n**\nkwargs\n:\nAny\n)\n->\nAgent\n[\nTContext\n]:\n\"\"\"Make a copy of the agent, with the given arguments changed. For example, you could do:\n```\nnew_agent = agent.clone(instructions=\"New instructions\")\n```\n\"\"\"\nreturn\ndataclasses\n.\nreplace\n(\nself\n,\n**\nkwargs\n)\nas_tool\nas_tool\n(\ntool_name\n:\nstr\n|\nNone\n,\ntool_description\n:\nstr\n|\nNone\n,\ncustom_output_extractor\n:\nCallable\n[\n[\nRunResult\n],\nAwaitable\n[\nstr\n]\n]\n|\nNone\n=\nNone\n,\n)\n->\nTool\nTransform this agent into a tool, callable by other agents.\nThis is different from handoffs in two ways:\n1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\n   receives generated input.\n2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\n   called as a tool, and the conversation is continued by the original agent.\nParameters:\nName\nType\nDescription\nDefault\ntool_name\nstr\n| None\nThe name of the tool. If not provided, the agent's name will be used.\nrequired\ntool_description\nstr\n| None\nThe description of the tool, which should indicate what it does and\nwhen to use it.\nrequired\ncustom_output_extractor\nCallable\n[[\nRunResult\n],\nAwaitable\n[\nstr\n]] | None\nA function that extracts the output from the agent. If not\nprovided, the last message from the agent will be used.\nNone\nSource code in\nsrc/agents/agent.py\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\ndef\nas_tool\n(\nself\n,\ntool_name\n:\nstr\n|\nNone\n,\ntool_description\n:\nstr\n|\nNone\n,\ncustom_output_extractor\n:\nCallable\n[[\nRunResult\n],\nAwaitable\n[\nstr\n]]\n|\nNone\n=\nNone\n,\n)\n->\nTool\n:\n\"\"\"Transform this agent into a tool, callable by other agents.\nThis is different from handoffs in two ways:\n1. In handoffs, the new agent receives the conversation history. In this tool, the new agent\nreceives generated input.\n2. In handoffs, the new agent takes over the conversation. In this tool, the new agent is\ncalled as a tool, and the conversation is continued by the original agent.\nArgs:\ntool_name: The name of the tool. If not provided, the agent's name will be used.\ntool_description: The description of the tool, which should indicate what it does and\nwhen to use it.\ncustom_output_extractor: A function that extracts the output from the agent. If not\nprovided, the last message from the agent will be used.\n\"\"\"\n@function_tool\n(\nname_override\n=\ntool_name\nor\n_transforms\n.\ntransform_string_function_style\n(\nself\n.\nname\n),\ndescription_override\n=\ntool_description\nor\n\"\"\n,\n)\nasync\ndef\nrun_agent\n(\ncontext\n:\nRunContextWrapper\n,\ninput\n:\nstr\n)\n->\nstr\n:\nfrom\n.run\nimport\nRunner\noutput\n=\nawait\nRunner\n.\nrun\n(\nstarting_agent\n=\nself\n,\ninput\n=\ninput\n,\ncontext\n=\ncontext\n.\ncontext\n,\n)\nif\ncustom_output_extractor\n:\nreturn\nawait\ncustom_output_extractor\n(\noutput\n)\nreturn\nItemHelpers\n.\ntext_message_outputs\n(\noutput\n.\nnew_items\n)\nreturn\nrun_agent\nget_system_prompt\nasync\nget_system_prompt\n(\nrun_context\n:\nRunContextWrapper\n[\nTContext\n],\n)\n->\nstr\n|\nNone\nGet the system prompt for the agent.\nSource code in\nsrc/agents/agent.py\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\nasync\ndef\nget_system_prompt\n(\nself\n,\nrun_context\n:\nRunContextWrapper\n[\nTContext\n])\n->\nstr\n|\nNone\n:\n\"\"\"Get the system prompt for the agent.\"\"\"\nif\nisinstance\n(\nself\n.\ninstructions\n,\nstr\n):\nreturn\nself\n.\ninstructions\nelif\ncallable\n(\nself\n.\ninstructions\n):\nif\ninspect\n.\niscoroutinefunction\n(\nself\n.\ninstructions\n):\nreturn\nawait\ncast\n(\nAwaitable\n[\nstr\n],\nself\n.\ninstructions\n(\nrun_context\n,\nself\n))\nelse\n:\nreturn\ncast\n(\nstr\n,\nself\n.\ninstructions\n(\nrun_context\n,\nself\n))\nelif\nself\n.\ninstructions\nis\nnot\nNone\n:\nlogger\n.\nerror\n(\nf\n\"Instructions must be a string or a function, got\n{\nself\n.\ninstructions\n}\n\"\n)\nreturn\nNone\nget_mcp_tools\nasync\nget_mcp_tools\n()\n->\nlist\n[\nTool\n]\nFetches the available tools from the MCP servers.\nSource code in\nsrc/agents/agent.py\n237\n238\n239\n240\nasync\ndef\nget_mcp_tools\n(\nself\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Fetches the available tools from the MCP servers.\"\"\"\nconvert_schemas_to_strict\n=\nself\n.\nmcp_config\n.\nget\n(\n\"convert_schemas_to_strict\"\n,\nFalse\n)\nreturn\nawait\nMCPUtil\n.\nget_all_function_tools\n(\nself\n.\nmcp_servers\n,\nconvert_schemas_to_strict\n)\nget_all_tools\nasync\nget_all_tools\n()\n->\nlist\n[\nTool\n]\nAll agent tools, including MCP tools and function tools.\nSource code in\nsrc/agents/agent.py\n242\n243\n244\n245\nasync\ndef\nget_all_tools\n(\nself\n)\n->\nlist\n[\nTool\n]:\n\"\"\"All agent tools, including MCP tools and function tools.\"\"\"\nmcp_tools\n=\nawait\nself\n.\nget_mcp_tools\n()\nreturn\nmcp_tools\n+\nself\n.\ntools",
  "Running agents": "Running agents\nYou can run agents via the\nRunner\nclass. You have 3 options:\nRunner.run()\n, which runs async and returns a\nRunResult\n.\nRunner.run_sync()\n, which is a sync method and just runs\n.run()\nunder the hood.\nRunner.run_streamed()\n, which runs async and returns a\nRunResultStreaming\n. It calls the LLM in streaming mode, and streams those events to you as they are received.\nfrom\nagents\nimport\nAgent\n,\nRunner\nasync\ndef\nmain\n():\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You are a helpful assistant\"\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"Write a haiku about recursion in programming.\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\nRead more in the\nresults guide\n.\nThe agent loop\nWhen you use the run method in\nRunner\n, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.\nThe runner then runs a loop:\nWe call the LLM for the current agent, with the current input.\nThe LLM produces its output.\nIf the LLM returns a\nfinal_output\n, the loop ends and we return the result.\nIf the LLM does a handoff, we update the current agent and input, and re-run the loop.\nIf the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.\nIf we exceed the\nmax_turns\npassed, we raise a\nMaxTurnsExceeded\nexception.\nNote\nThe rule for whether the LLM output is considered as a \"final output\" is that it produces text output with the desired type, and there are no tool calls.\nStreaming\nStreaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the\nRunResultStreaming\nwill contain the complete information about the run, including all the new outputs produces. You can call\n.stream_events()\nfor the streaming events. Read more in the\nstreaming guide\n.\nRun config\nThe\nrun_config\nparameter lets you configure some global settings for the agent run:\nmodel\n: Allows setting a global LLM model to use, irrespective of what\nmodel\neach Agent has.\nmodel_provider\n: A model provider for looking up model names, which defaults to OpenAI.\nmodel_settings\n: Overrides agent-specific settings. For example, you can set a global\ntemperature\nor\ntop_p\n.\ninput_guardrails\n,\noutput_guardrails\n: A list of input or output guardrails to include on all runs.\nhandoff_input_filter\n: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in\nHandoff.input_filter\nfor more details.\ntracing_disabled\n: Allows you to disable\ntracing\nfor the entire run.\ntrace_include_sensitive_data\n: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.\nworkflow_name\n,\ntrace_id\n,\ngroup_id\n: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting\nworkflow_name\n. The group ID is an optional field that lets you link traces across multiple runs.\ntrace_metadata\n: Metadata to include on all traces.\nConversations/chat threads\nCalling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:\nUser turn: user enter text\nRunner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.\nAt the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.\nYou can use the base\nRunResultBase.to_input_list()\nmethod to get the inputs for the next turn.\nasync\ndef\nmain\n():\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"Reply very concisely.\"\n)\nwith\ntrace\n(\nworkflow_name\n=\n\"Conversation\"\n,\ngroup_id\n=\nthread_id\n):\n# First turn\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What city is the Golden Gate Bridge in?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# San Francisco\n# Second turn\nnew_input\n=\nresult\n.\nto_input_list\n()\n+\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What state is it in?\"\n}]\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\nnew_input\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# California\nExceptions\nThe SDK raises exceptions in certain cases. The full list is in\nagents.exceptions\n. As an overview:\nAgentsException\nis the base class for all exceptions raised in the SDK.\nMaxTurnsExceeded\nis raised when the run exceeds the\nmax_turns\npassed to the run methods.\nModelBehaviorError\nis raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.\nUserError\nis raised when you (the person writing code using the SDK) make an error using the SDK.\nInputGuardrailTripwireTriggered\n,\nOutputGuardrailTripwireTriggered\nis raised when a\nguardrail\nis tripped.",
  "Results": "Results\nRunResultBase\ndataclass\nBases:\nABC\nSource code in\nsrc/agents/result.py\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n@dataclass\nclass\nRunResultBase\n(\nabc\n.\nABC\n):\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\n\"\"\"The original input items i.e. the items before run() was called. This may be a mutated\nversion of the input, if there are handoff input filters that mutate the input.\n\"\"\"\nnew_items\n:\nlist\n[\nRunItem\n]\n\"\"\"The new items generated during the agent run. These include things like new messages, tool\ncalls and their outputs, etc.\n\"\"\"\nraw_responses\n:\nlist\n[\nModelResponse\n]\n\"\"\"The raw LLM responses generated by the model during the agent run.\"\"\"\nfinal_output\n:\nAny\n\"\"\"The output of the last agent.\"\"\"\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\n\"\"\"Guardrail results for the input messages.\"\"\"\noutput_guardrail_results\n:\nlist\n[\nOutputGuardrailResult\n]\n\"\"\"Guardrail results for the final output of the agent.\"\"\"\n@property\n@abc\n.\nabstractmethod\ndef\nlast_agent\n(\nself\n)\n->\nAgent\n[\nAny\n]:\n\"\"\"The last agent that was run.\"\"\"\ndef\nfinal_output_as\n(\nself\n,\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\n:\n\"\"\"A convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\nTypeError if the final output is not of the given type.\nArgs:\ncls: The type to cast the final output to.\nraise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\nthe given type.\nReturns:\nThe final output casted to the given type.\n\"\"\"\nif\nraise_if_incorrect_type\nand\nnot\nisinstance\n(\nself\n.\nfinal_output\n,\ncls\n):\nraise\nTypeError\n(\nf\n\"Final output is not of type\n{\ncls\n.\n__name__\n}\n\"\n)\nreturn\ncast\n(\nT\n,\nself\n.\nfinal_output\n)\ndef\nto_input_list\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\noriginal_items\n:\nlist\n[\nTResponseInputItem\n]\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\nself\n.\ninput\n)\nnew_items\n=\n[\nitem\n.\nto_input_item\n()\nfor\nitem\nin\nself\n.\nnew_items\n]\nreturn\noriginal_items\n+\nnew_items\n@property\ndef\nlast_response_id\n(\nself\n)\n->\nstr\n|\nNone\n:\n\"\"\"Convenience method to get the response ID of the last model response.\"\"\"\nif\nnot\nself\n.\nraw_responses\n:\nreturn\nNone\nreturn\nself\n.\nraw_responses\n[\n-\n1\n]\n.\nresponse_id\ninput\ninstance-attribute\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe original input items i.e. the items before run() was called. This may be a mutated\nversion of the input, if there are handoff input filters that mutate the input.\nnew_items\ninstance-attribute\nnew_items\n:\nlist\n[\nRunItem\n]\nThe new items generated during the agent run. These include things like new messages, tool\ncalls and their outputs, etc.\nraw_responses\ninstance-attribute\nraw_responses\n:\nlist\n[\nModelResponse\n]\nThe raw LLM responses generated by the model during the agent run.\nfinal_output\ninstance-attribute\nfinal_output\n:\nAny\nThe output of the last agent.\ninput_guardrail_results\ninstance-attribute\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\nGuardrail results for the input messages.\noutput_guardrail_results\ninstance-attribute\noutput_guardrail_results\n:\nlist\n[\nOutputGuardrailResult\n]\nGuardrail results for the final output of the agent.\nlast_agent\nabstractmethod\nproperty\nlast_agent\n:\nAgent\n[\nAny\n]\nThe last agent that was run.\nlast_response_id\nproperty\nlast_response_id\n:\nstr\n|\nNone\nConvenience method to get the response ID of the last model response.\nfinal_output_as\nfinal_output_as\n(\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\nA convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set\nraise_if_incorrect_type\nto True, we'll raise a\nTypeError if the final output is not of the given type.\nParameters:\nName\nType\nDescription\nDefault\ncls\ntype\n[\nT\n]\nThe type to cast the final output to.\nrequired\nraise_if_incorrect_type\nbool\nIf True, we'll raise a TypeError if the final output is not of\nthe given type.\nFalse\nReturns:\nType\nDescription\nT\nThe final output casted to the given type.\nSource code in\nsrc/agents/result.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\ndef\nfinal_output_as\n(\nself\n,\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\n:\n\"\"\"A convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\nTypeError if the final output is not of the given type.\nArgs:\ncls: The type to cast the final output to.\nraise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\nthe given type.\nReturns:\nThe final output casted to the given type.\n\"\"\"\nif\nraise_if_incorrect_type\nand\nnot\nisinstance\n(\nself\n.\nfinal_output\n,\ncls\n):\nraise\nTypeError\n(\nf\n\"Final output is not of type\n{\ncls\n.\n__name__\n}\n\"\n)\nreturn\ncast\n(\nT\n,\nself\n.\nfinal_output\n)\nto_input_list\nto_input_list\n()\n->\nlist\n[\nTResponseInputItem\n]\nCreates a new input list, merging the original input with all the new items generated.\nSource code in\nsrc/agents/result.py\n76\n77\n78\n79\n80\n81\ndef\nto_input_list\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\noriginal_items\n:\nlist\n[\nTResponseInputItem\n]\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\nself\n.\ninput\n)\nnew_items\n=\n[\nitem\n.\nto_input_item\n()\nfor\nitem\nin\nself\n.\nnew_items\n]\nreturn\noriginal_items\n+\nnew_items\nRunResult\ndataclass\nBases:\nRunResultBase\nSource code in\nsrc/agents/result.py\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n@dataclass\nclass\nRunResult\n(\nRunResultBase\n):\n_last_agent\n:\nAgent\n[\nAny\n]\n@property\ndef\nlast_agent\n(\nself\n)\n->\nAgent\n[\nAny\n]:\n\"\"\"The last agent that was run.\"\"\"\nreturn\nself\n.\n_last_agent\ndef\n__str__\n(\nself\n)\n->\nstr\n:\nreturn\npretty_print_result\n(\nself\n)\nlast_agent\nproperty\nlast_agent\n:\nAgent\n[\nAny\n]\nThe last agent that was run.\ninput\ninstance-attribute\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe original input items i.e. the items before run() was called. This may be a mutated\nversion of the input, if there are handoff input filters that mutate the input.\nnew_items\ninstance-attribute\nnew_items\n:\nlist\n[\nRunItem\n]\nThe new items generated during the agent run. These include things like new messages, tool\ncalls and their outputs, etc.\nraw_responses\ninstance-attribute\nraw_responses\n:\nlist\n[\nModelResponse\n]\nThe raw LLM responses generated by the model during the agent run.\nfinal_output\ninstance-attribute\nfinal_output\n:\nAny\nThe output of the last agent.\ninput_guardrail_results\ninstance-attribute\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\nGuardrail results for the input messages.\noutput_guardrail_results\ninstance-attribute\noutput_guardrail_results\n:\nlist\n[\nOutputGuardrailResult\n]\nGuardrail results for the final output of the agent.\nlast_response_id\nproperty\nlast_response_id\n:\nstr\n|\nNone\nConvenience method to get the response ID of the last model response.\nfinal_output_as\nfinal_output_as\n(\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\nA convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set\nraise_if_incorrect_type\nto True, we'll raise a\nTypeError if the final output is not of the given type.\nParameters:\nName\nType\nDescription\nDefault\ncls\ntype\n[\nT\n]\nThe type to cast the final output to.\nrequired\nraise_if_incorrect_type\nbool\nIf True, we'll raise a TypeError if the final output is not of\nthe given type.\nFalse\nReturns:\nType\nDescription\nT\nThe final output casted to the given type.\nSource code in\nsrc/agents/result.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\ndef\nfinal_output_as\n(\nself\n,\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\n:\n\"\"\"A convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\nTypeError if the final output is not of the given type.\nArgs:\ncls: The type to cast the final output to.\nraise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\nthe given type.\nReturns:\nThe final output casted to the given type.\n\"\"\"\nif\nraise_if_incorrect_type\nand\nnot\nisinstance\n(\nself\n.\nfinal_output\n,\ncls\n):\nraise\nTypeError\n(\nf\n\"Final output is not of type\n{\ncls\n.\n__name__\n}\n\"\n)\nreturn\ncast\n(\nT\n,\nself\n.\nfinal_output\n)\nto_input_list\nto_input_list\n()\n->\nlist\n[\nTResponseInputItem\n]\nCreates a new input list, merging the original input with all the new items generated.\nSource code in\nsrc/agents/result.py\n76\n77\n78\n79\n80\n81\ndef\nto_input_list\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\noriginal_items\n:\nlist\n[\nTResponseInputItem\n]\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\nself\n.\ninput\n)\nnew_items\n=\n[\nitem\n.\nto_input_item\n()\nfor\nitem\nin\nself\n.\nnew_items\n]\nreturn\noriginal_items\n+\nnew_items\nRunResultStreaming\ndataclass\nBases:\nRunResultBase\nThe result of an agent run in streaming mode. You can use the\nstream_events\nmethod to\nreceive semantic events as they are generated.\nThe streaming method will raise:\n- A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n- A GuardrailTripwireTriggered exception if a guardrail is tripped.\nSource code in\nsrc/agents/result.py\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n@dataclass\nclass\nRunResultStreaming\n(\nRunResultBase\n):\n\"\"\"The result of an agent run in streaming mode. You can use the `stream_events` method to\nreceive semantic events as they are generated.\nThe streaming method will raise:\n- A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n- A GuardrailTripwireTriggered exception if a guardrail is tripped.\n\"\"\"\ncurrent_agent\n:\nAgent\n[\nAny\n]\n\"\"\"The current agent that is running.\"\"\"\ncurrent_turn\n:\nint\n\"\"\"The current turn number.\"\"\"\nmax_turns\n:\nint\n\"\"\"The maximum number of turns the agent can run for.\"\"\"\nfinal_output\n:\nAny\n\"\"\"The final output of the agent. This is None until the agent has finished running.\"\"\"\n_current_agent_output_schema\n:\nAgentOutputSchema\n|\nNone\n=\nfield\n(\nrepr\n=\nFalse\n)\n_trace\n:\nTrace\n|\nNone\n=\nfield\n(\nrepr\n=\nFalse\n)\nis_complete\n:\nbool\n=\nFalse\n\"\"\"Whether the agent has finished running.\"\"\"\n# Queues that the background run_loop writes to\n_event_queue\n:\nasyncio\n.\nQueue\n[\nStreamEvent\n|\nQueueCompleteSentinel\n]\n=\nfield\n(\ndefault_factory\n=\nasyncio\n.\nQueue\n,\nrepr\n=\nFalse\n)\n_input_guardrail_queue\n:\nasyncio\n.\nQueue\n[\nInputGuardrailResult\n]\n=\nfield\n(\ndefault_factory\n=\nasyncio\n.\nQueue\n,\nrepr\n=\nFalse\n)\n# Store the asyncio tasks that we're waiting on\n_run_impl_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nfield\n(\ndefault\n=\nNone\n,\nrepr\n=\nFalse\n)\n_input_guardrails_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nfield\n(\ndefault\n=\nNone\n,\nrepr\n=\nFalse\n)\n_output_guardrails_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nfield\n(\ndefault\n=\nNone\n,\nrepr\n=\nFalse\n)\n_stored_exception\n:\nException\n|\nNone\n=\nfield\n(\ndefault\n=\nNone\n,\nrepr\n=\nFalse\n)\n@property\ndef\nlast_agent\n(\nself\n)\n->\nAgent\n[\nAny\n]:\n\"\"\"The last agent that was run. Updates as the agent run progresses, so the true last agent\nis only available after the agent run is complete.\n\"\"\"\nreturn\nself\n.\ncurrent_agent\nasync\ndef\nstream_events\n(\nself\n)\n->\nAsyncIterator\n[\nStreamEvent\n]:\n\"\"\"Stream deltas for new items as they are generated. We're using the types from the\nOpenAI Responses API, so these are semantic events: each event has a `type` field that\ndescribes the type of the event, along with the data for that event.\nThis will raise:\n- A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n- A GuardrailTripwireTriggered exception if a guardrail is tripped.\n\"\"\"\nwhile\nTrue\n:\nself\n.\n_check_errors\n()\nif\nself\n.\n_stored_exception\n:\nlogger\n.\ndebug\n(\n\"Breaking due to stored exception\"\n)\nself\n.\nis_complete\n=\nTrue\nbreak\nif\nself\n.\nis_complete\nand\nself\n.\n_event_queue\n.\nempty\n():\nbreak\ntry\n:\nitem\n=\nawait\nself\n.\n_event_queue\n.\nget\n()\nexcept\nasyncio\n.\nCancelledError\n:\nbreak\nif\nisinstance\n(\nitem\n,\nQueueCompleteSentinel\n):\nself\n.\n_event_queue\n.\ntask_done\n()\n# Check for errors, in case the queue was completed due to an exception\nself\n.\n_check_errors\n()\nbreak\nyield\nitem\nself\n.\n_event_queue\n.\ntask_done\n()\nif\nself\n.\n_trace\n:\nself\n.\n_trace\n.\nfinish\n(\nreset_current\n=\nTrue\n)\nself\n.\n_cleanup_tasks\n()\nif\nself\n.\n_stored_exception\n:\nraise\nself\n.\n_stored_exception\ndef\n_check_errors\n(\nself\n):\nif\nself\n.\ncurrent_turn\n>\nself\n.\nmax_turns\n:\nself\n.\n_stored_exception\n=\nMaxTurnsExceeded\n(\nf\n\"Max turns (\n{\nself\n.\nmax_turns\n}\n) exceeded\"\n)\n# Fetch all the completed guardrail results from the queue and raise if needed\nwhile\nnot\nself\n.\n_input_guardrail_queue\n.\nempty\n():\nguardrail_result\n=\nself\n.\n_input_guardrail_queue\n.\nget_nowait\n()\nif\nguardrail_result\n.\noutput\n.\ntripwire_triggered\n:\nself\n.\n_stored_exception\n=\nInputGuardrailTripwireTriggered\n(\nguardrail_result\n)\n# Check the tasks for any exceptions\nif\nself\n.\n_run_impl_task\nand\nself\n.\n_run_impl_task\n.\ndone\n():\nexc\n=\nself\n.\n_run_impl_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\nif\nself\n.\n_input_guardrails_task\nand\nself\n.\n_input_guardrails_task\n.\ndone\n():\nexc\n=\nself\n.\n_input_guardrails_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\nif\nself\n.\n_output_guardrails_task\nand\nself\n.\n_output_guardrails_task\n.\ndone\n():\nexc\n=\nself\n.\n_output_guardrails_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\ndef\n_cleanup_tasks\n(\nself\n):\nif\nself\n.\n_run_impl_task\nand\nnot\nself\n.\n_run_impl_task\n.\ndone\n():\nself\n.\n_run_impl_task\n.\ncancel\n()\nif\nself\n.\n_input_guardrails_task\nand\nnot\nself\n.\n_input_guardrails_task\n.\ndone\n():\nself\n.\n_input_guardrails_task\n.\ncancel\n()\nif\nself\n.\n_output_guardrails_task\nand\nnot\nself\n.\n_output_guardrails_task\n.\ndone\n():\nself\n.\n_output_guardrails_task\n.\ncancel\n()\ndef\n__str__\n(\nself\n)\n->\nstr\n:\nreturn\npretty_print_run_result_streaming\n(\nself\n)\ncurrent_agent\ninstance-attribute\ncurrent_agent\n:\nAgent\n[\nAny\n]\nThe current agent that is running.\ncurrent_turn\ninstance-attribute\ncurrent_turn\n:\nint\nThe current turn number.\nmax_turns\ninstance-attribute\nmax_turns\n:\nint\nThe maximum number of turns the agent can run for.\nfinal_output\ninstance-attribute\nfinal_output\n:\nAny\nThe final output of the agent. This is None until the agent has finished running.\nis_complete\nclass-attribute\ninstance-attribute\nis_complete\n:\nbool\n=\nFalse\nWhether the agent has finished running.\nlast_agent\nproperty\nlast_agent\n:\nAgent\n[\nAny\n]\nThe last agent that was run. Updates as the agent run progresses, so the true last agent\nis only available after the agent run is complete.\ninput\ninstance-attribute\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe original input items i.e. the items before run() was called. This may be a mutated\nversion of the input, if there are handoff input filters that mutate the input.\nnew_items\ninstance-attribute\nnew_items\n:\nlist\n[\nRunItem\n]\nThe new items generated during the agent run. These include things like new messages, tool\ncalls and their outputs, etc.\nraw_responses\ninstance-attribute\nraw_responses\n:\nlist\n[\nModelResponse\n]\nThe raw LLM responses generated by the model during the agent run.\ninput_guardrail_results\ninstance-attribute\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\nGuardrail results for the input messages.\noutput_guardrail_results\ninstance-attribute\noutput_guardrail_results\n:\nlist\n[\nOutputGuardrailResult\n]\nGuardrail results for the final output of the agent.\nlast_response_id\nproperty\nlast_response_id\n:\nstr\n|\nNone\nConvenience method to get the response ID of the last model response.\nstream_events\nasync\nstream_events\n()\n->\nAsyncIterator\n[\nStreamEvent\n]\nStream deltas for new items as they are generated. We're using the types from the\nOpenAI Responses API, so these are semantic events: each event has a\ntype\nfield that\ndescribes the type of the event, along with the data for that event.\nThis will raise:\n- A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n- A GuardrailTripwireTriggered exception if a guardrail is tripped.\nSource code in\nsrc/agents/result.py\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\nasync\ndef\nstream_events\n(\nself\n)\n->\nAsyncIterator\n[\nStreamEvent\n]:\n\"\"\"Stream deltas for new items as they are generated. We're using the types from the\nOpenAI Responses API, so these are semantic events: each event has a `type` field that\ndescribes the type of the event, along with the data for that event.\nThis will raise:\n- A MaxTurnsExceeded exception if the agent exceeds the max_turns limit.\n- A GuardrailTripwireTriggered exception if a guardrail is tripped.\n\"\"\"\nwhile\nTrue\n:\nself\n.\n_check_errors\n()\nif\nself\n.\n_stored_exception\n:\nlogger\n.\ndebug\n(\n\"Breaking due to stored exception\"\n)\nself\n.\nis_complete\n=\nTrue\nbreak\nif\nself\n.\nis_complete\nand\nself\n.\n_event_queue\n.\nempty\n():\nbreak\ntry\n:\nitem\n=\nawait\nself\n.\n_event_queue\n.\nget\n()\nexcept\nasyncio\n.\nCancelledError\n:\nbreak\nif\nisinstance\n(\nitem\n,\nQueueCompleteSentinel\n):\nself\n.\n_event_queue\n.\ntask_done\n()\n# Check for errors, in case the queue was completed due to an exception\nself\n.\n_check_errors\n()\nbreak\nyield\nitem\nself\n.\n_event_queue\n.\ntask_done\n()\nif\nself\n.\n_trace\n:\nself\n.\n_trace\n.\nfinish\n(\nreset_current\n=\nTrue\n)\nself\n.\n_cleanup_tasks\n()\nif\nself\n.\n_stored_exception\n:\nraise\nself\n.\n_stored_exception\nfinal_output_as\nfinal_output_as\n(\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\nA convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set\nraise_if_incorrect_type\nto True, we'll raise a\nTypeError if the final output is not of the given type.\nParameters:\nName\nType\nDescription\nDefault\ncls\ntype\n[\nT\n]\nThe type to cast the final output to.\nrequired\nraise_if_incorrect_type\nbool\nIf True, we'll raise a TypeError if the final output is not of\nthe given type.\nFalse\nReturns:\nType\nDescription\nT\nThe final output casted to the given type.\nSource code in\nsrc/agents/result.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\ndef\nfinal_output_as\n(\nself\n,\ncls\n:\ntype\n[\nT\n],\nraise_if_incorrect_type\n:\nbool\n=\nFalse\n)\n->\nT\n:\n\"\"\"A convenience method to cast the final output to a specific type. By default, the cast\nis only for the typechecker. If you set `raise_if_incorrect_type` to True, we'll raise a\nTypeError if the final output is not of the given type.\nArgs:\ncls: The type to cast the final output to.\nraise_if_incorrect_type: If True, we'll raise a TypeError if the final output is not of\nthe given type.\nReturns:\nThe final output casted to the given type.\n\"\"\"\nif\nraise_if_incorrect_type\nand\nnot\nisinstance\n(\nself\n.\nfinal_output\n,\ncls\n):\nraise\nTypeError\n(\nf\n\"Final output is not of type\n{\ncls\n.\n__name__\n}\n\"\n)\nreturn\ncast\n(\nT\n,\nself\n.\nfinal_output\n)\nto_input_list\nto_input_list\n()\n->\nlist\n[\nTResponseInputItem\n]\nCreates a new input list, merging the original input with all the new items generated.\nSource code in\nsrc/agents/result.py\n76\n77\n78\n79\n80\n81\ndef\nto_input_list\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Creates a new input list, merging the original input with all the new items generated.\"\"\"\noriginal_items\n:\nlist\n[\nTResponseInputItem\n]\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\nself\n.\ninput\n)\nnew_items\n=\n[\nitem\n.\nto_input_item\n()\nfor\nitem\nin\nself\n.\nnew_items\n]\nreturn\noriginal_items\n+\nnew_items",
  "Streaming": "Streaming\nStreaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.\nTo stream, you can call\nRunner.run_streamed()\n, which will give you a\nRunResultStreaming\n. Calling\nresult.stream_events()\ngives you an async stream of\nStreamEvent\nobjects, which are described below.\nRaw response events\nRawResponsesStreamEvent\nare raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like\nresponse.created\n,\nresponse.output_text.delta\n, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.\nFor example, this will output the text generated by the LLM token-by-token.\nimport\nasyncio\nfrom\nopenai.types.responses\nimport\nResponseTextDeltaEvent\nfrom\nagents\nimport\nAgent\n,\nRunner\nasync\ndef\nmain\n():\nagent\n=\nAgent\n(\nname\n=\n\"Joker\"\n,\ninstructions\n=\n\"You are a helpful assistant.\"\n,\n)\nresult\n=\nRunner\n.\nrun_streamed\n(\nagent\n,\ninput\n=\n\"Please tell me 5 jokes.\"\n)\nasync\nfor\nevent\nin\nresult\n.\nstream_events\n():\nif\nevent\n.\ntype\n==\n\"raw_response_event\"\nand\nisinstance\n(\nevent\n.\ndata\n,\nResponseTextDeltaEvent\n):\nprint\n(\nevent\n.\ndata\n.\ndelta\n,\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nRun item events and agent events\nRunItemStreamEvent\ns are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of \"message generated\", \"tool ran\", etc, instead of each token. Similarly,\nAgentUpdatedStreamEvent\ngives you updates when the current agent changes (e.g. as the result of a handoff).\nFor example, this will ignore raw events and stream updates to the user.\nimport\nasyncio\nimport\nrandom\nfrom\nagents\nimport\nAgent\n,\nItemHelpers\n,\nRunner\n,\nfunction_tool\n@function_tool\ndef\nhow_many_jokes\n()\n->\nint\n:\nreturn\nrandom\n.\nrandint\n(\n1\n,\n10\n)\nasync\ndef\nmain\n():\nagent\n=\nAgent\n(\nname\n=\n\"Joker\"\n,\ninstructions\n=\n\"First call the `how_many_jokes` tool, then tell that many jokes.\"\n,\ntools\n=\n[\nhow_many_jokes\n],\n)\nresult\n=\nRunner\n.\nrun_streamed\n(\nagent\n,\ninput\n=\n\"Hello\"\n,\n)\nprint\n(\n\"=== Run starting ===\"\n)\nasync\nfor\nevent\nin\nresult\n.\nstream_events\n():\n# We'll ignore the raw responses event deltas\nif\nevent\n.\ntype\n==\n\"raw_response_event\"\n:\ncontinue\n# When the agent updates, print that\nelif\nevent\n.\ntype\n==\n\"agent_updated_stream_event\"\n:\nprint\n(\nf\n\"Agent updated:\n{\nevent\n.\nnew_agent\n.\nname\n}\n\"\n)\ncontinue\n# When items are generated, print them\nelif\nevent\n.\ntype\n==\n\"run_item_stream_event\"\n:\nif\nevent\n.\nitem\n.\ntype\n==\n\"tool_call_item\"\n:\nprint\n(\n\"-- Tool was called\"\n)\nelif\nevent\n.\nitem\n.\ntype\n==\n\"tool_call_output_item\"\n:\nprint\n(\nf\n\"-- Tool output:\n{\nevent\n.\nitem\n.\noutput\n}\n\"\n)\nelif\nevent\n.\nitem\n.\ntype\n==\n\"message_output_item\"\n:\nprint\n(\nf\n\"-- Message output:\n\\n\n{\nItemHelpers\n.\ntext_message_output\n(\nevent\n.\nitem\n)\n}\n\"\n)\nelse\n:\npass\n# Ignore other event types\nprint\n(\n\"=== Run complete ===\"\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())",
  "Tools": "Tools\nTool\nmodule-attribute\nTool\n=\nUnion\n[\nFunctionTool\n,\nFileSearchTool\n,\nWebSearchTool\n,\nComputerTool\n,\n]\nA tool that can be used in an agent.\nFunctionToolResult\ndataclass\nSource code in\nsrc/agents/tool.py\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n@dataclass\nclass\nFunctionToolResult\n:\ntool\n:\nFunctionTool\n\"\"\"The tool that was run.\"\"\"\noutput\n:\nAny\n\"\"\"The output of the tool.\"\"\"\nrun_item\n:\nRunItem\n\"\"\"The run item that was produced as a result of the tool call.\"\"\"\ntool\ninstance-attribute\ntool\n:\nFunctionTool\nThe tool that was run.\noutput\ninstance-attribute\noutput\n:\nAny\nThe output of the tool.\nrun_item\ninstance-attribute\nrun_item\n:\nRunItem\nThe run item that was produced as a result of the tool call.\nFunctionTool\ndataclass\nA tool that wraps a function. In most cases, you should use  the\nfunction_tool\nhelpers to\ncreate a FunctionTool, as they let you easily wrap a Python function.\nSource code in\nsrc/agents/tool.py\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n@dataclass\nclass\nFunctionTool\n:\n\"\"\"A tool that wraps a function. In most cases, you should use  the `function_tool` helpers to\ncreate a FunctionTool, as they let you easily wrap a Python function.\n\"\"\"\nname\n:\nstr\n\"\"\"The name of the tool, as shown to the LLM. Generally the name of the function.\"\"\"\ndescription\n:\nstr\n\"\"\"A description of the tool, as shown to the LLM.\"\"\"\nparams_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\n\"\"\"The JSON schema for the tool's parameters.\"\"\"\non_invoke_tool\n:\nCallable\n[[\nRunContextWrapper\n[\nAny\n],\nstr\n],\nAwaitable\n[\nAny\n]]\n\"\"\"A function that invokes the tool with the given context and parameters. The params passed\nare:\n1. The tool run context.\n2. The arguments from the LLM, as a JSON string.\nYou must return a string representation of the tool output, or something we can call `str()` on.\nIn case of errors, you can either raise an Exception (which will cause the run to fail) or\nreturn a string error message (which will be sent back to the LLM).\n\"\"\"\nstrict_json_schema\n:\nbool\n=\nTrue\n\"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\nas it increases the likelihood of correct JSON input.\"\"\"\nname\ninstance-attribute\nname\n:\nstr\nThe name of the tool, as shown to the LLM. Generally the name of the function.\ndescription\ninstance-attribute\ndescription\n:\nstr\nA description of the tool, as shown to the LLM.\nparams_json_schema\ninstance-attribute\nparams_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\nThe JSON schema for the tool's parameters.\non_invoke_tool\ninstance-attribute\non_invoke_tool\n:\nCallable\n[\n[\nRunContextWrapper\n[\nAny\n],\nstr\n],\nAwaitable\n[\nAny\n]\n]\nA function that invokes the tool with the given context and parameters. The params passed\nare:\n1. The tool run context.\n2. The arguments from the LLM, as a JSON string.\nYou must return a string representation of the tool output, or something we can call\nstr()\non.\nIn case of errors, you can either raise an Exception (which will cause the run to fail) or\nreturn a string error message (which will be sent back to the LLM).\nstrict_json_schema\nclass-attribute\ninstance-attribute\nstrict_json_schema\n:\nbool\n=\nTrue\nWhether the JSON schema is in strict mode. We\nstrongly\nrecommend setting this to True,\nas it increases the likelihood of correct JSON input.\nFileSearchTool\ndataclass\nA hosted tool that lets the LLM search through a vector store. Currently only supported with\nOpenAI models, using the Responses API.\nSource code in\nsrc/agents/tool.py\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n@dataclass\nclass\nFileSearchTool\n:\n\"\"\"A hosted tool that lets the LLM search through a vector store. Currently only supported with\nOpenAI models, using the Responses API.\n\"\"\"\nvector_store_ids\n:\nlist\n[\nstr\n]\n\"\"\"The IDs of the vector stores to search.\"\"\"\nmax_num_results\n:\nint\n|\nNone\n=\nNone\n\"\"\"The maximum number of results to return.\"\"\"\ninclude_search_results\n:\nbool\n=\nFalse\n\"\"\"Whether to include the search results in the output produced by the LLM.\"\"\"\nranking_options\n:\nRankingOptions\n|\nNone\n=\nNone\n\"\"\"Ranking options for search.\"\"\"\nfilters\n:\nFilters\n|\nNone\n=\nNone\n\"\"\"A filter to apply based on file attributes.\"\"\"\n@property\ndef\nname\n(\nself\n):\nreturn\n\"file_search\"\nvector_store_ids\ninstance-attribute\nvector_store_ids\n:\nlist\n[\nstr\n]\nThe IDs of the vector stores to search.\nmax_num_results\nclass-attribute\ninstance-attribute\nmax_num_results\n:\nint\n|\nNone\n=\nNone\nThe maximum number of results to return.\ninclude_search_results\nclass-attribute\ninstance-attribute\ninclude_search_results\n:\nbool\n=\nFalse\nWhether to include the search results in the output produced by the LLM.\nranking_options\nclass-attribute\ninstance-attribute\nranking_options\n:\nRankingOptions\n|\nNone\n=\nNone\nRanking options for search.\nfilters\nclass-attribute\ninstance-attribute\nfilters\n:\nFilters\n|\nNone\n=\nNone\nA filter to apply based on file attributes.\nWebSearchTool\ndataclass\nA hosted tool that lets the LLM search the web. Currently only supported with OpenAI models,\nusing the Responses API.\nSource code in\nsrc/agents/tool.py\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n@dataclass\nclass\nWebSearchTool\n:\n\"\"\"A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models,\nusing the Responses API.\n\"\"\"\nuser_location\n:\nUserLocation\n|\nNone\n=\nNone\n\"\"\"Optional location for the search. Lets you customize results to be relevant to a location.\"\"\"\nsearch_context_size\n:\nLiteral\n[\n\"low\"\n,\n\"medium\"\n,\n\"high\"\n]\n=\n\"medium\"\n\"\"\"The amount of context to use for the search.\"\"\"\n@property\ndef\nname\n(\nself\n):\nreturn\n\"web_search_preview\"\nuser_location\nclass-attribute\ninstance-attribute\nuser_location\n:\nUserLocation\n|\nNone\n=\nNone\nOptional location for the search. Lets you customize results to be relevant to a location.\nsearch_context_size\nclass-attribute\ninstance-attribute\nsearch_context_size\n:\nLiteral\n[\n\"low\"\n,\n\"medium\"\n,\n\"high\"\n]\n=\n(\n\"medium\"\n)\nThe amount of context to use for the search.\nComputerTool\ndataclass\nA hosted tool that lets the LLM control a computer.\nSource code in\nsrc/agents/tool.py\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n@dataclass\nclass\nComputerTool\n:\n\"\"\"A hosted tool that lets the LLM control a computer.\"\"\"\ncomputer\n:\nComputer\n|\nAsyncComputer\n\"\"\"The computer implementation, which describes the environment and dimensions of the computer,\nas well as implements the computer actions like click, screenshot, etc.\n\"\"\"\n@property\ndef\nname\n(\nself\n):\nreturn\n\"computer_use_preview\"\ncomputer\ninstance-attribute\ncomputer\n:\nComputer\n|\nAsyncComputer\nThe computer implementation, which describes the environment and dimensions of the computer,\nas well as implements the computer actions like click, screenshot, etc.\ndefault_tool_error_function\ndefault_tool_error_function\n(\nctx\n:\nRunContextWrapper\n[\nAny\n],\nerror\n:\nException\n)\n->\nstr\nThe default tool error function, which just returns a generic error message.\nSource code in\nsrc/agents/tool.py\n137\n138\n139\ndef\ndefault_tool_error_function\n(\nctx\n:\nRunContextWrapper\n[\nAny\n],\nerror\n:\nException\n)\n->\nstr\n:\n\"\"\"The default tool error function, which just returns a generic error message.\"\"\"\nreturn\nf\n\"An error occurred while running the tool. Please try again. Error:\n{\nstr\n(\nerror\n)\n}\n\"\nfunction_tool\nfunction_tool\n(\nfunc\n:\nToolFunction\n[\n...\n],\n*\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nfailure_error_function\n:\nToolErrorFunction\n|\nNone\n=\nNone\n,\nstrict_mode\n:\nbool\n=\nTrue\n,\n)\n->\nFunctionTool\nfunction_tool\n(\n*\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nfailure_error_function\n:\nToolErrorFunction\n|\nNone\n=\nNone\n,\nstrict_mode\n:\nbool\n=\nTrue\n,\n)\n->\nCallable\n[[\nToolFunction\n[\n...\n]],\nFunctionTool\n]\nfunction_tool\n(\nfunc\n:\nToolFunction\n[\n...\n]\n|\nNone\n=\nNone\n,\n*\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nfailure_error_function\n:\nToolErrorFunction\n|\nNone\n=\ndefault_tool_error_function\n,\nstrict_mode\n:\nbool\n=\nTrue\n,\n)\n->\n(\nFunctionTool\n|\nCallable\n[[\nToolFunction\n[\n...\n]],\nFunctionTool\n]\n)\nDecorator to create a FunctionTool from a function. By default, we will:\n1. Parse the function signature to create a JSON schema for the tool's parameters.\n2. Use the function's docstring to populate the tool's description.\n3. Use the function's docstring to populate argument descriptions.\nThe docstring style is detected automatically, but you can override it.\nIf the function takes a\nRunContextWrapper\nas the first argument, it\nmust\nmatch the\ncontext type of the agent that uses the tool.\nParameters:\nName\nType\nDescription\nDefault\nfunc\nToolFunction\n[...] | None\nThe function to wrap.\nNone\nname_override\nstr\n| None\nIf provided, use this name for the tool instead of the function's name.\nNone\ndescription_override\nstr\n| None\nIf provided, use this description for the tool instead of the\nfunction's docstring.\nNone\ndocstring_style\nDocstringStyle\n| None\nIf provided, use this style for the tool's docstring. If not provided,\nwe will attempt to auto-detect the style.\nNone\nuse_docstring_info\nbool\nIf True, use the function's docstring to populate the tool's\ndescription and argument descriptions.\nTrue\nfailure_error_function\nToolErrorFunction\n| None\nIf provided, use this function to generate an error message when\nthe tool call fails. The error message is sent to the LLM. If you pass None, then no\nerror message will be sent and instead an Exception will be raised.\ndefault_tool_error_function\nstrict_mode\nbool\nWhether to enable strict mode for the tool's JSON schema. We\nstrongly\nrecommend setting this to True, as it increases the likelihood of correct JSON input.\nIf False, it allows non-strict JSON schemas. For example, if a parameter has a default\nvalue, it will be optional, additional properties are allowed, etc. See here for more:\nhttps://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas\nTrue\nSource code in\nsrc/agents/tool.py\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\ndef\nfunction_tool\n(\nfunc\n:\nToolFunction\n[\n...\n]\n|\nNone\n=\nNone\n,\n*\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nfailure_error_function\n:\nToolErrorFunction\n|\nNone\n=\ndefault_tool_error_function\n,\nstrict_mode\n:\nbool\n=\nTrue\n,\n)\n->\nFunctionTool\n|\nCallable\n[[\nToolFunction\n[\n...\n]],\nFunctionTool\n]:\n\"\"\"\nDecorator to create a FunctionTool from a function. By default, we will:\n1. Parse the function signature to create a JSON schema for the tool's parameters.\n2. Use the function's docstring to populate the tool's description.\n3. Use the function's docstring to populate argument descriptions.\nThe docstring style is detected automatically, but you can override it.\nIf the function takes a `RunContextWrapper` as the first argument, it *must* match the\ncontext type of the agent that uses the tool.\nArgs:\nfunc: The function to wrap.\nname_override: If provided, use this name for the tool instead of the function's name.\ndescription_override: If provided, use this description for the tool instead of the\nfunction's docstring.\ndocstring_style: If provided, use this style for the tool's docstring. If not provided,\nwe will attempt to auto-detect the style.\nuse_docstring_info: If True, use the function's docstring to populate the tool's\ndescription and argument descriptions.\nfailure_error_function: If provided, use this function to generate an error message when\nthe tool call fails. The error message is sent to the LLM. If you pass None, then no\nerror message will be sent and instead an Exception will be raised.\nstrict_mode: Whether to enable strict mode for the tool's JSON schema. We *strongly*\nrecommend setting this to True, as it increases the likelihood of correct JSON input.\nIf False, it allows non-strict JSON schemas. For example, if a parameter has a default\nvalue, it will be optional, additional properties are allowed, etc. See here for more:\nhttps://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas\n\"\"\"\ndef\n_create_function_tool\n(\nthe_func\n:\nToolFunction\n[\n...\n])\n->\nFunctionTool\n:\nschema\n=\nfunction_schema\n(\nfunc\n=\nthe_func\n,\nname_override\n=\nname_override\n,\ndescription_override\n=\ndescription_override\n,\ndocstring_style\n=\ndocstring_style\n,\nuse_docstring_info\n=\nuse_docstring_info\n,\nstrict_json_schema\n=\nstrict_mode\n,\n)\nasync\ndef\n_on_invoke_tool_impl\n(\nctx\n:\nRunContextWrapper\n[\nAny\n],\ninput\n:\nstr\n)\n->\nAny\n:\ntry\n:\njson_data\n:\ndict\n[\nstr\n,\nAny\n]\n=\njson\n.\nloads\n(\ninput\n)\nif\ninput\nelse\n{}\nexcept\nException\nas\ne\n:\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\nschema\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\nschema\n.\nname\n}\n:\n{\ninput\n}\n\"\n)\nraise\nModelBehaviorError\n(\nf\n\"Invalid JSON input for tool\n{\nschema\n.\nname\n}\n:\n{\ninput\n}\n\"\n)\nfrom\ne\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking tool\n{\nschema\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking tool\n{\nschema\n.\nname\n}\nwith input\n{\ninput\n}\n\"\n)\ntry\n:\nparsed\n=\n(\nschema\n.\nparams_pydantic_model\n(\n**\njson_data\n)\nif\njson_data\nelse\nschema\n.\nparams_pydantic_model\n()\n)\nexcept\nValidationError\nas\ne\n:\nraise\nModelBehaviorError\n(\nf\n\"Invalid JSON input for tool\n{\nschema\n.\nname\n}\n:\n{\ne\n}\n\"\n)\nfrom\ne\nargs\n,\nkwargs_dict\n=\nschema\n.\nto_call_args\n(\nparsed\n)\nif\nnot\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Tool call args:\n{\nargs\n}\n, kwargs:\n{\nkwargs_dict\n}\n\"\n)\nif\ninspect\n.\niscoroutinefunction\n(\nthe_func\n):\nif\nschema\n.\ntakes_context\n:\nresult\n=\nawait\nthe_func\n(\nctx\n,\n*\nargs\n,\n**\nkwargs_dict\n)\nelse\n:\nresult\n=\nawait\nthe_func\n(\n*\nargs\n,\n**\nkwargs_dict\n)\nelse\n:\nif\nschema\n.\ntakes_context\n:\nresult\n=\nthe_func\n(\nctx\n,\n*\nargs\n,\n**\nkwargs_dict\n)\nelse\n:\nresult\n=\nthe_func\n(\n*\nargs\n,\n**\nkwargs_dict\n)\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Tool\n{\nschema\n.\nname\n}\ncompleted.\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Tool\n{\nschema\n.\nname\n}\nreturned\n{\nresult\n}\n\"\n)\nreturn\nresult\nasync\ndef\n_on_invoke_tool\n(\nctx\n:\nRunContextWrapper\n[\nAny\n],\ninput\n:\nstr\n)\n->\nAny\n:\ntry\n:\nreturn\nawait\n_on_invoke_tool_impl\n(\nctx\n,\ninput\n)\nexcept\nException\nas\ne\n:\nif\nfailure_error_function\nis\nNone\n:\nraise\nresult\n=\nfailure_error_function\n(\nctx\n,\ne\n)\nif\ninspect\n.\nisawaitable\n(\nresult\n):\nreturn\nawait\nresult\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Error running tool (non-fatal)\"\n,\ndata\n=\n{\n\"tool_name\"\n:\nschema\n.\nname\n,\n\"error\"\n:\nstr\n(\ne\n),\n},\n)\n)\nreturn\nresult\nreturn\nFunctionTool\n(\nname\n=\nschema\n.\nname\n,\ndescription\n=\nschema\n.\ndescription\nor\n\"\"\n,\nparams_json_schema\n=\nschema\n.\nparams_json_schema\n,\non_invoke_tool\n=\n_on_invoke_tool\n,\nstrict_json_schema\n=\nstrict_mode\n,\n)\n# If func is actually a callable, we were used as @function_tool with no parentheses\nif\ncallable\n(\nfunc\n):\nreturn\n_create_function_tool\n(\nfunc\n)\n# Otherwise, we were used as @function_tool(...), so return a decorator\ndef\ndecorator\n(\nreal_func\n:\nToolFunction\n[\n...\n])\n->\nFunctionTool\n:\nreturn\n_create_function_tool\n(\nreal_func\n)\nreturn\ndecorator",
  "Model context protocol (MCP)": "Model context protocol (MCP)\nThe\nModel context protocol\n(aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:\nMCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\nThe Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.\nMCP servers\nCurrently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:\nstdio\nservers run as a subprocess of your application. You can think of them as running \"locally\".\nHTTP over SSE\nservers run remotely. You connect to them via a URL.\nYou can use the\nMCPServerStdio\nand\nMCPServerSse\nclasses to connect to these servers.\nFor example, this is how you'd use the\nofficial MCP filesystem server\n.\nasync\nwith\nMCPServerStdio\n(\nparams\n=\n{\n\"command\"\n:\n\"npx\"\n,\n\"args\"\n:\n[\n\"-y\"\n,\n\"@modelcontextprotocol/server-filesystem\"\n,\nsamples_dir\n],\n}\n)\nas\nserver\n:\ntools\n=\nawait\nserver\n.\nlist_tools\n()\nUsing MCP servers\nMCP servers can be added to Agents. The Agents SDK will call\nlist_tools()\non the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls\ncall_tool()\non that server.\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"Use the tools to achieve the task\"\n,\nmcp_servers\n=\n[\nmcp_server_1\n,\nmcp_server_2\n]\n)\nCaching\nEvery time an Agent runs, it calls\nlist_tools()\non the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass\ncache_tools_list=True\nto both\nMCPServerStdio\nand\nMCPServerSse\n. You should only do this if you're certain the tool list will not change.\nIf you want to invalidate the cache, you can call\ninvalidate_tools_cache()\non the servers.\nEnd-to-end examples\nView complete working examples at\nexamples/mcp\n.\nTracing\nTracing\nautomatically captures MCP operations, including:\nCalls to the MCP server to list tools\nMCP-related info on function calls",
  "Handoffs": "Handoffs\nHandoffInputFilter\nmodule-attribute\nHandoffInputFilter\n:\nTypeAlias\n=\nCallable\n[\n[\nHandoffInputData\n],\nHandoffInputData\n]\nA function that filters the input data passed to the next agent.\nHandoffInputData\ndataclass\nSource code in\nsrc/agents/handoffs.py\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n@dataclass\n(\nfrozen\n=\nTrue\n)\nclass\nHandoffInputData\n:\ninput_history\n:\nstr\n|\ntuple\n[\nTResponseInputItem\n,\n...\n]\n\"\"\"\nThe input history before `Runner.run()` was called.\n\"\"\"\npre_handoff_items\n:\ntuple\n[\nRunItem\n,\n...\n]\n\"\"\"\nThe items generated before the agent turn where the handoff was invoked.\n\"\"\"\nnew_items\n:\ntuple\n[\nRunItem\n,\n...\n]\n\"\"\"\nThe new items generated during the current agent turn, including the item that triggered the\nhandoff and the tool output message representing the response from the handoff output.\n\"\"\"\ninput_history\ninstance-attribute\ninput_history\n:\nstr\n|\ntuple\n[\nTResponseInputItem\n,\n...\n]\nThe input history before\nRunner.run()\nwas called.\npre_handoff_items\ninstance-attribute\npre_handoff_items\n:\ntuple\n[\nRunItem\n,\n...\n]\nThe items generated before the agent turn where the handoff was invoked.\nnew_items\ninstance-attribute\nnew_items\n:\ntuple\n[\nRunItem\n,\n...\n]\nThe new items generated during the current agent turn, including the item that triggered the\nhandoff and the tool output message representing the response from the handoff output.\nHandoff\ndataclass\nBases:\nGeneric\n[\nTContext\n]\nA handoff is when an agent delegates a task to another agent.\nFor example, in a customer support scenario you might have a \"triage agent\" that determines\nwhich agent should handle the user's request, and sub-agents that specialize in different\nareas like billing, account management, etc.\nSource code in\nsrc/agents/handoffs.py\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n@dataclass\nclass\nHandoff\n(\nGeneric\n[\nTContext\n]):\n\"\"\"A handoff is when an agent delegates a task to another agent.\nFor example, in a customer support scenario you might have a \"triage agent\" that determines\nwhich agent should handle the user's request, and sub-agents that specialize in different\nareas like billing, account management, etc.\n\"\"\"\ntool_name\n:\nstr\n\"\"\"The name of the tool that represents the handoff.\"\"\"\ntool_description\n:\nstr\n\"\"\"The description of the tool that represents the handoff.\"\"\"\ninput_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\n\"\"\"The JSON schema for the handoff input. Can be empty if the handoff does not take an input.\n\"\"\"\non_invoke_handoff\n:\nCallable\n[[\nRunContextWrapper\n[\nAny\n],\nstr\n],\nAwaitable\n[\nAgent\n[\nTContext\n]]]\n\"\"\"The function that invokes the handoff. The parameters passed are:\n1. The handoff run context\n2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty.\nMust return an agent.\n\"\"\"\nagent_name\n:\nstr\n\"\"\"The name of the agent that is being handed off to.\"\"\"\ninput_filter\n:\nHandoffInputFilter\n|\nNone\n=\nNone\n\"\"\"A function that filters the inputs that are passed to the next agent. By default, the new\nagent sees the entire conversation history. In some cases, you may want to filter inputs e.g.\nto remove older inputs, or remove tools from existing inputs.\nThe function will receive the entire conversation history so far, including the input item\nthat triggered the handoff and a tool call output item representing the handoff tool's output.\nYou are free to modify the input history or new items as you see fit. The next agent that\nruns will receive `handoff_input_data.all_items`.\nIMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\nitems generated before will already have been streamed.\n\"\"\"\nstrict_json_schema\n:\nbool\n=\nTrue\n\"\"\"Whether the input JSON schema is in strict mode. We **strongly** recommend setting this to\nTrue, as it increases the likelihood of correct JSON input.\n\"\"\"\ndef\nget_transfer_message\n(\nself\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nstr\n:\nbase\n=\nf\n\"\n{{\n'assistant': '\n{\nagent\n.\nname\n}\n'\n}}\n\"\nreturn\nbase\n@classmethod\ndef\ndefault_tool_name\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nstr\n:\nreturn\n_transforms\n.\ntransform_string_function_style\n(\nf\n\"transfer_to_\n{\nagent\n.\nname\n}\n\"\n)\n@classmethod\ndef\ndefault_tool_description\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nstr\n:\nreturn\n(\nf\n\"Handoff to the\n{\nagent\n.\nname\n}\nagent to handle the request. \"\nf\n\"\n{\nagent\n.\nhandoff_description\nor\n''\n}\n\"\n)\ntool_name\ninstance-attribute\ntool_name\n:\nstr\nThe name of the tool that represents the handoff.\ntool_description\ninstance-attribute\ntool_description\n:\nstr\nThe description of the tool that represents the handoff.\ninput_json_schema\ninstance-attribute\ninput_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\nThe JSON schema for the handoff input. Can be empty if the handoff does not take an input.\non_invoke_handoff\ninstance-attribute\non_invoke_handoff\n:\nCallable\n[\n[\nRunContextWrapper\n[\nAny\n],\nstr\n],\nAwaitable\n[\nAgent\n[\nTContext\n]],\n]\nThe function that invokes the handoff. The parameters passed are:\n1. The handoff run context\n2. The arguments from the LLM, as a JSON string. Empty string if input_json_schema is empty.\nMust return an agent.\nagent_name\ninstance-attribute\nagent_name\n:\nstr\nThe name of the agent that is being handed off to.\ninput_filter\nclass-attribute\ninstance-attribute\ninput_filter\n:\nHandoffInputFilter\n|\nNone\n=\nNone\nA function that filters the inputs that are passed to the next agent. By default, the new\nagent sees the entire conversation history. In some cases, you may want to filter inputs e.g.\nto remove older inputs, or remove tools from existing inputs.\nThe function will receive the entire conversation history so far, including the input item\nthat triggered the handoff and a tool call output item representing the handoff tool's output.\nYou are free to modify the input history or new items as you see fit. The next agent that\nruns will receive\nhandoff_input_data.all_items\n.\nIMPORTANT: in streaming mode, we will not stream anything as a result of this function. The\nitems generated before will already have been streamed.\nstrict_json_schema\nclass-attribute\ninstance-attribute\nstrict_json_schema\n:\nbool\n=\nTrue\nWhether the input JSON schema is in strict mode. We\nstrongly\nrecommend setting this to\nTrue, as it increases the likelihood of correct JSON input.\nhandoff\nhandoff\n(\nagent\n:\nAgent\n[\nTContext\n],\n*\n,\ntool_name_override\n:\nstr\n|\nNone\n=\nNone\n,\ntool_description_override\n:\nstr\n|\nNone\n=\nNone\n,\ninput_filter\n:\nCallable\n[\n[\nHandoffInputData\n],\nHandoffInputData\n]\n|\nNone\n=\nNone\n,\n)\n->\nHandoff\n[\nTContext\n]\nhandoff\n(\nagent\n:\nAgent\n[\nTContext\n],\n*\n,\non_handoff\n:\nOnHandoffWithInput\n[\nTHandoffInput\n],\ninput_type\n:\ntype\n[\nTHandoffInput\n],\ntool_description_override\n:\nstr\n|\nNone\n=\nNone\n,\ntool_name_override\n:\nstr\n|\nNone\n=\nNone\n,\ninput_filter\n:\nCallable\n[\n[\nHandoffInputData\n],\nHandoffInputData\n]\n|\nNone\n=\nNone\n,\n)\n->\nHandoff\n[\nTContext\n]\nhandoff\n(\nagent\n:\nAgent\n[\nTContext\n],\n*\n,\non_handoff\n:\nOnHandoffWithoutInput\n,\ntool_description_override\n:\nstr\n|\nNone\n=\nNone\n,\ntool_name_override\n:\nstr\n|\nNone\n=\nNone\n,\ninput_filter\n:\nCallable\n[\n[\nHandoffInputData\n],\nHandoffInputData\n]\n|\nNone\n=\nNone\n,\n)\n->\nHandoff\n[\nTContext\n]\nhandoff\n(\nagent\n:\nAgent\n[\nTContext\n],\ntool_name_override\n:\nstr\n|\nNone\n=\nNone\n,\ntool_description_override\n:\nstr\n|\nNone\n=\nNone\n,\non_handoff\n:\nOnHandoffWithInput\n[\nTHandoffInput\n]\n|\nOnHandoffWithoutInput\n|\nNone\n=\nNone\n,\ninput_type\n:\ntype\n[\nTHandoffInput\n]\n|\nNone\n=\nNone\n,\ninput_filter\n:\nCallable\n[\n[\nHandoffInputData\n],\nHandoffInputData\n]\n|\nNone\n=\nNone\n,\n)\n->\nHandoff\n[\nTContext\n]\nCreate a handoff from an agent.\nParameters:\nName\nType\nDescription\nDefault\nagent\nAgent\n[\nTContext\n]\nThe agent to handoff to, or a function that returns an agent.\nrequired\ntool_name_override\nstr\n| None\nOptional override for the name of the tool that represents the handoff.\nNone\ntool_description_override\nstr\n| None\nOptional override for the description of the tool that\nrepresents the handoff.\nNone\non_handoff\nOnHandoffWithInput\n[\nTHandoffInput\n] |\nOnHandoffWithoutInput\n| None\nA function that runs when the handoff is invoked.\nNone\ninput_type\ntype\n[\nTHandoffInput\n] | None\nthe type of the input to the handoff. If provided, the input will be validated\nagainst this type. Only relevant if you pass a function that takes an input.\nNone\ninput_filter\nCallable\n[[\nHandoffInputData\n],\nHandoffInputData\n] | None\na function that filters the inputs that are passed to the next agent.\nNone\nSource code in\nsrc/agents/handoffs.py\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\ndef\nhandoff\n(\nagent\n:\nAgent\n[\nTContext\n],\ntool_name_override\n:\nstr\n|\nNone\n=\nNone\n,\ntool_description_override\n:\nstr\n|\nNone\n=\nNone\n,\non_handoff\n:\nOnHandoffWithInput\n[\nTHandoffInput\n]\n|\nOnHandoffWithoutInput\n|\nNone\n=\nNone\n,\ninput_type\n:\ntype\n[\nTHandoffInput\n]\n|\nNone\n=\nNone\n,\ninput_filter\n:\nCallable\n[[\nHandoffInputData\n],\nHandoffInputData\n]\n|\nNone\n=\nNone\n,\n)\n->\nHandoff\n[\nTContext\n]:\n\"\"\"Create a handoff from an agent.\nArgs:\nagent: The agent to handoff to, or a function that returns an agent.\ntool_name_override: Optional override for the name of the tool that represents the handoff.\ntool_description_override: Optional override for the description of the tool that\nrepresents the handoff.\non_handoff: A function that runs when the handoff is invoked.\ninput_type: the type of the input to the handoff. If provided, the input will be validated\nagainst this type. Only relevant if you pass a function that takes an input.\ninput_filter: a function that filters the inputs that are passed to the next agent.\n\"\"\"\nassert\n(\non_handoff\nand\ninput_type\n)\nor\nnot\n(\non_handoff\nand\ninput_type\n),\n(\n\"You must provide either both on_input and input_type, or neither\"\n)\ntype_adapter\n:\nTypeAdapter\n[\nAny\n]\n|\nNone\nif\ninput_type\nis\nnot\nNone\n:\nassert\ncallable\n(\non_handoff\n),\n\"on_handoff must be callable\"\nsig\n=\ninspect\n.\nsignature\n(\non_handoff\n)\nif\nlen\n(\nsig\n.\nparameters\n)\n!=\n2\n:\nraise\nUserError\n(\n\"on_handoff must take two arguments: context and input\"\n)\ntype_adapter\n=\nTypeAdapter\n(\ninput_type\n)\ninput_json_schema\n=\ntype_adapter\n.\njson_schema\n()\nelse\n:\ntype_adapter\n=\nNone\ninput_json_schema\n=\n{}\nif\non_handoff\nis\nnot\nNone\n:\nsig\n=\ninspect\n.\nsignature\n(\non_handoff\n)\nif\nlen\n(\nsig\n.\nparameters\n)\n!=\n1\n:\nraise\nUserError\n(\n\"on_handoff must take one argument: context\"\n)\nasync\ndef\n_invoke_handoff\n(\nctx\n:\nRunContextWrapper\n[\nAny\n],\ninput_json\n:\nstr\n|\nNone\n=\nNone\n)\n->\nAgent\n[\nAny\n]:\nif\ninput_type\nis\nnot\nNone\nand\ntype_adapter\nis\nnot\nNone\n:\nif\ninput_json\nis\nNone\n:\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Handoff function expected non-null input, but got None\"\n,\ndata\n=\n{\n\"details\"\n:\n\"input_json is None\"\n},\n)\n)\nraise\nModelBehaviorError\n(\n\"Handoff function expected non-null input, but got None\"\n)\nvalidated_input\n=\n_json\n.\nvalidate_json\n(\njson_str\n=\ninput_json\n,\ntype_adapter\n=\ntype_adapter\n,\npartial\n=\nFalse\n,\n)\ninput_func\n=\ncast\n(\nOnHandoffWithInput\n[\nTHandoffInput\n],\non_handoff\n)\nif\ninspect\n.\niscoroutinefunction\n(\ninput_func\n):\nawait\ninput_func\n(\nctx\n,\nvalidated_input\n)\nelse\n:\ninput_func\n(\nctx\n,\nvalidated_input\n)\nelif\non_handoff\nis\nnot\nNone\n:\nno_input_func\n=\ncast\n(\nOnHandoffWithoutInput\n,\non_handoff\n)\nif\ninspect\n.\niscoroutinefunction\n(\nno_input_func\n):\nawait\nno_input_func\n(\nctx\n)\nelse\n:\nno_input_func\n(\nctx\n)\nreturn\nagent\ntool_name\n=\ntool_name_override\nor\nHandoff\n.\ndefault_tool_name\n(\nagent\n)\ntool_description\n=\ntool_description_override\nor\nHandoff\n.\ndefault_tool_description\n(\nagent\n)\n# Always ensure the input JSON schema is in strict mode\n# If there is a need, we can make this configurable in the future\ninput_json_schema\n=\nensure_strict_json_schema\n(\ninput_json_schema\n)\nreturn\nHandoff\n(\ntool_name\n=\ntool_name\n,\ntool_description\n=\ntool_description\n,\ninput_json_schema\n=\ninput_json_schema\n,\non_invoke_handoff\n=\n_invoke_handoff\n,\ninput_filter\n=\ninput_filter\n,\nagent_name\n=\nagent\n.\nname\n,\n)",
  "Tracing": "Tracing\nJust like the way\nagents are traced\n, voice pipelines are also automatically traced.\nYou can read the tracing doc above for basic tracing information, but you can additionally configure tracing of a pipeline via\nVoicePipelineConfig\n.\nKey tracing related fields are:\ntracing_disabled\n: controls whether tracing is disabled. By default, tracing is enabled.\ntrace_include_sensitive_data\n: controls whether traces include potentially sensitive data, like audio transcripts. This is specifically for the voice pipeline, and not for anything that goes on inside your Workflow.\ntrace_include_sensitive_audio_data\n: controls whether traces include audio data.\nworkflow_name\n: The name of the trace workflow.\ngroup_id\n: The\ngroup_id\nof the trace, which lets you link multiple traces.\ntrace_metadata\n: Additional metadata to include with the trace.",
  "Context management": "Context management\nContext is an overloaded term. There are two main classes of context you might care about:\nContext available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like\non_handoff\n, in lifecycle hooks, etc.\nContext available to LLMs: this is data the LLM sees when generating a response.\nLocal context\nThis is represented via the\nRunContextWrapper\nclass and the\ncontext\nproperty within it. The way this works is:\nYou create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.\nYou pass that object to the various run methods (e.g.\nRunner.run(..., **context=whatever**))\n.\nAll your tool calls, lifecycle hooks etc will be passed a wrapper object,\nRunContextWrapper[T]\n, where\nT\nrepresents your context object type which you can access via\nwrapper.context\n.\nThe\nmost important\nthing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same\ntype\nof context.\nYou can use the context for things like:\nContextual data for your run (e.g. things like a username/uid or other information about the user)\nDependencies (e.g. logger objects, data fetchers, etc)\nHelper functions\nNote\nThe context object is\nnot\nsent to the LLM. It is purely a local object that you can read from, write to and call methods on it.\nimport\nasyncio\nfrom\ndataclasses\nimport\ndataclass\nfrom\nagents\nimport\nAgent\n,\nRunContextWrapper\n,\nRunner\n,\nfunction_tool\n@dataclass\nclass\nUserInfo\n:\n# (1)!\nname\n:\nstr\nuid\n:\nint\n@function_tool\nasync\ndef\nfetch_user_age\n(\nwrapper\n:\nRunContextWrapper\n[\nUserInfo\n])\n->\nstr\n:\n# (2)!\nreturn\nf\n\"User\n{\nwrapper\n.\ncontext\n.\nname\n}\nis 47 years old\"\nasync\ndef\nmain\n():\nuser_info\n=\nUserInfo\n(\nname\n=\n\"John\"\n,\nuid\n=\n123\n)\nagent\n=\nAgent\n[\nUserInfo\n](\n# (3)!\nname\n=\n\"Assistant\"\n,\ntools\n=\n[\nfetch_user_age\n],\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\n# (4)!\nstarting_agent\n=\nagent\n,\ninput\n=\n\"What is the age of the user?\"\n,\ncontext\n=\nuser_info\n,\n)\nprint\n(\nresult\n.\nfinal_output\n)\n# (5)!\n# The user John is 47 years old.\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nThis is the context object. We've used a dataclass here, but you can use any type.\nThis is a tool. You can see it takes a\nRunContextWrapper[UserInfo]\n. The tool implementation reads from the context.\nWe mark the agent with the generic\nUserInfo\n, so that the typechecker can catch errors (for example, if we tried to pass a tool that took a different context type).\nThe context is passed to the\nrun\nfunction.\nThe agent correctly calls the tool and gets the age.\nAgent/LLM context\nWhen an LLM is called, the\nonly\ndata it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:\nYou can add it to the Agent\ninstructions\n. This is also known as a \"system prompt\" or \"developer message\". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).\nAdd it to the\ninput\nwhen calling the\nRunner.run\nfunctions. This is similar to the\ninstructions\ntactic, but allows you to have messages that are lower in the\nchain of command\n.\nExpose it via function tools. This is useful for\non-demand\ncontext - the LLM decides when it needs some data, and can call the tool to fetch that data.\nUse retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for \"grounding\" the response in relevant contextual data.",
  "Guardrails": "Guardrails\nGuardrailFunctionOutput\ndataclass\nThe output of a guardrail function.\nSource code in\nsrc/agents/guardrail.py\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n@dataclass\nclass\nGuardrailFunctionOutput\n:\n\"\"\"The output of a guardrail function.\"\"\"\noutput_info\n:\nAny\n\"\"\"\nOptional information about the guardrail's output. For example, the guardrail could include\ninformation about the checks it performed and granular results.\n\"\"\"\ntripwire_triggered\n:\nbool\n\"\"\"\nWhether the tripwire was triggered. If triggered, the agent's execution will be halted.\n\"\"\"\noutput_info\ninstance-attribute\noutput_info\n:\nAny\nOptional information about the guardrail's output. For example, the guardrail could include\ninformation about the checks it performed and granular results.\ntripwire_triggered\ninstance-attribute\ntripwire_triggered\n:\nbool\nWhether the tripwire was triggered. If triggered, the agent's execution will be halted.\nInputGuardrailResult\ndataclass\nThe result of a guardrail run.\nSource code in\nsrc/agents/guardrail.py\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n@dataclass\nclass\nInputGuardrailResult\n:\n\"\"\"The result of a guardrail run.\"\"\"\nguardrail\n:\nInputGuardrail\n[\nAny\n]\n\"\"\"\nThe guardrail that was run.\n\"\"\"\noutput\n:\nGuardrailFunctionOutput\n\"\"\"The output of the guardrail function.\"\"\"\nguardrail\ninstance-attribute\nguardrail\n:\nInputGuardrail\n[\nAny\n]\nThe guardrail that was run.\noutput\ninstance-attribute\noutput\n:\nGuardrailFunctionOutput\nThe output of the guardrail function.\nOutputGuardrailResult\ndataclass\nThe result of a guardrail run.\nSource code in\nsrc/agents/guardrail.py\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n@dataclass\nclass\nOutputGuardrailResult\n:\n\"\"\"The result of a guardrail run.\"\"\"\nguardrail\n:\nOutputGuardrail\n[\nAny\n]\n\"\"\"\nThe guardrail that was run.\n\"\"\"\nagent_output\n:\nAny\n\"\"\"\nThe output of the agent that was checked by the guardrail.\n\"\"\"\nagent\n:\nAgent\n[\nAny\n]\n\"\"\"\nThe agent that was checked by the guardrail.\n\"\"\"\noutput\n:\nGuardrailFunctionOutput\n\"\"\"The output of the guardrail function.\"\"\"\nguardrail\ninstance-attribute\nguardrail\n:\nOutputGuardrail\n[\nAny\n]\nThe guardrail that was run.\nagent_output\ninstance-attribute\nagent_output\n:\nAny\nThe output of the agent that was checked by the guardrail.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent that was checked by the guardrail.\noutput\ninstance-attribute\noutput\n:\nGuardrailFunctionOutput\nThe output of the guardrail function.\nInputGuardrail\ndataclass\nBases:\nGeneric\n[\nTContext\n]\nInput guardrails are checks that run in parallel to the agent's execution.\nThey can be used to do things like:\n- Check if input messages are off-topic\n- Take over control of the agent's execution if an unexpected input is detected\nYou can use the\n@input_guardrail()\ndecorator to turn a function into an\nInputGuardrail\n, or\ncreate an\nInputGuardrail\nmanually.\nGuardrails return a\nGuardrailResult\n. If\nresult.tripwire_triggered\nis\nTrue\n, the agent\nexecution will immediately stop and a\nInputGuardrailTripwireTriggered\nexception will be raised\nSource code in\nsrc/agents/guardrail.py\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n@dataclass\nclass\nInputGuardrail\n(\nGeneric\n[\nTContext\n]):\n\"\"\"Input guardrails are checks that run in parallel to the agent's execution.\nThey can be used to do things like:\n- Check if input messages are off-topic\n- Take over control of the agent's execution if an unexpected input is detected\nYou can use the `@input_guardrail()` decorator to turn a function into an `InputGuardrail`, or\ncreate an `InputGuardrail` manually.\nGuardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`, the agent\nexecution will immediately stop and a `InputGuardrailTripwireTriggered` exception will be raised\n\"\"\"\nguardrail_function\n:\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nAny\n],\nstr\n|\nlist\n[\nTResponseInputItem\n]],\nMaybeAwaitable\n[\nGuardrailFunctionOutput\n],\n]\n\"\"\"A function that receives the agent input and the context, and returns a\n`GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\ninclude information about the guardrail's output.\n\"\"\"\nname\n:\nstr\n|\nNone\n=\nNone\n\"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\nfunction's name.\n\"\"\"\ndef\nget_name\n(\nself\n)\n->\nstr\n:\nif\nself\n.\nname\n:\nreturn\nself\n.\nname\nreturn\nself\n.\nguardrail_function\n.\n__name__\nasync\ndef\nrun\n(\nself\n,\nagent\n:\nAgent\n[\nAny\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\n)\n->\nInputGuardrailResult\n:\nif\nnot\ncallable\n(\nself\n.\nguardrail_function\n):\nraise\nUserError\n(\nf\n\"Guardrail function must be callable, got\n{\nself\n.\nguardrail_function\n}\n\"\n)\noutput\n=\nself\n.\nguardrail_function\n(\ncontext\n,\nagent\n,\ninput\n)\nif\ninspect\n.\nisawaitable\n(\noutput\n):\nreturn\nInputGuardrailResult\n(\nguardrail\n=\nself\n,\noutput\n=\nawait\noutput\n,\n)\nreturn\nInputGuardrailResult\n(\nguardrail\n=\nself\n,\noutput\n=\noutput\n,\n)\nguardrail_function\ninstance-attribute\nguardrail_function\n:\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nAny\n],\nstr\n|\nlist\n[\nTResponseInputItem\n],\n],\nMaybeAwaitable\n[\nGuardrailFunctionOutput\n],\n]\nA function that receives the agent input and the context, and returns a\nGuardrailResult\n. The result marks whether the tripwire was triggered, and can optionally\ninclude information about the guardrail's output.\nname\nclass-attribute\ninstance-attribute\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the guardrail, used for tracing. If not provided, we'll use the guardrail\nfunction's name.\nOutputGuardrail\ndataclass\nBases:\nGeneric\n[\nTContext\n]\nOutput guardrails are checks that run on the final output of an agent.\nThey can be used to do check if the output passes certain validation criteria\nYou can use the\n@output_guardrail()\ndecorator to turn a function into an\nOutputGuardrail\n,\nor create an\nOutputGuardrail\nmanually.\nGuardrails return a\nGuardrailResult\n. If\nresult.tripwire_triggered\nis\nTrue\n, a\nOutputGuardrailTripwireTriggered\nexception will be raised.\nSource code in\nsrc/agents/guardrail.py\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n@dataclass\nclass\nOutputGuardrail\n(\nGeneric\n[\nTContext\n]):\n\"\"\"Output guardrails are checks that run on the final output of an agent.\nThey can be used to do check if the output passes certain validation criteria\nYou can use the `@output_guardrail()` decorator to turn a function into an `OutputGuardrail`,\nor create an `OutputGuardrail` manually.\nGuardrails return a `GuardrailResult`. If `result.tripwire_triggered` is `True`, a\n`OutputGuardrailTripwireTriggered` exception will be raised.\n\"\"\"\nguardrail_function\n:\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nAny\n],\nAny\n],\nMaybeAwaitable\n[\nGuardrailFunctionOutput\n],\n]\n\"\"\"A function that receives the final agent, its output, and the context, and returns a\n`GuardrailResult`. The result marks whether the tripwire was triggered, and can optionally\ninclude information about the guardrail's output.\n\"\"\"\nname\n:\nstr\n|\nNone\n=\nNone\n\"\"\"The name of the guardrail, used for tracing. If not provided, we'll use the guardrail\nfunction's name.\n\"\"\"\ndef\nget_name\n(\nself\n)\n->\nstr\n:\nif\nself\n.\nname\n:\nreturn\nself\n.\nname\nreturn\nself\n.\nguardrail_function\n.\n__name__\nasync\ndef\nrun\n(\nself\n,\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nAny\n],\nagent_output\n:\nAny\n)\n->\nOutputGuardrailResult\n:\nif\nnot\ncallable\n(\nself\n.\nguardrail_function\n):\nraise\nUserError\n(\nf\n\"Guardrail function must be callable, got\n{\nself\n.\nguardrail_function\n}\n\"\n)\noutput\n=\nself\n.\nguardrail_function\n(\ncontext\n,\nagent\n,\nagent_output\n)\nif\ninspect\n.\nisawaitable\n(\noutput\n):\nreturn\nOutputGuardrailResult\n(\nguardrail\n=\nself\n,\nagent\n=\nagent\n,\nagent_output\n=\nagent_output\n,\noutput\n=\nawait\noutput\n,\n)\nreturn\nOutputGuardrailResult\n(\nguardrail\n=\nself\n,\nagent\n=\nagent\n,\nagent_output\n=\nagent_output\n,\noutput\n=\noutput\n,\n)\nguardrail_function\ninstance-attribute\nguardrail_function\n:\nCallable\n[\n[\nRunContextWrapper\n[\nTContext\n],\nAgent\n[\nAny\n],\nAny\n],\nMaybeAwaitable\n[\nGuardrailFunctionOutput\n],\n]\nA function that receives the final agent, its output, and the context, and returns a\nGuardrailResult\n. The result marks whether the tripwire was triggered, and can optionally\ninclude information about the guardrail's output.\nname\nclass-attribute\ninstance-attribute\nname\n:\nstr\n|\nNone\n=\nNone\nThe name of the guardrail, used for tracing. If not provided, we'll use the guardrail\nfunction's name.\ninput_guardrail\ninput_guardrail\n(\nfunc\n:\n_InputGuardrailFuncSync\n[\nTContext_co\n],\n)\n->\nInputGuardrail\n[\nTContext_co\n]\ninput_guardrail\n(\nfunc\n:\n_InputGuardrailFuncAsync\n[\nTContext_co\n],\n)\n->\nInputGuardrail\n[\nTContext_co\n]\ninput_guardrail\n(\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nCallable\n[\n[\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n]\n],\nInputGuardrail\n[\nTContext_co\n],\n]\ninput_guardrail\n(\nfunc\n:\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nInputGuardrail\n[\nTContext_co\n]\n|\nCallable\n[\n[\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n]\n],\nInputGuardrail\n[\nTContext_co\n],\n]\n)\nDecorator that transforms a sync or async function into an\nInputGuardrail\n.\nIt can be used directly (no parentheses) or with keyword args, e.g.:\n@input_guardrail\ndef my_sync_guardrail(...): ...\n\n@input_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\nSource code in\nsrc/agents/guardrail.py\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\ndef\ninput_guardrail\n(\nfunc\n:\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nInputGuardrail\n[\nTContext_co\n]\n|\nCallable\n[\n[\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n]],\nInputGuardrail\n[\nTContext_co\n],\n]\n):\n\"\"\"\nDecorator that transforms a sync or async function into an `InputGuardrail`.\nIt can be used directly (no parentheses) or with keyword args, e.g.:\n@input_guardrail\ndef my_sync_guardrail(...): ...\n@input_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\n\"\"\"\ndef\ndecorator\n(\nf\n:\n_InputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_InputGuardrailFuncAsync\n[\nTContext_co\n],\n)\n->\nInputGuardrail\n[\nTContext_co\n]:\nreturn\nInputGuardrail\n(\nguardrail_function\n=\nf\n,\nname\n=\nname\n)\nif\nfunc\nis\nnot\nNone\n:\n# Decorator was used without parentheses\nreturn\ndecorator\n(\nfunc\n)\n# Decorator used with keyword arguments\nreturn\ndecorator\noutput_guardrail\noutput_guardrail\n(\nfunc\n:\n_OutputGuardrailFuncSync\n[\nTContext_co\n],\n)\n->\nOutputGuardrail\n[\nTContext_co\n]\noutput_guardrail\n(\nfunc\n:\n_OutputGuardrailFuncAsync\n[\nTContext_co\n],\n)\n->\nOutputGuardrail\n[\nTContext_co\n]\noutput_guardrail\n(\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n)\n->\nCallable\n[\n[\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n]\n],\nOutputGuardrail\n[\nTContext_co\n],\n]\noutput_guardrail\n(\nfunc\n:\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nOutputGuardrail\n[\nTContext_co\n]\n|\nCallable\n[\n[\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n]\n],\nOutputGuardrail\n[\nTContext_co\n],\n]\n)\nDecorator that transforms a sync or async function into an\nOutputGuardrail\n.\nIt can be used directly (no parentheses) or with keyword args, e.g.:\n@output_guardrail\ndef my_sync_guardrail(...): ...\n\n@output_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\nSource code in\nsrc/agents/guardrail.py\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\ndef\noutput_guardrail\n(\nfunc\n:\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n]\n|\nNone\n=\nNone\n,\n*\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\n(\nOutputGuardrail\n[\nTContext_co\n]\n|\nCallable\n[\n[\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n]],\nOutputGuardrail\n[\nTContext_co\n],\n]\n):\n\"\"\"\nDecorator that transforms a sync or async function into an `OutputGuardrail`.\nIt can be used directly (no parentheses) or with keyword args, e.g.:\n@output_guardrail\ndef my_sync_guardrail(...): ...\n@output_guardrail(name=\"guardrail_name\")\nasync def my_async_guardrail(...): ...\n\"\"\"\ndef\ndecorator\n(\nf\n:\n_OutputGuardrailFuncSync\n[\nTContext_co\n]\n|\n_OutputGuardrailFuncAsync\n[\nTContext_co\n],\n)\n->\nOutputGuardrail\n[\nTContext_co\n]:\nreturn\nOutputGuardrail\n(\nguardrail_function\n=\nf\n,\nname\n=\nname\n)\nif\nfunc\nis\nnot\nNone\n:\n# Decorator was used without parentheses\nreturn\ndecorator\n(\nfunc\n)\n# Decorator used with keyword arguments\nreturn\ndecorator",
  "Orchestrating multiple agents": "Orchestrating multiple agents\nOrchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:\nAllowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.\nOrchestrating via code: determining the flow of agents via your code.\nYou can mix and match these patterns. Each has their own tradeoffs, described below.\nOrchestrating via LLM\nAn agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:\nWeb search to find information online\nFile search and retrieval to search through proprietary data and connections\nComputer use to take actions on a computer\nCode execution to do data analysis\nHandoffs to specialized agents that are great at planning, report writing and more.\nThis pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:\nInvest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.\nMonitor your app and iterate on it. See where things go wrong, and iterate on your prompts.\nAllow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.\nHave specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.\nInvest in\nevals\n. This lets you train your agents to improve and get better at tasks.\nOrchestrating via code\nWhile orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:\nUsing\nstructured outputs\nto generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.\nChaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.\nRunning the agent that performs the task in a\nwhile\nloop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.\nRunning multiple agents in parallel, e.g. via Python primitives like\nasyncio.gather\n. This is useful for speed when you have multiple tasks that don't depend on each other.\nWe have a number of examples in\nexamples/agent_patterns\n.",
  "Models": "Models\nThe Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:\nRecommended\n: the\nOpenAIResponsesModel\n, which calls OpenAI APIs using the new\nResponses API\n.\nThe\nOpenAIChatCompletionsModel\n, which calls OpenAI APIs using the\nChat Completions API\n.\nMixing and matching models\nWithin a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an\nAgent\n, you can select a specific model by either:\nPassing the name of an OpenAI model.\nPassing any model name + a\nModelProvider\nthat can map that name to a Model instance.\nDirectly providing a\nModel\nimplementation.\nNote\nWhile our SDK supports both the\nOpenAIResponsesModel\nand the\nOpenAIChatCompletionsModel\nshapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nAsyncOpenAI\n,\nOpenAIChatCompletionsModel\nimport\nasyncio\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish agent\"\n,\ninstructions\n=\n\"You only speak Spanish.\"\n,\nmodel\n=\n\"o3-mini\"\n,\n# (1)!\n)\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\nmodel\n=\nOpenAIChatCompletionsModel\n(\n# (2)!\nmodel\n=\n\"gpt-4o\"\n,\nopenai_client\n=\nAsyncOpenAI\n()\n),\n)\ntriage_agent\n=\nAgent\n(\nname\n=\n\"Triage agent\"\n,\ninstructions\n=\n\"Handoff to the appropriate agent based on the language of the request.\"\n,\nhandoffs\n=\n[\nspanish_agent\n,\nenglish_agent\n],\nmodel\n=\n\"gpt-3.5-turbo\"\n,\n)\nasync\ndef\nmain\n():\nresult\n=\nawait\nRunner\n.\nrun\n(\ntriage_agent\n,\ninput\n=\n\"Hola, Â¿cÃ³mo estÃ¡s?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\nSets the name of an OpenAI model directly.\nProvides a\nModel\nimplementation.\nWhen you want to further configure the model used for an agent, you can pass\nModelSettings\n, which provides optional model configuration parameters such as temperature.\nfrom\nagents\nimport\nAgent\n,\nModelSettings\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\nmodel\n=\n\"gpt-4o\"\n,\nmodel_settings\n=\nModelSettings\n(\ntemperature\n=\n0.1\n),\n)\nUsing other LLM providers\nYou can use other LLM providers in 3 ways (examples\nhere\n):\nset_default_openai_client\nis useful in cases where you want to globally use an instance of\nAsyncOpenAI\nas the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the\nbase_url\nand\napi_key\n. See a configurable example in\nexamples/model_providers/custom_example_global.py\n.\nModelProvider\nis at the\nRunner.run\nlevel. This lets you say \"use a custom model provider for all agents in this run\". See a configurable example in\nexamples/model_providers/custom_example_provider.py\n.\nAgent.model\nlets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in\nexamples/model_providers/custom_example_agent.py\n. An easy way to use most available models is via the\nLiteLLM integration\n.\nIn cases where you do not have an API key from\nplatform.openai.com\n, we recommend disabling tracing via\nset_tracing_disabled()\n, or setting up a\ndifferent tracing processor\n.\nNote\nIn these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.\nCommon issues with using other LLM providers\nTracing client error 401\nIf you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:\nDisable tracing entirely:\nset_tracing_disabled(True)\n.\nSet an OpenAI key for tracing:\nset_tracing_export_api_key(...)\n. This API key will only be used for uploading traces, and must be from\nplatform.openai.com\n.\nUse a non-OpenAI trace processor. See the\ntracing docs\n.\nResponses API support\nThe SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:\nCall\nset_default_openai_api(\"chat_completions\")\n. This works if you are setting\nOPENAI_API_KEY\nand\nOPENAI_BASE_URL\nvia environment vars.\nUse\nOpenAIChatCompletionsModel\n. There are examples\nhere\n.\nStructured outputs support\nSome model providers don't have support for\nstructured outputs\n. This sometimes results in an error that looks something like this:\nBadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\nThis is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the\njson_schema\nto use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.",
  "Using any model via LiteLLM": "Using any model via LiteLLM\nNote\nThe LiteLLM integration is in beta. You may run into issues with some model providers, especially smaller ones. Please report any issues via\nGithub issues\nand we'll fix quickly.\nLiteLLM\nis a library that allows you to use 100+ models via a single interface. We've added a LiteLLM integration to allow you to use any AI model in the Agents SDK.\nSetup\nYou'll need to ensure\nlitellm\nis available. You can do this by installing the optional\nlitellm\ndependency group:\npip\ninstall\n\"openai-agents[litellm]\"\nOnce done, you can use\nLitellmModel\nin any agent.\nExample\nThis is a fully working example. When you run it, you'll be prompted for a model name and API key. For example, you could enter:\nopenai/gpt-4.1\nfor the model, and your OpenAI API key\nanthropic/claude-3-5-sonnet-20240620\nfor the model, and your Anthropic API key\netc\nFor a full list of models supported in LiteLLM, see the\nlitellm providers docs\n.\nfrom\n__future__\nimport\nannotations\nimport\nasyncio\nfrom\nagents\nimport\nAgent\n,\nRunner\n,\nfunction_tool\n,\nset_tracing_disabled\nfrom\nagents.extensions.models.litellm_model\nimport\nLitellmModel\n@function_tool\ndef\nget_weather\n(\ncity\n:\nstr\n):\nprint\n(\nf\n\"[debug] getting weather for\n{\ncity\n}\n\"\n)\nreturn\nf\n\"The weather in\n{\ncity\n}\nis sunny.\"\nasync\ndef\nmain\n(\nmodel\n:\nstr\n,\napi_key\n:\nstr\n):\nagent\n=\nAgent\n(\nname\n=\n\"Assistant\"\n,\ninstructions\n=\n\"You only respond in haikus.\"\n,\nmodel\n=\nLitellmModel\n(\nmodel\n=\nmodel\n,\napi_key\n=\napi_key\n),\ntools\n=\n[\nget_weather\n],\n)\nresult\n=\nawait\nRunner\n.\nrun\n(\nagent\n,\n\"What's the weather in Tokyo?\"\n)\nprint\n(\nresult\n.\nfinal_output\n)\nif\n__name__\n==\n\"__main__\"\n:\n# First try to get model/api key from args\nimport\nargparse\nparser\n=\nargparse\n.\nArgumentParser\n()\nparser\n.\nadd_argument\n(\n\"--model\"\n,\ntype\n=\nstr\n,\nrequired\n=\nFalse\n)\nparser\n.\nadd_argument\n(\n\"--api-key\"\n,\ntype\n=\nstr\n,\nrequired\n=\nFalse\n)\nargs\n=\nparser\n.\nparse_args\n()\nmodel\n=\nargs\n.\nmodel\nif\nnot\nmodel\n:\nmodel\n=\ninput\n(\n\"Enter a model name for Litellm: \"\n)\napi_key\n=\nargs\n.\napi_key\nif\nnot\napi_key\n:\napi_key\n=\ninput\n(\n\"Enter an API key for Litellm: \"\n)\nasyncio\n.\nrun\n(\nmain\n(\nmodel\n,\napi_key\n))",
  "Configuring the SDK": "Configuring the SDK\nAPI keys and clients\nBy default, the SDK looks for the\nOPENAI_API_KEY\nenvironment variable for LLM requests and tracing, as soon as it is imported. If you are unable to set that environment variable before your app starts, you can use the\nset_default_openai_key()\nfunction to set the key.\nfrom\nagents\nimport\nset_default_openai_key\nset_default_openai_key\n(\n\"sk-...\"\n)\nAlternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an\nAsyncOpenAI\ninstance, using the API key from the environment variable or the default key set above. You can change this by using the\nset_default_openai_client()\nfunction.\nfrom\nopenai\nimport\nAsyncOpenAI\nfrom\nagents\nimport\nset_default_openai_client\ncustom_client\n=\nAsyncOpenAI\n(\nbase_url\n=\n\"...\"\n,\napi_key\n=\n\"...\"\n)\nset_default_openai_client\n(\ncustom_client\n)\nFinally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the\nset_default_openai_api()\nfunction.\nfrom\nagents\nimport\nset_default_openai_api\nset_default_openai_api\n(\n\"chat_completions\"\n)\nTracing\nTracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set). You can specifically set the API key used for tracing by using the\nset_tracing_export_api_key\nfunction.\nfrom\nagents\nimport\nset_tracing_export_api_key\nset_tracing_export_api_key\n(\n\"sk-...\"\n)\nYou can also disable tracing entirely by using the\nset_tracing_disabled()\nfunction.\nfrom\nagents\nimport\nset_tracing_disabled\nset_tracing_disabled\n(\nTrue\n)\nDebug logging\nThe SDK has two Python loggers without any handlers set. By default, this means that warnings and errors are sent to\nstdout\n, but other logs are suppressed.\nTo enable verbose logging, use the\nenable_verbose_stdout_logging()\nfunction.\nfrom\nagents\nimport\nenable_verbose_stdout_logging\nenable_verbose_stdout_logging\n()\nAlternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the\nPython logging guide\n.\nimport\nlogging\nlogger\n=\nlogging\n.\ngetLogger\n(\n\"openai.agents\"\n)\n# or openai.agents.tracing for the Tracing logger\n# To make all logs show up\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n# To make info and above show up\nlogger\n.\nsetLevel\n(\nlogging\n.\nINFO\n)\n# To make warning and above show up\nlogger\n.\nsetLevel\n(\nlogging\n.\nWARNING\n)\n# etc\n# You can customize this as needed, but this will output to `stderr` by default\nlogger\n.\naddHandler\n(\nlogging\n.\nStreamHandler\n())\nSensitive data in logs\nCertain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.\nTo disable logging LLM inputs and outputs:\nexport\nOPENAI_AGENTS_DONT_LOG_MODEL_DATA\n=\n1\nTo disable logging tool inputs and outputs:\nexport\nOPENAI_AGENTS_DONT_LOG_TOOL_DATA\n=\n1",
  "Agent Visualization": "Agent Visualization\nAgent visualization allows you to generate a structured graphical representation of agents and their relationships using\nGraphviz\n. This is useful for understanding how agents, tools, and handoffs interact within an application.\nInstallation\nInstall the optional\nviz\ndependency group:\npip\ninstall\n\"openai-agents[viz]\"\nGenerating a Graph\nYou can generate an agent visualization using the\ndraw_graph\nfunction. This function creates a directed graph where:\nAgents\nare represented as yellow boxes.\nTools\nare represented as green ellipses.\nHandoffs\nare directed edges from one agent to another.\nExample Usage\nfrom\nagents\nimport\nAgent\n,\nfunction_tool\nfrom\nagents.extensions.visualization\nimport\ndraw_graph\n@function_tool\ndef\nget_weather\n(\ncity\n:\nstr\n)\n->\nstr\n:\nreturn\nf\n\"The weather in\n{\ncity\n}\nis sunny.\"\nspanish_agent\n=\nAgent\n(\nname\n=\n\"Spanish agent\"\n,\ninstructions\n=\n\"You only speak Spanish.\"\n,\n)\nenglish_agent\n=\nAgent\n(\nname\n=\n\"English agent\"\n,\ninstructions\n=\n\"You only speak English\"\n,\n)\ntriage_agent\n=\nAgent\n(\nname\n=\n\"Triage agent\"\n,\ninstructions\n=\n\"Handoff to the appropriate agent based on the language of the request.\"\n,\nhandoffs\n=\n[\nspanish_agent\n,\nenglish_agent\n],\ntools\n=\n[\nget_weather\n],\n)\ndraw_graph\n(\ntriage_agent\n)\nThis generates a graph that visually represents the structure of the\ntriage agent\nand its connections to sub-agents and tools.\nUnderstanding the Visualization\nThe generated graph includes:\nA\nstart node\n(\n__start__\n) indicating the entry point.\nAgents represented as\nrectangles\nwith yellow fill.\nTools represented as\nellipses\nwith green fill.\nDirected edges indicating interactions:\nSolid arrows\nfor agent-to-agent handoffs.\nDotted arrows\nfor tool invocations.\nAn\nend node\n(\n__end__\n) indicating where execution terminates.\nCustomizing the Graph\nShowing the Graph\nBy default,\ndraw_graph\ndisplays the graph inline. To show the graph in a separate window, write the following:\ndraw_graph\n(\ntriage_agent\n)\n.\nview\n()\nSaving the Graph\nBy default,\ndraw_graph\ndisplays the graph inline. To save it as a file, specify a filename:\ndraw_graph\n(\ntriage_agent\n,\nfilename\n=\n\"agent_graph.png\"\n)\nThis will generate\nagent_graph.png\nin the working directory.",
  "Pipelines and workflows": "Pipelines and workflows\nVoicePipeline\nis a class that makes it easy to turn your agentic workflows into a voice app. You pass in a workflow to run, and the pipeline takes care of transcribing input audio, detecting when the audio ends, calling your workflow at the right time, and turning the workflow output back into audio.\ngraph LR\n    %% Input\n    A[\"ðŸŽ¤ Audio Input\"]\n\n    %% Voice Pipeline\n    subgraph Voice_Pipeline [Voice Pipeline]\n        direction TB\n        B[\"Transcribe (speech-to-text)\"]\n        C[\"Your Code\"]:::highlight\n        D[\"Text-to-speech\"]\n        B --> C --> D\n    end\n\n    %% Output\n    E[\"ðŸŽ§ Audio Output\"]\n\n    %% Flow\n    A --> Voice_Pipeline\n    Voice_Pipeline --> E\n\n    %% Custom styling\n    classDef highlight fill:#ffcc66,stroke:#333,stroke-width:1px,font-weight:700;\nConfiguring a pipeline\nWhen you create a pipeline, you can set a few things:\nThe\nworkflow\n, which is the code that runs each time new audio is transcribed.\nThe\nspeech-to-text\nand\ntext-to-speech\nmodels used\nThe\nconfig\n, which lets you configure things like:\nA model provider, which can map model names to models\nTracing, including whether to disable tracing, whether audio files are uploaded, the workflow name, trace IDs etc.\nSettings on the TTS and STT models, like the prompt, language and data types used.\nRunning a pipeline\nYou can run a pipeline via the\nrun()\nmethod, which lets you pass in audio input in two forms:\nAudioInput\nis used when you have a full audio transcript, and just want to produce a result for it. This is useful in cases where you don't need to detect when a speaker is done speaking; for example, when you have pre-recorded audio or in push-to-talk apps where it's clear when the user is done speaking.\nStreamedAudioInput\nis used when you might need to detect when a user is done speaking. It allows you to push audio chunks as they are detected, and the voice pipeline will automatically run the agent workflow at the right time, via a process called \"activity detection\".\nResults\nThe result of a voice pipeline run is a\nStreamedAudioResult\n. This is an object that lets you stream events as they occur. There are a few kinds of\nVoiceStreamEvent\n, including:\nVoiceStreamEventAudio\n, which contains a chunk of audio.\nVoiceStreamEventLifecycle\n, which informs you of lifecycle events like a turn starting or ending.\nVoiceStreamEventError\n, is an error event.\nresult\n=\nawait\npipeline\n.\nrun\n(\ninput\n)\nasync\nfor\nevent\nin\nresult\n.\nstream\n():\nif\nevent\n.\ntype\n==\n\"voice_stream_event_audio\"\n:\n# play audio\nelif\nevent\n.\ntype\n==\n\"voice_stream_event_lifecycle\"\n:\n# lifecycle\nelif\nevent\n.\ntype\n==\n\"voice_stream_event_error\"\n# error\n...\nBest practices\nInterruptions\nThe Agents SDK currently does not support any built-in interruptions support for\nStreamedAudioInput\n. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the\nVoiceStreamEventLifecycle\nevents.\nturn_started\nwill indicate that a new turn was transcribed and processing is beginning.\nturn_ended\nwill trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.",
  "Agents module": "Agents module\nset_default_openai_key\nset_default_openai_key\n(\nkey\n:\nstr\n,\nuse_for_tracing\n:\nbool\n=\nTrue\n)\n->\nNone\nSet the default OpenAI API key to use for LLM requests (and optionally tracing(). This is\nonly necessary if the OPENAI_API_KEY environment variable is not already set.\nIf provided, this key will be used instead of the OPENAI_API_KEY environment variable.\nParameters:\nName\nType\nDescription\nDefault\nkey\nstr\nThe OpenAI key to use.\nrequired\nuse_for_tracing\nbool\nWhether to also use this key to send traces to OpenAI. Defaults to True\nIf False, you'll either need to set the OPENAI_API_KEY environment variable or call\nset_tracing_export_api_key() with the API key you want to use for tracing.\nTrue\nSource code in\nsrc/agents/__init__.py\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\ndef\nset_default_openai_key\n(\nkey\n:\nstr\n,\nuse_for_tracing\n:\nbool\n=\nTrue\n)\n->\nNone\n:\n\"\"\"Set the default OpenAI API key to use for LLM requests (and optionally tracing(). This is\nonly necessary if the OPENAI_API_KEY environment variable is not already set.\nIf provided, this key will be used instead of the OPENAI_API_KEY environment variable.\nArgs:\nkey: The OpenAI key to use.\nuse_for_tracing: Whether to also use this key to send traces to OpenAI. Defaults to True\nIf False, you'll either need to set the OPENAI_API_KEY environment variable or call\nset_tracing_export_api_key() with the API key you want to use for tracing.\n\"\"\"\n_config\n.\nset_default_openai_key\n(\nkey\n,\nuse_for_tracing\n)\nset_default_openai_client\nset_default_openai_client\n(\nclient\n:\nAsyncOpenAI\n,\nuse_for_tracing\n:\nbool\n=\nTrue\n)\n->\nNone\nSet the default OpenAI client to use for LLM requests and/or tracing. If provided, this\nclient will be used instead of the default OpenAI client.\nParameters:\nName\nType\nDescription\nDefault\nclient\nAsyncOpenAI\nThe OpenAI client to use.\nrequired\nuse_for_tracing\nbool\nWhether to use the API key from this client for uploading traces. If False,\nyou'll either need to set the OPENAI_API_KEY environment variable or call\nset_tracing_export_api_key() with the API key you want to use for tracing.\nTrue\nSource code in\nsrc/agents/__init__.py\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\ndef\nset_default_openai_client\n(\nclient\n:\nAsyncOpenAI\n,\nuse_for_tracing\n:\nbool\n=\nTrue\n)\n->\nNone\n:\n\"\"\"Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this\nclient will be used instead of the default OpenAI client.\nArgs:\nclient: The OpenAI client to use.\nuse_for_tracing: Whether to use the API key from this client for uploading traces. If False,\nyou'll either need to set the OPENAI_API_KEY environment variable or call\nset_tracing_export_api_key() with the API key you want to use for tracing.\n\"\"\"\n_config\n.\nset_default_openai_client\n(\nclient\n,\nuse_for_tracing\n)\nset_default_openai_api\nset_default_openai_api\n(\napi\n:\nLiteral\n[\n\"chat_completions\"\n,\n\"responses\"\n],\n)\n->\nNone\nSet the default API to use for OpenAI LLM requests. By default, we will use the responses API\nbut you can set this to use the chat completions API instead.\nSource code in\nsrc/agents/__init__.py\n134\n135\n136\n137\n138\ndef\nset_default_openai_api\n(\napi\n:\nLiteral\n[\n\"chat_completions\"\n,\n\"responses\"\n])\n->\nNone\n:\n\"\"\"Set the default API to use for OpenAI LLM requests. By default, we will use the responses API\nbut you can set this to use the chat completions API instead.\n\"\"\"\n_config\n.\nset_default_openai_api\n(\napi\n)\nset_tracing_export_api_key\nset_tracing_export_api_key\n(\napi_key\n:\nstr\n)\n->\nNone\nSet the OpenAI API key for the backend exporter.\nSource code in\nsrc/agents/tracing/__init__.py\n100\n101\n102\n103\n104\ndef\nset_tracing_export_api_key\n(\napi_key\n:\nstr\n)\n->\nNone\n:\n\"\"\"\nSet the OpenAI API key for the backend exporter.\n\"\"\"\ndefault_exporter\n()\n.\nset_api_key\n(\napi_key\n)\nset_tracing_disabled\nset_tracing_disabled\n(\ndisabled\n:\nbool\n)\n->\nNone\nSet whether tracing is globally disabled.\nSource code in\nsrc/agents/tracing/__init__.py\n93\n94\n95\n96\n97\ndef\nset_tracing_disabled\n(\ndisabled\n:\nbool\n)\n->\nNone\n:\n\"\"\"\nSet whether tracing is globally disabled.\n\"\"\"\nGLOBAL_TRACE_PROVIDER\n.\nset_disabled\n(\ndisabled\n)\nset_trace_processors\nset_trace_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n],\n)\n->\nNone\nSet the list of trace processors. This will replace the current list of processors.\nSource code in\nsrc/agents/tracing/__init__.py\n86\n87\n88\n89\n90\ndef\nset_trace_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n])\n->\nNone\n:\n\"\"\"\nSet the list of trace processors. This will replace the current list of processors.\n\"\"\"\nGLOBAL_TRACE_PROVIDER\n.\nset_processors\n(\nprocessors\n)\nenable_verbose_stdout_logging\nenable_verbose_stdout_logging\n()\nEnables verbose logging to stdout. This is useful for debugging.\nSource code in\nsrc/agents/__init__.py\n141\n142\n143\n144\n145\ndef\nenable_verbose_stdout_logging\n():\n\"\"\"Enables verbose logging to stdout. This is useful for debugging.\"\"\"\nlogger\n=\nlogging\n.\ngetLogger\n(\n\"openai.agents\"\n)\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\nlogger\n.\naddHandler\n(\nlogging\n.\nStreamHandler\n(\nsys\n.\nstdout\n))",
  "Runner": "Runner\nRunner\nSource code in\nsrc/agents/run.py\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\nclass\nRunner\n:\n@classmethod\nasync\ndef\nrun\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\n:\n\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\noutput is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\"\"\"\nif\nhooks\nis\nNone\n:\nhooks\n=\nRunHooks\n[\nAny\n]()\nif\nrun_config\nis\nNone\n:\nrun_config\n=\nRunConfig\n()\ntool_use_tracker\n=\nAgentToolUseTracker\n()\nwith\nTraceCtxManager\n(\nworkflow_name\n=\nrun_config\n.\nworkflow_name\n,\ntrace_id\n=\nrun_config\n.\ntrace_id\n,\ngroup_id\n=\nrun_config\n.\ngroup_id\n,\nmetadata\n=\nrun_config\n.\ntrace_metadata\n,\ndisabled\n=\nrun_config\n.\ntracing_disabled\n,\n):\ncurrent_turn\n=\n0\noriginal_input\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\n=\ncopy\n.\ndeepcopy\n(\ninput\n)\ngenerated_items\n:\nlist\n[\nRunItem\n]\n=\n[]\nmodel_responses\n:\nlist\n[\nModelResponse\n]\n=\n[]\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n]\n=\nRunContextWrapper\n(\ncontext\n=\ncontext\n,\n# type: ignore\n)\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\n=\n[]\ncurrent_span\n:\nSpan\n[\nAgentSpanData\n]\n|\nNone\n=\nNone\ncurrent_agent\n=\nstarting_agent\nshould_run_agent_start_hooks\n=\nTrue\ntry\n:\nwhile\nTrue\n:\n# Start an agent span if we don't have one. This span is ended if the current\n# agent changes, or if the agent loop ends.\nif\ncurrent_span\nis\nNone\n:\nhandoff_names\n=\n[\nh\n.\nagent_name\nfor\nh\nin\ncls\n.\n_get_handoffs\n(\ncurrent_agent\n)]\nif\noutput_schema\n:=\ncls\n.\n_get_output_schema\n(\ncurrent_agent\n):\noutput_type_name\n=\noutput_schema\n.\noutput_type_name\n()\nelse\n:\noutput_type_name\n=\n\"str\"\ncurrent_span\n=\nagent_span\n(\nname\n=\ncurrent_agent\n.\nname\n,\nhandoffs\n=\nhandoff_names\n,\noutput_type\n=\noutput_type_name\n,\n)\ncurrent_span\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nall_tools\n=\nawait\ncls\n.\n_get_all_tools\n(\ncurrent_agent\n)\ncurrent_span\n.\nspan_data\n.\ntools\n=\n[\nt\n.\nname\nfor\nt\nin\nall_tools\n]\ncurrent_turn\n+=\n1\nif\ncurrent_turn\n>\nmax_turns\n:\n_error_tracing\n.\nattach_error_to_span\n(\ncurrent_span\n,\nSpanError\n(\nmessage\n=\n\"Max turns exceeded\"\n,\ndata\n=\n{\n\"max_turns\"\n:\nmax_turns\n},\n),\n)\nraise\nMaxTurnsExceeded\n(\nf\n\"Max turns (\n{\nmax_turns\n}\n) exceeded\"\n)\nlogger\n.\ndebug\n(\nf\n\"Running agent\n{\ncurrent_agent\n.\nname\n}\n(turn\n{\ncurrent_turn\n}\n)\"\n,\n)\nif\ncurrent_turn\n==\n1\n:\ninput_guardrail_results\n,\nturn_result\n=\nawait\nasyncio\n.\ngather\n(\ncls\n.\n_run_input_guardrails\n(\nstarting_agent\n,\nstarting_agent\n.\ninput_guardrails\n+\n(\nrun_config\n.\ninput_guardrails\nor\n[]),\ncopy\n.\ndeepcopy\n(\ninput\n),\ncontext_wrapper\n,\n),\ncls\n.\n_run_single_turn\n(\nagent\n=\ncurrent_agent\n,\nall_tools\n=\nall_tools\n,\noriginal_input\n=\noriginal_input\n,\ngenerated_items\n=\ngenerated_items\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nshould_run_agent_start_hooks\n=\nshould_run_agent_start_hooks\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\nprevious_response_id\n=\nprevious_response_id\n,\n),\n)\nelse\n:\nturn_result\n=\nawait\ncls\n.\n_run_single_turn\n(\nagent\n=\ncurrent_agent\n,\nall_tools\n=\nall_tools\n,\noriginal_input\n=\noriginal_input\n,\ngenerated_items\n=\ngenerated_items\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nshould_run_agent_start_hooks\n=\nshould_run_agent_start_hooks\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\nshould_run_agent_start_hooks\n=\nFalse\nmodel_responses\n.\nappend\n(\nturn_result\n.\nmodel_response\n)\noriginal_input\n=\nturn_result\n.\noriginal_input\ngenerated_items\n=\nturn_result\n.\ngenerated_items\nif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepFinalOutput\n):\noutput_guardrail_results\n=\nawait\ncls\n.\n_run_output_guardrails\n(\ncurrent_agent\n.\noutput_guardrails\n+\n(\nrun_config\n.\noutput_guardrails\nor\n[]),\ncurrent_agent\n,\nturn_result\n.\nnext_step\n.\noutput\n,\ncontext_wrapper\n,\n)\nreturn\nRunResult\n(\ninput\n=\noriginal_input\n,\nnew_items\n=\ngenerated_items\n,\nraw_responses\n=\nmodel_responses\n,\nfinal_output\n=\nturn_result\n.\nnext_step\n.\noutput\n,\n_last_agent\n=\ncurrent_agent\n,\ninput_guardrail_results\n=\ninput_guardrail_results\n,\noutput_guardrail_results\n=\noutput_guardrail_results\n,\n)\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepHandoff\n):\ncurrent_agent\n=\ncast\n(\nAgent\n[\nTContext\n],\nturn_result\n.\nnext_step\n.\nnew_agent\n)\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\ncurrent_span\n=\nNone\nshould_run_agent_start_hooks\n=\nTrue\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepRunAgain\n):\npass\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unknown next step type:\n{\ntype\n(\nturn_result\n.\nnext_step\n)\n}\n\"\n)\nfinally\n:\nif\ncurrent_span\n:\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\n@classmethod\ndef\nrun_sync\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\n:\n\"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\n`run` method, so it will not work if there's already an event loop (e.g. inside an async\nfunction, or in a Jupyter notebook or async context like FastAPI). For those cases, use\nthe `run` method instead.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\"\"\"\nreturn\nasyncio\n.\nget_event_loop\n()\n.\nrun_until_complete\n(\ncls\n.\nrun\n(\nstarting_agent\n,\ninput\n,\ncontext\n=\ncontext\n,\nmax_turns\n=\nmax_turns\n,\nhooks\n=\nhooks\n,\nrun_config\n=\nrun_config\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\n)\n@classmethod\ndef\nrun_streamed\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResultStreaming\n:\n\"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object\ncontains a method you can use to stream semantic events as they are generated.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA result object that contains data about the run, as well as a method to stream events.\n\"\"\"\nif\nhooks\nis\nNone\n:\nhooks\n=\nRunHooks\n[\nAny\n]()\nif\nrun_config\nis\nNone\n:\nrun_config\n=\nRunConfig\n()\n# If there's already a trace, we don't create a new one. In addition, we can't end the\n# trace here, because the actual work is done in `stream_events` and this method ends\n# before that.\nnew_trace\n=\n(\nNone\nif\nget_current_trace\n()\nelse\ntrace\n(\nworkflow_name\n=\nrun_config\n.\nworkflow_name\n,\ntrace_id\n=\nrun_config\n.\ntrace_id\n,\ngroup_id\n=\nrun_config\n.\ngroup_id\n,\nmetadata\n=\nrun_config\n.\ntrace_metadata\n,\ndisabled\n=\nrun_config\n.\ntracing_disabled\n,\n)\n)\n# Need to start the trace here, because the current trace contextvar is captured at\n# asyncio.create_task time\nif\nnew_trace\n:\nnew_trace\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\noutput_schema\n=\ncls\n.\n_get_output_schema\n(\nstarting_agent\n)\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n]\n=\nRunContextWrapper\n(\ncontext\n=\ncontext\n# type: ignore\n)\nstreamed_result\n=\nRunResultStreaming\n(\ninput\n=\ncopy\n.\ndeepcopy\n(\ninput\n),\nnew_items\n=\n[],\ncurrent_agent\n=\nstarting_agent\n,\nraw_responses\n=\n[],\nfinal_output\n=\nNone\n,\nis_complete\n=\nFalse\n,\ncurrent_turn\n=\n0\n,\nmax_turns\n=\nmax_turns\n,\ninput_guardrail_results\n=\n[],\noutput_guardrail_results\n=\n[],\n_current_agent_output_schema\n=\noutput_schema\n,\n_trace\n=\nnew_trace\n,\n)\n# Kick off the actual agent loop in the background and return the streamed result object.\nstreamed_result\n.\n_run_impl_task\n=\nasyncio\n.\ncreate_task\n(\ncls\n.\n_run_streamed_impl\n(\nstarting_input\n=\ninput\n,\nstreamed_result\n=\nstreamed_result\n,\nstarting_agent\n=\nstarting_agent\n,\nmax_turns\n=\nmax_turns\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\n)\nreturn\nstreamed_result\n@classmethod\nasync\ndef\n_run_input_guardrails_with_queue\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n],\nguardrails\n:\nlist\n[\nInputGuardrail\n[\nTContext\n]],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nstreamed_result\n:\nRunResultStreaming\n,\nparent_span\n:\nSpan\n[\nAny\n],\n):\nqueue\n=\nstreamed_result\n.\n_input_guardrail_queue\n# We'll run the guardrails and push them onto the queue as they complete\nguardrail_tasks\n=\n[\nasyncio\n.\ncreate_task\n(\nRunImpl\n.\nrun_single_input_guardrail\n(\nagent\n,\nguardrail\n,\ninput\n,\ncontext\n)\n)\nfor\nguardrail\nin\nguardrails\n]\nguardrail_results\n=\n[]\ntry\n:\nfor\ndone\nin\nasyncio\n.\nas_completed\n(\nguardrail_tasks\n):\nresult\n=\nawait\ndone\nif\nresult\n.\noutput\n.\ntripwire_triggered\n:\n_error_tracing\n.\nattach_error_to_span\n(\nparent_span\n,\nSpanError\n(\nmessage\n=\n\"Guardrail tripwire triggered\"\n,\ndata\n=\n{\n\"guardrail\"\n:\nresult\n.\nguardrail\n.\nget_name\n(),\n\"type\"\n:\n\"input_guardrail\"\n,\n},\n),\n)\nqueue\n.\nput_nowait\n(\nresult\n)\nguardrail_results\n.\nappend\n(\nresult\n)\nexcept\nException\n:\nfor\nt\nin\nguardrail_tasks\n:\nt\n.\ncancel\n()\nraise\nstreamed_result\n.\ninput_guardrail_results\n=\nguardrail_results\n@classmethod\nasync\ndef\n_run_streamed_impl\n(\ncls\n,\nstarting_input\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nstreamed_result\n:\nRunResultStreaming\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\nmax_turns\n:\nint\n,\nhooks\n:\nRunHooks\n[\nTContext\n],\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n],\nrun_config\n:\nRunConfig\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n):\ncurrent_span\n:\nSpan\n[\nAgentSpanData\n]\n|\nNone\n=\nNone\ncurrent_agent\n=\nstarting_agent\ncurrent_turn\n=\n0\nshould_run_agent_start_hooks\n=\nTrue\ntool_use_tracker\n=\nAgentToolUseTracker\n()\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nAgentUpdatedStreamEvent\n(\nnew_agent\n=\ncurrent_agent\n))\ntry\n:\nwhile\nTrue\n:\nif\nstreamed_result\n.\nis_complete\n:\nbreak\n# Start an agent span if we don't have one. This span is ended if the current\n# agent changes, or if the agent loop ends.\nif\ncurrent_span\nis\nNone\n:\nhandoff_names\n=\n[\nh\n.\nagent_name\nfor\nh\nin\ncls\n.\n_get_handoffs\n(\ncurrent_agent\n)]\nif\noutput_schema\n:=\ncls\n.\n_get_output_schema\n(\ncurrent_agent\n):\noutput_type_name\n=\noutput_schema\n.\noutput_type_name\n()\nelse\n:\noutput_type_name\n=\n\"str\"\ncurrent_span\n=\nagent_span\n(\nname\n=\ncurrent_agent\n.\nname\n,\nhandoffs\n=\nhandoff_names\n,\noutput_type\n=\noutput_type_name\n,\n)\ncurrent_span\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nall_tools\n=\nawait\ncls\n.\n_get_all_tools\n(\ncurrent_agent\n)\ntool_names\n=\n[\nt\n.\nname\nfor\nt\nin\nall_tools\n]\ncurrent_span\n.\nspan_data\n.\ntools\n=\ntool_names\ncurrent_turn\n+=\n1\nstreamed_result\n.\ncurrent_turn\n=\ncurrent_turn\nif\ncurrent_turn\n>\nmax_turns\n:\n_error_tracing\n.\nattach_error_to_span\n(\ncurrent_span\n,\nSpanError\n(\nmessage\n=\n\"Max turns exceeded\"\n,\ndata\n=\n{\n\"max_turns\"\n:\nmax_turns\n},\n),\n)\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nQueueCompleteSentinel\n())\nbreak\nif\ncurrent_turn\n==\n1\n:\n# Run the input guardrails in the background and put the results on the queue\nstreamed_result\n.\n_input_guardrails_task\n=\nasyncio\n.\ncreate_task\n(\ncls\n.\n_run_input_guardrails_with_queue\n(\nstarting_agent\n,\nstarting_agent\n.\ninput_guardrails\n+\n(\nrun_config\n.\ninput_guardrails\nor\n[]),\ncopy\n.\ndeepcopy\n(\nItemHelpers\n.\ninput_to_new_input_list\n(\nstarting_input\n)),\ncontext_wrapper\n,\nstreamed_result\n,\ncurrent_span\n,\n)\n)\ntry\n:\nturn_result\n=\nawait\ncls\n.\n_run_single_turn_streamed\n(\nstreamed_result\n,\ncurrent_agent\n,\nhooks\n,\ncontext_wrapper\n,\nrun_config\n,\nshould_run_agent_start_hooks\n,\ntool_use_tracker\n,\nall_tools\n,\nprevious_response_id\n,\n)\nshould_run_agent_start_hooks\n=\nFalse\nstreamed_result\n.\nraw_responses\n=\nstreamed_result\n.\nraw_responses\n+\n[\nturn_result\n.\nmodel_response\n]\nstreamed_result\n.\ninput\n=\nturn_result\n.\noriginal_input\nstreamed_result\n.\nnew_items\n=\nturn_result\n.\ngenerated_items\nif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepHandoff\n):\ncurrent_agent\n=\nturn_result\n.\nnext_step\n.\nnew_agent\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\ncurrent_span\n=\nNone\nshould_run_agent_start_hooks\n=\nTrue\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nAgentUpdatedStreamEvent\n(\nnew_agent\n=\ncurrent_agent\n)\n)\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepFinalOutput\n):\nstreamed_result\n.\n_output_guardrails_task\n=\nasyncio\n.\ncreate_task\n(\ncls\n.\n_run_output_guardrails\n(\ncurrent_agent\n.\noutput_guardrails\n+\n(\nrun_config\n.\noutput_guardrails\nor\n[]),\ncurrent_agent\n,\nturn_result\n.\nnext_step\n.\noutput\n,\ncontext_wrapper\n,\n)\n)\ntry\n:\noutput_guardrail_results\n=\nawait\nstreamed_result\n.\n_output_guardrails_task\nexcept\nException\n:\n# Exceptions will be checked in the stream_events loop\noutput_guardrail_results\n=\n[]\nstreamed_result\n.\noutput_guardrail_results\n=\noutput_guardrail_results\nstreamed_result\n.\nfinal_output\n=\nturn_result\n.\nnext_step\n.\noutput\nstreamed_result\n.\nis_complete\n=\nTrue\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nQueueCompleteSentinel\n())\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepRunAgain\n):\npass\nexcept\nException\nas\ne\n:\nif\ncurrent_span\n:\n_error_tracing\n.\nattach_error_to_span\n(\ncurrent_span\n,\nSpanError\n(\nmessage\n=\n\"Error in agent run\"\n,\ndata\n=\n{\n\"error\"\n:\nstr\n(\ne\n)},\n),\n)\nstreamed_result\n.\nis_complete\n=\nTrue\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nQueueCompleteSentinel\n())\nraise\nstreamed_result\n.\nis_complete\n=\nTrue\nfinally\n:\nif\ncurrent_span\n:\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\n@classmethod\nasync\ndef\n_run_single_turn_streamed\n(\ncls\n,\nstreamed_result\n:\nRunResultStreaming\n,\nagent\n:\nAgent\n[\nTContext\n],\nhooks\n:\nRunHooks\n[\nTContext\n],\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n],\nrun_config\n:\nRunConfig\n,\nshould_run_agent_start_hooks\n:\nbool\n,\ntool_use_tracker\n:\nAgentToolUseTracker\n,\nall_tools\n:\nlist\n[\nTool\n],\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nSingleStepResult\n:\nif\nshould_run_agent_start_hooks\n:\nawait\nasyncio\n.\ngather\n(\nhooks\n.\non_agent_start\n(\ncontext_wrapper\n,\nagent\n),\n(\nagent\n.\nhooks\n.\non_start\n(\ncontext_wrapper\n,\nagent\n)\nif\nagent\n.\nhooks\nelse\n_coro\n.\nnoop_coroutine\n()\n),\n)\noutput_schema\n=\ncls\n.\n_get_output_schema\n(\nagent\n)\nstreamed_result\n.\ncurrent_agent\n=\nagent\nstreamed_result\n.\n_current_agent_output_schema\n=\noutput_schema\nsystem_prompt\n=\nawait\nagent\n.\nget_system_prompt\n(\ncontext_wrapper\n)\nhandoffs\n=\ncls\n.\n_get_handoffs\n(\nagent\n)\nmodel\n=\ncls\n.\n_get_model\n(\nagent\n,\nrun_config\n)\nmodel_settings\n=\nagent\n.\nmodel_settings\n.\nresolve\n(\nrun_config\n.\nmodel_settings\n)\nmodel_settings\n=\nRunImpl\n.\nmaybe_reset_tool_choice\n(\nagent\n,\ntool_use_tracker\n,\nmodel_settings\n)\nfinal_response\n:\nModelResponse\n|\nNone\n=\nNone\ninput\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\nstreamed_result\n.\ninput\n)\ninput\n.\nextend\n([\nitem\n.\nto_input_item\n()\nfor\nitem\nin\nstreamed_result\n.\nnew_items\n])\n# 1. Stream the output events\nasync\nfor\nevent\nin\nmodel\n.\nstream_response\n(\nsystem_prompt\n,\ninput\n,\nmodel_settings\n,\nall_tools\n,\noutput_schema\n,\nhandoffs\n,\nget_model_tracing_impl\n(\nrun_config\n.\ntracing_disabled\n,\nrun_config\n.\ntrace_include_sensitive_data\n),\nprevious_response_id\n=\nprevious_response_id\n,\n):\nif\nisinstance\n(\nevent\n,\nResponseCompletedEvent\n):\nusage\n=\n(\nUsage\n(\nrequests\n=\n1\n,\ninput_tokens\n=\nevent\n.\nresponse\n.\nusage\n.\ninput_tokens\n,\noutput_tokens\n=\nevent\n.\nresponse\n.\nusage\n.\noutput_tokens\n,\ntotal_tokens\n=\nevent\n.\nresponse\n.\nusage\n.\ntotal_tokens\n,\n)\nif\nevent\n.\nresponse\n.\nusage\nelse\nUsage\n()\n)\nfinal_response\n=\nModelResponse\n(\noutput\n=\nevent\n.\nresponse\n.\noutput\n,\nusage\n=\nusage\n,\nresponse_id\n=\nevent\n.\nresponse\n.\nid\n,\n)\nstreamed_result\n.\n_event_queue\n.\nput_nowait\n(\nRawResponsesStreamEvent\n(\ndata\n=\nevent\n))\n# 2. At this point, the streaming is complete for this turn of the agent loop.\nif\nnot\nfinal_response\n:\nraise\nModelBehaviorError\n(\n\"Model did not produce a final response!\"\n)\n# 3. Now, we can process the turn as we do in the non-streaming case\nsingle_step_result\n=\nawait\ncls\n.\n_get_single_step_result_from_response\n(\nagent\n=\nagent\n,\noriginal_input\n=\nstreamed_result\n.\ninput\n,\npre_step_items\n=\nstreamed_result\n.\nnew_items\n,\nnew_response\n=\nfinal_response\n,\noutput_schema\n=\noutput_schema\n,\nall_tools\n=\nall_tools\n,\nhandoffs\n=\nhandoffs\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\n)\nRunImpl\n.\nstream_step_result_to_queue\n(\nsingle_step_result\n,\nstreamed_result\n.\n_event_queue\n)\nreturn\nsingle_step_result\n@classmethod\nasync\ndef\n_run_single_turn\n(\ncls\n,\n*\n,\nagent\n:\nAgent\n[\nTContext\n],\nall_tools\n:\nlist\n[\nTool\n],\noriginal_input\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ngenerated_items\n:\nlist\n[\nRunItem\n],\nhooks\n:\nRunHooks\n[\nTContext\n],\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n],\nrun_config\n:\nRunConfig\n,\nshould_run_agent_start_hooks\n:\nbool\n,\ntool_use_tracker\n:\nAgentToolUseTracker\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nSingleStepResult\n:\n# Ensure we run the hooks before anything else\nif\nshould_run_agent_start_hooks\n:\nawait\nasyncio\n.\ngather\n(\nhooks\n.\non_agent_start\n(\ncontext_wrapper\n,\nagent\n),\n(\nagent\n.\nhooks\n.\non_start\n(\ncontext_wrapper\n,\nagent\n)\nif\nagent\n.\nhooks\nelse\n_coro\n.\nnoop_coroutine\n()\n),\n)\nsystem_prompt\n=\nawait\nagent\n.\nget_system_prompt\n(\ncontext_wrapper\n)\noutput_schema\n=\ncls\n.\n_get_output_schema\n(\nagent\n)\nhandoffs\n=\ncls\n.\n_get_handoffs\n(\nagent\n)\ninput\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\noriginal_input\n)\ninput\n.\nextend\n([\ngenerated_item\n.\nto_input_item\n()\nfor\ngenerated_item\nin\ngenerated_items\n])\nnew_response\n=\nawait\ncls\n.\n_get_new_response\n(\nagent\n,\nsystem_prompt\n,\ninput\n,\noutput_schema\n,\nall_tools\n,\nhandoffs\n,\ncontext_wrapper\n,\nrun_config\n,\ntool_use_tracker\n,\nprevious_response_id\n,\n)\nreturn\nawait\ncls\n.\n_get_single_step_result_from_response\n(\nagent\n=\nagent\n,\noriginal_input\n=\noriginal_input\n,\npre_step_items\n=\ngenerated_items\n,\nnew_response\n=\nnew_response\n,\noutput_schema\n=\noutput_schema\n,\nall_tools\n=\nall_tools\n,\nhandoffs\n=\nhandoffs\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\n)\n@classmethod\nasync\ndef\n_get_single_step_result_from_response\n(\ncls\n,\n*\n,\nagent\n:\nAgent\n[\nTContext\n],\nall_tools\n:\nlist\n[\nTool\n],\noriginal_input\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\npre_step_items\n:\nlist\n[\nRunItem\n],\nnew_response\n:\nModelResponse\n,\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nhooks\n:\nRunHooks\n[\nTContext\n],\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n],\nrun_config\n:\nRunConfig\n,\ntool_use_tracker\n:\nAgentToolUseTracker\n,\n)\n->\nSingleStepResult\n:\nprocessed_response\n=\nRunImpl\n.\nprocess_model_response\n(\nagent\n=\nagent\n,\nall_tools\n=\nall_tools\n,\nresponse\n=\nnew_response\n,\noutput_schema\n=\noutput_schema\n,\nhandoffs\n=\nhandoffs\n,\n)\ntool_use_tracker\n.\nadd_tool_use\n(\nagent\n,\nprocessed_response\n.\ntools_used\n)\nreturn\nawait\nRunImpl\n.\nexecute_tools_and_side_effects\n(\nagent\n=\nagent\n,\noriginal_input\n=\noriginal_input\n,\npre_step_items\n=\npre_step_items\n,\nnew_response\n=\nnew_response\n,\nprocessed_response\n=\nprocessed_response\n,\noutput_schema\n=\noutput_schema\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\n)\n@classmethod\nasync\ndef\n_run_input_guardrails\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n],\nguardrails\n:\nlist\n[\nInputGuardrail\n[\nTContext\n]],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\n)\n->\nlist\n[\nInputGuardrailResult\n]:\nif\nnot\nguardrails\n:\nreturn\n[]\nguardrail_tasks\n=\n[\nasyncio\n.\ncreate_task\n(\nRunImpl\n.\nrun_single_input_guardrail\n(\nagent\n,\nguardrail\n,\ninput\n,\ncontext\n)\n)\nfor\nguardrail\nin\nguardrails\n]\nguardrail_results\n=\n[]\nfor\ndone\nin\nasyncio\n.\nas_completed\n(\nguardrail_tasks\n):\nresult\n=\nawait\ndone\nif\nresult\n.\noutput\n.\ntripwire_triggered\n:\n# Cancel all guardrail tasks if a tripwire is triggered.\nfor\nt\nin\nguardrail_tasks\n:\nt\n.\ncancel\n()\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Guardrail tripwire triggered\"\n,\ndata\n=\n{\n\"guardrail\"\n:\nresult\n.\nguardrail\n.\nget_name\n()},\n)\n)\nraise\nInputGuardrailTripwireTriggered\n(\nresult\n)\nelse\n:\nguardrail_results\n.\nappend\n(\nresult\n)\nreturn\nguardrail_results\n@classmethod\nasync\ndef\n_run_output_guardrails\n(\ncls\n,\nguardrails\n:\nlist\n[\nOutputGuardrail\n[\nTContext\n]],\nagent\n:\nAgent\n[\nTContext\n],\nagent_output\n:\nAny\n,\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\n)\n->\nlist\n[\nOutputGuardrailResult\n]:\nif\nnot\nguardrails\n:\nreturn\n[]\nguardrail_tasks\n=\n[\nasyncio\n.\ncreate_task\n(\nRunImpl\n.\nrun_single_output_guardrail\n(\nguardrail\n,\nagent\n,\nagent_output\n,\ncontext\n)\n)\nfor\nguardrail\nin\nguardrails\n]\nguardrail_results\n=\n[]\nfor\ndone\nin\nasyncio\n.\nas_completed\n(\nguardrail_tasks\n):\nresult\n=\nawait\ndone\nif\nresult\n.\noutput\n.\ntripwire_triggered\n:\n# Cancel all guardrail tasks if a tripwire is triggered.\nfor\nt\nin\nguardrail_tasks\n:\nt\n.\ncancel\n()\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Guardrail tripwire triggered\"\n,\ndata\n=\n{\n\"guardrail\"\n:\nresult\n.\nguardrail\n.\nget_name\n()},\n)\n)\nraise\nOutputGuardrailTripwireTriggered\n(\nresult\n)\nelse\n:\nguardrail_results\n.\nappend\n(\nresult\n)\nreturn\nguardrail_results\n@classmethod\nasync\ndef\n_get_new_response\n(\ncls\n,\nagent\n:\nAgent\n[\nTContext\n],\nsystem_prompt\n:\nstr\n|\nNone\n,\ninput\n:\nlist\n[\nTResponseInputItem\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nall_tools\n:\nlist\n[\nTool\n],\nhandoffs\n:\nlist\n[\nHandoff\n],\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n],\nrun_config\n:\nRunConfig\n,\ntool_use_tracker\n:\nAgentToolUseTracker\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\nmodel\n=\ncls\n.\n_get_model\n(\nagent\n,\nrun_config\n)\nmodel_settings\n=\nagent\n.\nmodel_settings\n.\nresolve\n(\nrun_config\n.\nmodel_settings\n)\nmodel_settings\n=\nRunImpl\n.\nmaybe_reset_tool_choice\n(\nagent\n,\ntool_use_tracker\n,\nmodel_settings\n)\nnew_response\n=\nawait\nmodel\n.\nget_response\n(\nsystem_instructions\n=\nsystem_prompt\n,\ninput\n=\ninput\n,\nmodel_settings\n=\nmodel_settings\n,\ntools\n=\nall_tools\n,\noutput_schema\n=\noutput_schema\n,\nhandoffs\n=\nhandoffs\n,\ntracing\n=\nget_model_tracing_impl\n(\nrun_config\n.\ntracing_disabled\n,\nrun_config\n.\ntrace_include_sensitive_data\n),\nprevious_response_id\n=\nprevious_response_id\n,\n)\ncontext_wrapper\n.\nusage\n.\nadd\n(\nnew_response\n.\nusage\n)\nreturn\nnew_response\n@classmethod\ndef\n_get_output_schema\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nAgentOutputSchema\n|\nNone\n:\nif\nagent\n.\noutput_type\nis\nNone\nor\nagent\n.\noutput_type\nis\nstr\n:\nreturn\nNone\nreturn\nAgentOutputSchema\n(\nagent\n.\noutput_type\n)\n@classmethod\ndef\n_get_handoffs\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nlist\n[\nHandoff\n]:\nhandoffs\n=\n[]\nfor\nhandoff_item\nin\nagent\n.\nhandoffs\n:\nif\nisinstance\n(\nhandoff_item\n,\nHandoff\n):\nhandoffs\n.\nappend\n(\nhandoff_item\n)\nelif\nisinstance\n(\nhandoff_item\n,\nAgent\n):\nhandoffs\n.\nappend\n(\nhandoff\n(\nhandoff_item\n))\nreturn\nhandoffs\n@classmethod\nasync\ndef\n_get_all_tools\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n])\n->\nlist\n[\nTool\n]:\nreturn\nawait\nagent\n.\nget_all_tools\n()\n@classmethod\ndef\n_get_model\n(\ncls\n,\nagent\n:\nAgent\n[\nAny\n],\nrun_config\n:\nRunConfig\n)\n->\nModel\n:\nif\nisinstance\n(\nrun_config\n.\nmodel\n,\nModel\n):\nreturn\nrun_config\n.\nmodel\nelif\nisinstance\n(\nrun_config\n.\nmodel\n,\nstr\n):\nreturn\nrun_config\n.\nmodel_provider\n.\nget_model\n(\nrun_config\n.\nmodel\n)\nelif\nisinstance\n(\nagent\n.\nmodel\n,\nModel\n):\nreturn\nagent\n.\nmodel\nreturn\nrun_config\n.\nmodel_provider\n.\nget_model\n(\nagent\n.\nmodel\n)\nrun\nasync\nclassmethod\nrun\n(\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\nRun a workflow starting at the given agent. The agent will run in a loop until a final\noutput is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\nagent.output_type\n, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nParameters:\nName\nType\nDescription\nDefault\nstarting_agent\nAgent\n[\nTContext\n]\nThe starting agent to run.\nrequired\ninput\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\nrequired\ncontext\nTContext\n| None\nThe context to run the agent with.\nNone\nmax_turns\nint\nThe maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nDEFAULT_MAX_TURNS\nhooks\nRunHooks\n[\nTContext\n] | None\nAn object that receives callbacks on various lifecycle events.\nNone\nrun_config\nRunConfig\n| None\nGlobal settings for the entire agent run.\nNone\nprevious_response_id\nstr\n| None\nThe ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nNone\nReturns:\nType\nDescription\nRunResult\nA run result containing all the inputs, guardrail results and the output of the last\nRunResult\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\nSource code in\nsrc/agents/run.py\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n@classmethod\nasync\ndef\nrun\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\n:\n\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\noutput is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\"\"\"\nif\nhooks\nis\nNone\n:\nhooks\n=\nRunHooks\n[\nAny\n]()\nif\nrun_config\nis\nNone\n:\nrun_config\n=\nRunConfig\n()\ntool_use_tracker\n=\nAgentToolUseTracker\n()\nwith\nTraceCtxManager\n(\nworkflow_name\n=\nrun_config\n.\nworkflow_name\n,\ntrace_id\n=\nrun_config\n.\ntrace_id\n,\ngroup_id\n=\nrun_config\n.\ngroup_id\n,\nmetadata\n=\nrun_config\n.\ntrace_metadata\n,\ndisabled\n=\nrun_config\n.\ntracing_disabled\n,\n):\ncurrent_turn\n=\n0\noriginal_input\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\n=\ncopy\n.\ndeepcopy\n(\ninput\n)\ngenerated_items\n:\nlist\n[\nRunItem\n]\n=\n[]\nmodel_responses\n:\nlist\n[\nModelResponse\n]\n=\n[]\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n]\n=\nRunContextWrapper\n(\ncontext\n=\ncontext\n,\n# type: ignore\n)\ninput_guardrail_results\n:\nlist\n[\nInputGuardrailResult\n]\n=\n[]\ncurrent_span\n:\nSpan\n[\nAgentSpanData\n]\n|\nNone\n=\nNone\ncurrent_agent\n=\nstarting_agent\nshould_run_agent_start_hooks\n=\nTrue\ntry\n:\nwhile\nTrue\n:\n# Start an agent span if we don't have one. This span is ended if the current\n# agent changes, or if the agent loop ends.\nif\ncurrent_span\nis\nNone\n:\nhandoff_names\n=\n[\nh\n.\nagent_name\nfor\nh\nin\ncls\n.\n_get_handoffs\n(\ncurrent_agent\n)]\nif\noutput_schema\n:=\ncls\n.\n_get_output_schema\n(\ncurrent_agent\n):\noutput_type_name\n=\noutput_schema\n.\noutput_type_name\n()\nelse\n:\noutput_type_name\n=\n\"str\"\ncurrent_span\n=\nagent_span\n(\nname\n=\ncurrent_agent\n.\nname\n,\nhandoffs\n=\nhandoff_names\n,\noutput_type\n=\noutput_type_name\n,\n)\ncurrent_span\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nall_tools\n=\nawait\ncls\n.\n_get_all_tools\n(\ncurrent_agent\n)\ncurrent_span\n.\nspan_data\n.\ntools\n=\n[\nt\n.\nname\nfor\nt\nin\nall_tools\n]\ncurrent_turn\n+=\n1\nif\ncurrent_turn\n>\nmax_turns\n:\n_error_tracing\n.\nattach_error_to_span\n(\ncurrent_span\n,\nSpanError\n(\nmessage\n=\n\"Max turns exceeded\"\n,\ndata\n=\n{\n\"max_turns\"\n:\nmax_turns\n},\n),\n)\nraise\nMaxTurnsExceeded\n(\nf\n\"Max turns (\n{\nmax_turns\n}\n) exceeded\"\n)\nlogger\n.\ndebug\n(\nf\n\"Running agent\n{\ncurrent_agent\n.\nname\n}\n(turn\n{\ncurrent_turn\n}\n)\"\n,\n)\nif\ncurrent_turn\n==\n1\n:\ninput_guardrail_results\n,\nturn_result\n=\nawait\nasyncio\n.\ngather\n(\ncls\n.\n_run_input_guardrails\n(\nstarting_agent\n,\nstarting_agent\n.\ninput_guardrails\n+\n(\nrun_config\n.\ninput_guardrails\nor\n[]),\ncopy\n.\ndeepcopy\n(\ninput\n),\ncontext_wrapper\n,\n),\ncls\n.\n_run_single_turn\n(\nagent\n=\ncurrent_agent\n,\nall_tools\n=\nall_tools\n,\noriginal_input\n=\noriginal_input\n,\ngenerated_items\n=\ngenerated_items\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nshould_run_agent_start_hooks\n=\nshould_run_agent_start_hooks\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\nprevious_response_id\n=\nprevious_response_id\n,\n),\n)\nelse\n:\nturn_result\n=\nawait\ncls\n.\n_run_single_turn\n(\nagent\n=\ncurrent_agent\n,\nall_tools\n=\nall_tools\n,\noriginal_input\n=\noriginal_input\n,\ngenerated_items\n=\ngenerated_items\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nshould_run_agent_start_hooks\n=\nshould_run_agent_start_hooks\n,\ntool_use_tracker\n=\ntool_use_tracker\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\nshould_run_agent_start_hooks\n=\nFalse\nmodel_responses\n.\nappend\n(\nturn_result\n.\nmodel_response\n)\noriginal_input\n=\nturn_result\n.\noriginal_input\ngenerated_items\n=\nturn_result\n.\ngenerated_items\nif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepFinalOutput\n):\noutput_guardrail_results\n=\nawait\ncls\n.\n_run_output_guardrails\n(\ncurrent_agent\n.\noutput_guardrails\n+\n(\nrun_config\n.\noutput_guardrails\nor\n[]),\ncurrent_agent\n,\nturn_result\n.\nnext_step\n.\noutput\n,\ncontext_wrapper\n,\n)\nreturn\nRunResult\n(\ninput\n=\noriginal_input\n,\nnew_items\n=\ngenerated_items\n,\nraw_responses\n=\nmodel_responses\n,\nfinal_output\n=\nturn_result\n.\nnext_step\n.\noutput\n,\n_last_agent\n=\ncurrent_agent\n,\ninput_guardrail_results\n=\ninput_guardrail_results\n,\noutput_guardrail_results\n=\noutput_guardrail_results\n,\n)\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepHandoff\n):\ncurrent_agent\n=\ncast\n(\nAgent\n[\nTContext\n],\nturn_result\n.\nnext_step\n.\nnew_agent\n)\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\ncurrent_span\n=\nNone\nshould_run_agent_start_hooks\n=\nTrue\nelif\nisinstance\n(\nturn_result\n.\nnext_step\n,\nNextStepRunAgain\n):\npass\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unknown next step type:\n{\ntype\n(\nturn_result\n.\nnext_step\n)\n}\n\"\n)\nfinally\n:\nif\ncurrent_span\n:\ncurrent_span\n.\nfinish\n(\nreset_current\n=\nTrue\n)\nrun_sync\nclassmethod\nrun_sync\n(\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\nRun a workflow synchronously, starting at the given agent. Note that this just wraps the\nrun\nmethod, so it will not work if there's already an event loop (e.g. inside an async\nfunction, or in a Jupyter notebook or async context like FastAPI). For those cases, use\nthe\nrun\nmethod instead.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\nagent.output_type\n, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nParameters:\nName\nType\nDescription\nDefault\nstarting_agent\nAgent\n[\nTContext\n]\nThe starting agent to run.\nrequired\ninput\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\nrequired\ncontext\nTContext\n| None\nThe context to run the agent with.\nNone\nmax_turns\nint\nThe maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nDEFAULT_MAX_TURNS\nhooks\nRunHooks\n[\nTContext\n] | None\nAn object that receives callbacks on various lifecycle events.\nNone\nrun_config\nRunConfig\n| None\nGlobal settings for the entire agent run.\nNone\nprevious_response_id\nstr\n| None\nThe ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nNone\nReturns:\nType\nDescription\nRunResult\nA run result containing all the inputs, guardrail results and the output of the last\nRunResult\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\nSource code in\nsrc/agents/run.py\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n@classmethod\ndef\nrun_sync\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n*\n,\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResult\n:\n\"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\n`run` method, so it will not work if there's already an event loop (e.g. inside an async\nfunction, or in a Jupyter notebook or async context like FastAPI). For those cases, use\nthe `run` method instead.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\"\"\"\nreturn\nasyncio\n.\nget_event_loop\n()\n.\nrun_until_complete\n(\ncls\n.\nrun\n(\nstarting_agent\n,\ninput\n,\ncontext\n=\ncontext\n,\nmax_turns\n=\nmax_turns\n,\nhooks\n=\nhooks\n,\nrun_config\n=\nrun_config\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\n)\nrun_streamed\nclassmethod\nrun_streamed\n(\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResultStreaming\nRun a workflow starting at the given agent in streaming mode. The returned result object\ncontains a method you can use to stream semantic events as they are generated.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\nagent.output_type\n, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nParameters:\nName\nType\nDescription\nDefault\nstarting_agent\nAgent\n[\nTContext\n]\nThe starting agent to run.\nrequired\ninput\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\nrequired\ncontext\nTContext\n| None\nThe context to run the agent with.\nNone\nmax_turns\nint\nThe maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nDEFAULT_MAX_TURNS\nhooks\nRunHooks\n[\nTContext\n] | None\nAn object that receives callbacks on various lifecycle events.\nNone\nrun_config\nRunConfig\n| None\nGlobal settings for the entire agent run.\nNone\nprevious_response_id\nstr\n| None\nThe ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nNone\nReturns:\n    A result object that contains data about the run, as well as a method to stream events.\nSource code in\nsrc/agents/run.py\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n@classmethod\ndef\nrun_streamed\n(\ncls\n,\nstarting_agent\n:\nAgent\n[\nTContext\n],\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\ncontext\n:\nTContext\n|\nNone\n=\nNone\n,\nmax_turns\n:\nint\n=\nDEFAULT_MAX_TURNS\n,\nhooks\n:\nRunHooks\n[\nTContext\n]\n|\nNone\n=\nNone\n,\nrun_config\n:\nRunConfig\n|\nNone\n=\nNone\n,\nprevious_response_id\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nRunResultStreaming\n:\n\"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object\ncontains a method you can use to stream semantic events as they are generated.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1. The agent is invoked with the given input.\n2. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3. If there's a handoff, we run the loop again, with the new agent.\n4. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun_config: Global settings for the entire agent run.\nprevious_response_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA result object that contains data about the run, as well as a method to stream events.\n\"\"\"\nif\nhooks\nis\nNone\n:\nhooks\n=\nRunHooks\n[\nAny\n]()\nif\nrun_config\nis\nNone\n:\nrun_config\n=\nRunConfig\n()\n# If there's already a trace, we don't create a new one. In addition, we can't end the\n# trace here, because the actual work is done in `stream_events` and this method ends\n# before that.\nnew_trace\n=\n(\nNone\nif\nget_current_trace\n()\nelse\ntrace\n(\nworkflow_name\n=\nrun_config\n.\nworkflow_name\n,\ntrace_id\n=\nrun_config\n.\ntrace_id\n,\ngroup_id\n=\nrun_config\n.\ngroup_id\n,\nmetadata\n=\nrun_config\n.\ntrace_metadata\n,\ndisabled\n=\nrun_config\n.\ntracing_disabled\n,\n)\n)\n# Need to start the trace here, because the current trace contextvar is captured at\n# asyncio.create_task time\nif\nnew_trace\n:\nnew_trace\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\noutput_schema\n=\ncls\n.\n_get_output_schema\n(\nstarting_agent\n)\ncontext_wrapper\n:\nRunContextWrapper\n[\nTContext\n]\n=\nRunContextWrapper\n(\ncontext\n=\ncontext\n# type: ignore\n)\nstreamed_result\n=\nRunResultStreaming\n(\ninput\n=\ncopy\n.\ndeepcopy\n(\ninput\n),\nnew_items\n=\n[],\ncurrent_agent\n=\nstarting_agent\n,\nraw_responses\n=\n[],\nfinal_output\n=\nNone\n,\nis_complete\n=\nFalse\n,\ncurrent_turn\n=\n0\n,\nmax_turns\n=\nmax_turns\n,\ninput_guardrail_results\n=\n[],\noutput_guardrail_results\n=\n[],\n_current_agent_output_schema\n=\noutput_schema\n,\n_trace\n=\nnew_trace\n,\n)\n# Kick off the actual agent loop in the background and return the streamed result object.\nstreamed_result\n.\n_run_impl_task\n=\nasyncio\n.\ncreate_task\n(\ncls\n.\n_run_streamed_impl\n(\nstarting_input\n=\ninput\n,\nstreamed_result\n=\nstreamed_result\n,\nstarting_agent\n=\nstarting_agent\n,\nmax_turns\n=\nmax_turns\n,\nhooks\n=\nhooks\n,\ncontext_wrapper\n=\ncontext_wrapper\n,\nrun_config\n=\nrun_config\n,\nprevious_response_id\n=\nprevious_response_id\n,\n)\n)\nreturn\nstreamed_result\nRunConfig\ndataclass\nConfigures settings for the entire agent run.\nSource code in\nsrc/agents/run.py\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n@dataclass\nclass\nRunConfig\n:\n\"\"\"Configures settings for the entire agent run.\"\"\"\nmodel\n:\nstr\n|\nModel\n|\nNone\n=\nNone\n\"\"\"The model to use for the entire agent run. If set, will override the model set on every\nagent. The model_provider passed in below must be able to resolve this model name.\n\"\"\"\nmodel_provider\n:\nModelProvider\n=\nfield\n(\ndefault_factory\n=\nOpenAIProvider\n)\n\"\"\"The model provider to use when looking up string model names. Defaults to OpenAI.\"\"\"\nmodel_settings\n:\nModelSettings\n|\nNone\n=\nNone\n\"\"\"Configure global model settings. Any non-null values will override the agent-specific model\nsettings.\n\"\"\"\nhandoff_input_filter\n:\nHandoffInputFilter\n|\nNone\n=\nNone\n\"\"\"A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that\nwill take precedence. The input filter allows you to edit the inputs that are sent to the new\nagent. See the documentation in `Handoff.input_filter` for more details.\n\"\"\"\ninput_guardrails\n:\nlist\n[\nInputGuardrail\n[\nAny\n]]\n|\nNone\n=\nNone\n\"\"\"A list of input guardrails to run on the initial run input.\"\"\"\noutput_guardrails\n:\nlist\n[\nOutputGuardrail\n[\nAny\n]]\n|\nNone\n=\nNone\n\"\"\"A list of output guardrails to run on the final output of the run.\"\"\"\ntracing_disabled\n:\nbool\n=\nFalse\n\"\"\"Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\n\"\"\"\ntrace_include_sensitive_data\n:\nbool\n=\nTrue\n\"\"\"Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or\nLLM generations) in traces. If False, we'll still create spans for these events, but the\nsensitive data will not be included.\n\"\"\"\nworkflow_name\n:\nstr\n=\n\"Agent workflow\"\n\"\"\"The name of the run, used for tracing. Should be a logical name for the run, like\n\"Code generation workflow\" or \"Customer support agent\".\n\"\"\"\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n\"\"\"A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\"\"\"\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n\"\"\"\nA grouping identifier to use for tracing, to link multiple traces from the same conversation\nor process. For example, you might use a chat thread ID.\n\"\"\"\ntrace_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n\"\"\"\nAn optional dictionary of additional metadata to include with the trace.\n\"\"\"\nmodel\nclass-attribute\ninstance-attribute\nmodel\n:\nstr\n|\nModel\n|\nNone\n=\nNone\nThe model to use for the entire agent run. If set, will override the model set on every\nagent. The model_provider passed in below must be able to resolve this model name.\nmodel_provider\nclass-attribute\ninstance-attribute\nmodel_provider\n:\nModelProvider\n=\nfield\n(\ndefault_factory\n=\nOpenAIProvider\n)\nThe model provider to use when looking up string model names. Defaults to OpenAI.\nmodel_settings\nclass-attribute\ninstance-attribute\nmodel_settings\n:\nModelSettings\n|\nNone\n=\nNone\nConfigure global model settings. Any non-null values will override the agent-specific model\nsettings.\nhandoff_input_filter\nclass-attribute\ninstance-attribute\nhandoff_input_filter\n:\nHandoffInputFilter\n|\nNone\n=\nNone\nA global input filter to apply to all handoffs. If\nHandoff.input_filter\nis set, then that\nwill take precedence. The input filter allows you to edit the inputs that are sent to the new\nagent. See the documentation in\nHandoff.input_filter\nfor more details.\ninput_guardrails\nclass-attribute\ninstance-attribute\ninput_guardrails\n:\nlist\n[\nInputGuardrail\n[\nAny\n]]\n|\nNone\n=\nNone\nA list of input guardrails to run on the initial run input.\noutput_guardrails\nclass-attribute\ninstance-attribute\noutput_guardrails\n:\nlist\n[\nOutputGuardrail\n[\nAny\n]]\n|\nNone\n=\nNone\nA list of output guardrails to run on the final output of the run.\ntracing_disabled\nclass-attribute\ninstance-attribute\ntracing_disabled\n:\nbool\n=\nFalse\nWhether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\ntrace_include_sensitive_data\nclass-attribute\ninstance-attribute\ntrace_include_sensitive_data\n:\nbool\n=\nTrue\nWhether we include potentially sensitive data (for example: inputs/outputs of tool calls or\nLLM generations) in traces. If False, we'll still create spans for these events, but the\nsensitive data will not be included.\nworkflow_name\nclass-attribute\ninstance-attribute\nworkflow_name\n:\nstr\n=\n'Agent workflow'\nThe name of the run, used for tracing. Should be a logical name for the run, like\n\"Code generation workflow\" or \"Customer support agent\".\ntrace_id\nclass-attribute\ninstance-attribute\ntrace_id\n:\nstr\n|\nNone\n=\nNone\nA custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\ngroup_id\nclass-attribute\ninstance-attribute\ngroup_id\n:\nstr\n|\nNone\n=\nNone\nA grouping identifier to use for tracing, to link multiple traces from the same conversation\nor process. For example, you might use a chat thread ID.\ntrace_metadata\nclass-attribute\ninstance-attribute\ntrace_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nAn optional dictionary of additional metadata to include with the trace.",
  "Streaming events": "Streaming events\nStreamEvent\nmodule-attribute\nStreamEvent\n:\nTypeAlias\n=\nUnion\n[\nRawResponsesStreamEvent\n,\nRunItemStreamEvent\n,\nAgentUpdatedStreamEvent\n,\n]\nA streaming event from an agent.\nRawResponsesStreamEvent\ndataclass\nStreaming event from the LLM. These are 'raw' events, i.e. they are directly passed through\nfrom the LLM.\nSource code in\nsrc/agents/stream_events.py\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n@dataclass\nclass\nRawResponsesStreamEvent\n:\n\"\"\"Streaming event from the LLM. These are 'raw' events, i.e. they are directly passed through\nfrom the LLM.\n\"\"\"\ndata\n:\nTResponseStreamEvent\n\"\"\"The raw responses streaming event from the LLM.\"\"\"\ntype\n:\nLiteral\n[\n\"raw_response_event\"\n]\n=\n\"raw_response_event\"\n\"\"\"The type of the event.\"\"\"\ndata\ninstance-attribute\ndata\n:\nTResponseStreamEvent\nThe raw responses streaming event from the LLM.\ntype\nclass-attribute\ninstance-attribute\ntype\n:\nLiteral\n[\n'raw_response_event'\n]\n=\n'raw_response_event'\nThe type of the event.\nRunItemStreamEvent\ndataclass\nStreaming events that wrap a\nRunItem\n. As the agent processes the LLM response, it will\ngenerate these events for new messages, tool calls, tool outputs, handoffs, etc.\nSource code in\nsrc/agents/stream_events.py\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n@dataclass\nclass\nRunItemStreamEvent\n:\n\"\"\"Streaming events that wrap a `RunItem`. As the agent processes the LLM response, it will\ngenerate these events for new messages, tool calls, tool outputs, handoffs, etc.\n\"\"\"\nname\n:\nLiteral\n[\n\"message_output_created\"\n,\n\"handoff_requested\"\n,\n\"handoff_occured\"\n,\n\"tool_called\"\n,\n\"tool_output\"\n,\n\"reasoning_item_created\"\n,\n]\n\"\"\"The name of the event.\"\"\"\nitem\n:\nRunItem\n\"\"\"The item that was created.\"\"\"\ntype\n:\nLiteral\n[\n\"run_item_stream_event\"\n]\n=\n\"run_item_stream_event\"\nname\ninstance-attribute\nname\n:\nLiteral\n[\n\"message_output_created\"\n,\n\"handoff_requested\"\n,\n\"handoff_occured\"\n,\n\"tool_called\"\n,\n\"tool_output\"\n,\n\"reasoning_item_created\"\n,\n]\nThe name of the event.\nitem\ninstance-attribute\nitem\n:\nRunItem\nThe item that was created.\nAgentUpdatedStreamEvent\ndataclass\nEvent that notifies that there is a new agent running.\nSource code in\nsrc/agents/stream_events.py\n47\n48\n49\n50\n51\n52\n53\n54\n@dataclass\nclass\nAgentUpdatedStreamEvent\n:\n\"\"\"Event that notifies that there is a new agent running.\"\"\"\nnew_agent\n:\nAgent\n[\nAny\n]\n\"\"\"The new agent.\"\"\"\ntype\n:\nLiteral\n[\n\"agent_updated_stream_event\"\n]\n=\n\"agent_updated_stream_event\"\nnew_agent\ninstance-attribute\nnew_agent\n:\nAgent\n[\nAny\n]\nThe new agent.",
  "Lifecycle": "Lifecycle\nRunHooks\nBases:\nGeneric\n[\nTContext\n]\nA class that receives callbacks on various lifecycle events in an agent run. Subclass and\noverride the methods you need.\non_agent_start\nasync\non_agent_start\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\n)\n->\nNone\nCalled before the agent is invoked. Called each time the current agent changes.\non_agent_end\nasync\non_agent_end\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\noutput\n:\nAny\n,\n)\n->\nNone\nCalled when the agent produces a final output.\non_handoff\nasync\non_handoff\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nfrom_agent\n:\nAgent\n[\nTContext\n],\nto_agent\n:\nAgent\n[\nTContext\n],\n)\n->\nNone\nCalled when a handoff occurs.\non_tool_start\nasync\non_tool_start\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\ntool\n:\nTool\n,\n)\n->\nNone\nCalled before a tool is invoked.\non_tool_end\nasync\non_tool_end\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\ntool\n:\nTool\n,\nresult\n:\nstr\n,\n)\n->\nNone\nCalled after a tool is invoked.\nAgentHooks\nBases:\nGeneric\n[\nTContext\n]\nA class that receives callbacks on various lifecycle events for a specific agent. You can\nset this on\nagent.hooks\nto receive events for that specific agent.\nSubclass and override the methods you need.\non_start\nasync\non_start\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\n)\n->\nNone\nCalled before the agent is invoked. Called each time the running agent is changed to this\nagent.\non_end\nasync\non_end\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\noutput\n:\nAny\n,\n)\n->\nNone\nCalled when the agent produces a final output.\non_handoff\nasync\non_handoff\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\nsource\n:\nAgent\n[\nTContext\n],\n)\n->\nNone\nCalled when the agent is being handed off to. The\nsource\nis the agent that is handing\noff to this agent.\non_tool_start\nasync\non_tool_start\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\ntool\n:\nTool\n,\n)\n->\nNone\nCalled before a tool is invoked.\non_tool_end\nasync\non_tool_end\n(\ncontext\n:\nRunContextWrapper\n[\nTContext\n],\nagent\n:\nAgent\n[\nTContext\n],\ntool\n:\nTool\n,\nresult\n:\nstr\n,\n)\n->\nNone\nCalled after a tool is invoked.",
  "Items": "Items\nTResponse\nmodule-attribute\nTResponse\n=\nResponse\nA type alias for the Response type from the OpenAI SDK.\nTResponseInputItem\nmodule-attribute\nTResponseInputItem\n=\nResponseInputItemParam\nA type alias for the ResponseInputItemParam type from the OpenAI SDK.\nTResponseOutputItem\nmodule-attribute\nTResponseOutputItem\n=\nResponseOutputItem\nA type alias for the ResponseOutputItem type from the OpenAI SDK.\nTResponseStreamEvent\nmodule-attribute\nTResponseStreamEvent\n=\nResponseStreamEvent\nA type alias for the ResponseStreamEvent type from the OpenAI SDK.\nToolCallItemTypes\nmodule-attribute\nToolCallItemTypes\n:\nTypeAlias\n=\nUnion\n[\nResponseFunctionToolCall\n,\nResponseComputerToolCall\n,\nResponseFileSearchToolCall\n,\nResponseFunctionWebSearch\n,\n]\nA type that represents a tool call item.\nRunItem\nmodule-attribute\nRunItem\n:\nTypeAlias\n=\nUnion\n[\nMessageOutputItem\n,\nHandoffCallItem\n,\nHandoffOutputItem\n,\nToolCallItem\n,\nToolCallOutputItem\n,\nReasoningItem\n,\n]\nAn item generated by an agent.\nRunItemBase\ndataclass\nBases:\nGeneric\n[\nT\n]\n,\nABC\nSource code in\nsrc/agents/items.py\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n@dataclass\nclass\nRunItemBase\n(\nGeneric\n[\nT\n],\nabc\n.\nABC\n):\nagent\n:\nAgent\n[\nAny\n]\n\"\"\"The agent whose run caused this item to be generated.\"\"\"\nraw_item\n:\nT\n\"\"\"The raw Responses item from the run. This will always be a either an output item (i.e.\n`openai.types.responses.ResponseOutputItem` or an input item\n(i.e. `openai.types.responses.ResponseInputItemParam`).\n\"\"\"\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nraw_item\ninstance-attribute\nraw_item\n:\nT\nThe raw Responses item from the run. This will always be a either an output item (i.e.\nopenai.types.responses.ResponseOutputItem\nor an input item\n(i.e.\nopenai.types.responses.ResponseInputItemParam\n).\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nMessageOutputItem\ndataclass\nBases:\nRunItemBase\n[\nResponseOutputMessage\n]\nRepresents a message from the LLM.\nSource code in\nsrc/agents/items.py\n70\n71\n72\n73\n74\n75\n76\n77\n@dataclass\nclass\nMessageOutputItem\n(\nRunItemBase\n[\nResponseOutputMessage\n]):\n\"\"\"Represents a message from the LLM.\"\"\"\nraw_item\n:\nResponseOutputMessage\n\"\"\"The raw response output message.\"\"\"\ntype\n:\nLiteral\n[\n\"message_output_item\"\n]\n=\n\"message_output_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nResponseOutputMessage\nThe raw response output message.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nHandoffCallItem\ndataclass\nBases:\nRunItemBase\n[\nResponseFunctionToolCall\n]\nRepresents a tool call for a handoff from one agent to another.\nSource code in\nsrc/agents/items.py\n80\n81\n82\n83\n84\n85\n86\n87\n@dataclass\nclass\nHandoffCallItem\n(\nRunItemBase\n[\nResponseFunctionToolCall\n]):\n\"\"\"Represents a tool call for a handoff from one agent to another.\"\"\"\nraw_item\n:\nResponseFunctionToolCall\n\"\"\"The raw response function tool call that represents the handoff.\"\"\"\ntype\n:\nLiteral\n[\n\"handoff_call_item\"\n]\n=\n\"handoff_call_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nResponseFunctionToolCall\nThe raw response function tool call that represents the handoff.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nHandoffOutputItem\ndataclass\nBases:\nRunItemBase\n[\nTResponseInputItem\n]\nRepresents the output of a handoff.\nSource code in\nsrc/agents/items.py\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n@dataclass\nclass\nHandoffOutputItem\n(\nRunItemBase\n[\nTResponseInputItem\n]):\n\"\"\"Represents the output of a handoff.\"\"\"\nraw_item\n:\nTResponseInputItem\n\"\"\"The raw input item that represents the handoff taking place.\"\"\"\nsource_agent\n:\nAgent\n[\nAny\n]\n\"\"\"The agent that made the handoff.\"\"\"\ntarget_agent\n:\nAgent\n[\nAny\n]\n\"\"\"The agent that is being handed off to.\"\"\"\ntype\n:\nLiteral\n[\n\"handoff_output_item\"\n]\n=\n\"handoff_output_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nTResponseInputItem\nThe raw input item that represents the handoff taking place.\nsource_agent\ninstance-attribute\nsource_agent\n:\nAgent\n[\nAny\n]\nThe agent that made the handoff.\ntarget_agent\ninstance-attribute\ntarget_agent\n:\nAgent\n[\nAny\n]\nThe agent that is being handed off to.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nToolCallItem\ndataclass\nBases:\nRunItemBase\n[\nToolCallItemTypes\n]\nRepresents a tool call e.g. a function call or computer action call.\nSource code in\nsrc/agents/items.py\n115\n116\n117\n118\n119\n120\n121\n122\n@dataclass\nclass\nToolCallItem\n(\nRunItemBase\n[\nToolCallItemTypes\n]):\n\"\"\"Represents a tool call e.g. a function call or computer action call.\"\"\"\nraw_item\n:\nToolCallItemTypes\n\"\"\"The raw tool call item.\"\"\"\ntype\n:\nLiteral\n[\n\"tool_call_item\"\n]\n=\n\"tool_call_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nToolCallItemTypes\nThe raw tool call item.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nToolCallOutputItem\ndataclass\nBases:\nRunItemBase\n[\nUnion\n[\nFunctionCallOutput\n,\nComputerCallOutput\n]]\nRepresents the output of a tool call.\nSource code in\nsrc/agents/items.py\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n@dataclass\nclass\nToolCallOutputItem\n(\nRunItemBase\n[\nUnion\n[\nFunctionCallOutput\n,\nComputerCallOutput\n]]):\n\"\"\"Represents the output of a tool call.\"\"\"\nraw_item\n:\nFunctionCallOutput\n|\nComputerCallOutput\n\"\"\"The raw item from the model.\"\"\"\noutput\n:\nAny\n\"\"\"The output of the tool call. This is whatever the tool call returned; the `raw_item`\ncontains a string representation of the output.\n\"\"\"\ntype\n:\nLiteral\n[\n\"tool_call_output_item\"\n]\n=\n\"tool_call_output_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nFunctionCallOutput\n|\nComputerCallOutput\nThe raw item from the model.\noutput\ninstance-attribute\noutput\n:\nAny\nThe output of the tool call. This is whatever the tool call returned; the\nraw_item\ncontains a string representation of the output.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nReasoningItem\ndataclass\nBases:\nRunItemBase\n[\nResponseReasoningItem\n]\nRepresents a reasoning item.\nSource code in\nsrc/agents/items.py\n140\n141\n142\n143\n144\n145\n146\n147\n@dataclass\nclass\nReasoningItem\n(\nRunItemBase\n[\nResponseReasoningItem\n]):\n\"\"\"Represents a reasoning item.\"\"\"\nraw_item\n:\nResponseReasoningItem\n\"\"\"The raw reasoning item.\"\"\"\ntype\n:\nLiteral\n[\n\"reasoning_item\"\n]\n=\n\"reasoning_item\"\nraw_item\ninstance-attribute\nraw_item\n:\nResponseReasoningItem\nThe raw reasoning item.\nagent\ninstance-attribute\nagent\n:\nAgent\n[\nAny\n]\nThe agent whose run caused this item to be generated.\nto_input_item\nto_input_item\n()\n->\nTResponseInputItem\nConverts this item into an input item suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\ndef\nto_input_item\n(\nself\n)\n->\nTResponseInputItem\n:\n\"\"\"Converts this item into an input item suitable for passing to the model.\"\"\"\nif\nisinstance\n(\nself\n.\nraw_item\n,\ndict\n):\n# We know that input items are dicts, so we can ignore the type error\nreturn\nself\n.\nraw_item\n# type: ignore\nelif\nisinstance\n(\nself\n.\nraw_item\n,\nBaseModel\n):\n# All output items are Pydantic models that can be converted to input items.\nreturn\nself\n.\nraw_item\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\n# type: ignore\nelse\n:\nraise\nAgentsException\n(\nf\n\"Unexpected raw item type:\n{\ntype\n(\nself\n.\nraw_item\n)\n}\n\"\n)\nModelResponse\ndataclass\nSource code in\nsrc/agents/items.py\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n@dataclass\nclass\nModelResponse\n:\noutput\n:\nlist\n[\nTResponseOutputItem\n]\n\"\"\"A list of outputs (messages, tool calls, etc) generated by the model\"\"\"\nusage\n:\nUsage\n\"\"\"The usage information for the response.\"\"\"\nresponse_id\n:\nstr\n|\nNone\n\"\"\"An ID for the response which can be used to refer to the response in subsequent calls to the\nmodel. Not supported by all model providers.\nIf using OpenAI models via the Responses API, this is the `response_id` parameter, and it can\nbe passed to `Runner.run`.\n\"\"\"\ndef\nto_input_items\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n# We happen to know that the shape of the Pydantic output items are the same as the\n# equivalent TypedDict input items, so we can just convert each one.\n# This is also tested via unit tests.\nreturn\n[\nit\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\nfor\nit\nin\nself\n.\noutput\n]\n# type: ignore\noutput\ninstance-attribute\noutput\n:\nlist\n[\nTResponseOutputItem\n]\nA list of outputs (messages, tool calls, etc) generated by the model\nusage\ninstance-attribute\nusage\n:\nUsage\nThe usage information for the response.\nresponse_id\ninstance-attribute\nresponse_id\n:\nstr\n|\nNone\nAn ID for the response which can be used to refer to the response in subsequent calls to the\nmodel. Not supported by all model providers.\nIf using OpenAI models via the Responses API, this is the\nresponse_id\nparameter, and it can\nbe passed to\nRunner.run\n.\nto_input_items\nto_input_items\n()\n->\nlist\n[\nTResponseInputItem\n]\nConvert the output into a list of input items suitable for passing to the model.\nSource code in\nsrc/agents/items.py\n176\n177\n178\n179\n180\n181\ndef\nto_input_items\n(\nself\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Convert the output into a list of input items suitable for passing to the model.\"\"\"\n# We happen to know that the shape of the Pydantic output items are the same as the\n# equivalent TypedDict input items, so we can just convert each one.\n# This is also tested via unit tests.\nreturn\n[\nit\n.\nmodel_dump\n(\nexclude_unset\n=\nTrue\n)\nfor\nit\nin\nself\n.\noutput\n]\n# type: ignore\nItemHelpers\nSource code in\nsrc/agents/items.py\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\nclass\nItemHelpers\n:\n@classmethod\ndef\nextract_last_content\n(\ncls\n,\nmessage\n:\nTResponseOutputItem\n)\n->\nstr\n:\n\"\"\"Extracts the last text content or refusal from a message.\"\"\"\nif\nnot\nisinstance\n(\nmessage\n,\nResponseOutputMessage\n):\nreturn\n\"\"\nlast_content\n=\nmessage\n.\ncontent\n[\n-\n1\n]\nif\nisinstance\n(\nlast_content\n,\nResponseOutputText\n):\nreturn\nlast_content\n.\ntext\nelif\nisinstance\n(\nlast_content\n,\nResponseOutputRefusal\n):\nreturn\nlast_content\n.\nrefusal\nelse\n:\nraise\nModelBehaviorError\n(\nf\n\"Unexpected content type:\n{\ntype\n(\nlast_content\n)\n}\n\"\n)\n@classmethod\ndef\nextract_last_text\n(\ncls\n,\nmessage\n:\nTResponseOutputItem\n)\n->\nstr\n|\nNone\n:\n\"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\nif\nisinstance\n(\nmessage\n,\nResponseOutputMessage\n):\nlast_content\n=\nmessage\n.\ncontent\n[\n-\n1\n]\nif\nisinstance\n(\nlast_content\n,\nResponseOutputText\n):\nreturn\nlast_content\n.\ntext\nreturn\nNone\n@classmethod\ndef\ninput_to_new_input_list\n(\ncls\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Converts a string or list of input items into a list of input items.\"\"\"\nif\nisinstance\n(\ninput\n,\nstr\n):\nreturn\n[\n{\n\"content\"\n:\ninput\n,\n\"role\"\n:\n\"user\"\n,\n}\n]\nreturn\ncopy\n.\ndeepcopy\n(\ninput\n)\n@classmethod\ndef\ntext_message_outputs\n(\ncls\n,\nitems\n:\nlist\n[\nRunItem\n])\n->\nstr\n:\n\"\"\"Concatenates all the text content from a list of message output items.\"\"\"\ntext\n=\n\"\"\nfor\nitem\nin\nitems\n:\nif\nisinstance\n(\nitem\n,\nMessageOutputItem\n):\ntext\n+=\ncls\n.\ntext_message_output\n(\nitem\n)\nreturn\ntext\n@classmethod\ndef\ntext_message_output\n(\ncls\n,\nmessage\n:\nMessageOutputItem\n)\n->\nstr\n:\n\"\"\"Extracts all the text content from a single message output item.\"\"\"\ntext\n=\n\"\"\nfor\nitem\nin\nmessage\n.\nraw_item\n.\ncontent\n:\nif\nisinstance\n(\nitem\n,\nResponseOutputText\n):\ntext\n+=\nitem\n.\ntext\nreturn\ntext\n@classmethod\ndef\ntool_call_output_item\n(\ncls\n,\ntool_call\n:\nResponseFunctionToolCall\n,\noutput\n:\nstr\n)\n->\nFunctionCallOutput\n:\n\"\"\"Creates a tool call output item from a tool call and its output.\"\"\"\nreturn\n{\n\"call_id\"\n:\ntool_call\n.\ncall_id\n,\n\"output\"\n:\noutput\n,\n\"type\"\n:\n\"function_call_output\"\n,\n}\nextract_last_content\nclassmethod\nextract_last_content\n(\nmessage\n:\nTResponseOutputItem\n)\n->\nstr\nExtracts the last text content or refusal from a message.\nSource code in\nsrc/agents/items.py\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n@classmethod\ndef\nextract_last_content\n(\ncls\n,\nmessage\n:\nTResponseOutputItem\n)\n->\nstr\n:\n\"\"\"Extracts the last text content or refusal from a message.\"\"\"\nif\nnot\nisinstance\n(\nmessage\n,\nResponseOutputMessage\n):\nreturn\n\"\"\nlast_content\n=\nmessage\n.\ncontent\n[\n-\n1\n]\nif\nisinstance\n(\nlast_content\n,\nResponseOutputText\n):\nreturn\nlast_content\n.\ntext\nelif\nisinstance\n(\nlast_content\n,\nResponseOutputRefusal\n):\nreturn\nlast_content\n.\nrefusal\nelse\n:\nraise\nModelBehaviorError\n(\nf\n\"Unexpected content type:\n{\ntype\n(\nlast_content\n)\n}\n\"\n)\nextract_last_text\nclassmethod\nextract_last_text\n(\nmessage\n:\nTResponseOutputItem\n,\n)\n->\nstr\n|\nNone\nExtracts the last text content from a message, if any. Ignores refusals.\nSource code in\nsrc/agents/items.py\n199\n200\n201\n202\n203\n204\n205\n206\n207\n@classmethod\ndef\nextract_last_text\n(\ncls\n,\nmessage\n:\nTResponseOutputItem\n)\n->\nstr\n|\nNone\n:\n\"\"\"Extracts the last text content from a message, if any. Ignores refusals.\"\"\"\nif\nisinstance\n(\nmessage\n,\nResponseOutputMessage\n):\nlast_content\n=\nmessage\n.\ncontent\n[\n-\n1\n]\nif\nisinstance\n(\nlast_content\n,\nResponseOutputText\n):\nreturn\nlast_content\n.\ntext\nreturn\nNone\ninput_to_new_input_list\nclassmethod\ninput_to_new_input_list\n(\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\n)\n->\nlist\n[\nTResponseInputItem\n]\nConverts a string or list of input items into a list of input items.\nSource code in\nsrc/agents/items.py\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n@classmethod\ndef\ninput_to_new_input_list\n(\ncls\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n]\n)\n->\nlist\n[\nTResponseInputItem\n]:\n\"\"\"Converts a string or list of input items into a list of input items.\"\"\"\nif\nisinstance\n(\ninput\n,\nstr\n):\nreturn\n[\n{\n\"content\"\n:\ninput\n,\n\"role\"\n:\n\"user\"\n,\n}\n]\nreturn\ncopy\n.\ndeepcopy\n(\ninput\n)\ntext_message_outputs\nclassmethod\ntext_message_outputs\n(\nitems\n:\nlist\n[\nRunItem\n])\n->\nstr\nConcatenates all the text content from a list of message output items.\nSource code in\nsrc/agents/items.py\n223\n224\n225\n226\n227\n228\n229\n230\n@classmethod\ndef\ntext_message_outputs\n(\ncls\n,\nitems\n:\nlist\n[\nRunItem\n])\n->\nstr\n:\n\"\"\"Concatenates all the text content from a list of message output items.\"\"\"\ntext\n=\n\"\"\nfor\nitem\nin\nitems\n:\nif\nisinstance\n(\nitem\n,\nMessageOutputItem\n):\ntext\n+=\ncls\n.\ntext_message_output\n(\nitem\n)\nreturn\ntext\ntext_message_output\nclassmethod\ntext_message_output\n(\nmessage\n:\nMessageOutputItem\n)\n->\nstr\nExtracts all the text content from a single message output item.\nSource code in\nsrc/agents/items.py\n232\n233\n234\n235\n236\n237\n238\n239\n@classmethod\ndef\ntext_message_output\n(\ncls\n,\nmessage\n:\nMessageOutputItem\n)\n->\nstr\n:\n\"\"\"Extracts all the text content from a single message output item.\"\"\"\ntext\n=\n\"\"\nfor\nitem\nin\nmessage\n.\nraw_item\n.\ncontent\n:\nif\nisinstance\n(\nitem\n,\nResponseOutputText\n):\ntext\n+=\nitem\n.\ntext\nreturn\ntext\ntool_call_output_item\nclassmethod\ntool_call_output_item\n(\ntool_call\n:\nResponseFunctionToolCall\n,\noutput\n:\nstr\n)\n->\nFunctionCallOutput\nCreates a tool call output item from a tool call and its output.\nSource code in\nsrc/agents/items.py\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n@classmethod\ndef\ntool_call_output_item\n(\ncls\n,\ntool_call\n:\nResponseFunctionToolCall\n,\noutput\n:\nstr\n)\n->\nFunctionCallOutput\n:\n\"\"\"Creates a tool call output item from a tool call and its output.\"\"\"\nreturn\n{\n\"call_id\"\n:\ntool_call\n.\ncall_id\n,\n\"output\"\n:\noutput\n,\n\"type\"\n:\n\"function_call_output\"\n,\n}",
  "Run context": "Run context\nRunContextWrapper\ndataclass\nBases:\nGeneric\n[\nTContext\n]\nThis wraps the context object that you passed to\nRunner.run()\n. It also contains\ninformation about the usage of the agent run so far.\nNOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code\nyou implement, like tool functions, callbacks, hooks, etc.\nSource code in\nsrc/agents/run_context.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n@dataclass\nclass\nRunContextWrapper\n(\nGeneric\n[\nTContext\n]):\n\"\"\"This wraps the context object that you passed to `Runner.run()`. It also contains\ninformation about the usage of the agent run so far.\nNOTE: Contexts are not passed to the LLM. They're a way to pass dependencies and data to code\nyou implement, like tool functions, callbacks, hooks, etc.\n\"\"\"\ncontext\n:\nTContext\n\"\"\"The context object (or None), passed by you to `Runner.run()`\"\"\"\nusage\n:\nUsage\n=\nfield\n(\ndefault_factory\n=\nUsage\n)\n\"\"\"The usage of the agent run so far. For streamed responses, the usage will be stale until the\nlast chunk of the stream is processed.\n\"\"\"\ncontext\ninstance-attribute\ncontext\n:\nTContext\nThe context object (or None), passed by you to\nRunner.run()\nusage\nclass-attribute\ninstance-attribute\nusage\n:\nUsage\n=\nfield\n(\ndefault_factory\n=\nUsage\n)\nThe usage of the agent run so far. For streamed responses, the usage will be stale until the\nlast chunk of the stream is processed.",
  "Usage": "Usage\nUsage\ndataclass\nSource code in\nsrc/agents/usage.py\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n@dataclass\nclass\nUsage\n:\nrequests\n:\nint\n=\n0\n\"\"\"Total requests made to the LLM API.\"\"\"\ninput_tokens\n:\nint\n=\n0\n\"\"\"Total input tokens sent, across all requests.\"\"\"\noutput_tokens\n:\nint\n=\n0\n\"\"\"Total output tokens received, across all requests.\"\"\"\ntotal_tokens\n:\nint\n=\n0\n\"\"\"Total tokens sent and received, across all requests.\"\"\"\ndef\nadd\n(\nself\n,\nother\n:\n\"Usage\"\n)\n->\nNone\n:\nself\n.\nrequests\n+=\nother\n.\nrequests\nif\nother\n.\nrequests\nelse\n0\nself\n.\ninput_tokens\n+=\nother\n.\ninput_tokens\nif\nother\n.\ninput_tokens\nelse\n0\nself\n.\noutput_tokens\n+=\nother\n.\noutput_tokens\nif\nother\n.\noutput_tokens\nelse\n0\nself\n.\ntotal_tokens\n+=\nother\n.\ntotal_tokens\nif\nother\n.\ntotal_tokens\nelse\n0\nrequests\nclass-attribute\ninstance-attribute\nrequests\n:\nint\n=\n0\nTotal requests made to the LLM API.\ninput_tokens\nclass-attribute\ninstance-attribute\ninput_tokens\n:\nint\n=\n0\nTotal input tokens sent, across all requests.\noutput_tokens\nclass-attribute\ninstance-attribute\noutput_tokens\n:\nint\n=\n0\nTotal output tokens received, across all requests.\ntotal_tokens\nclass-attribute\ninstance-attribute\ntotal_tokens\n:\nint\n=\n0\nTotal tokens sent and received, across all requests.",
  "Exceptions": "Exceptions\nSTTWebsocketConnectionError\nBases:\nAgentsException\nException raised when the STT websocket connection fails.\nSource code in\nsrc/agents/voice/exceptions.py\n4\n5\n6\n7\n8\nclass\nSTTWebsocketConnectionError\n(\nAgentsException\n):\n\"\"\"Exception raised when the STT websocket connection fails.\"\"\"\ndef\n__init__\n(\nself\n,\nmessage\n:\nstr\n):\nself\n.\nmessage\n=\nmessage",
  "Model settings": "Model settings\nModelSettings\ndataclass\nSettings to use when calling an LLM.\nThis class holds optional model configuration parameters (e.g. temperature,\ntop_p, penalties, truncation, etc.).\nNot all models/providers support all of these parameters, so please check the API documentation\nfor the specific model and provider you are using.\nSource code in\nsrc/agents/model_settings.py\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n@dataclass\nclass\nModelSettings\n:\n\"\"\"Settings to use when calling an LLM.\nThis class holds optional model configuration parameters (e.g. temperature,\ntop_p, penalties, truncation, etc.).\nNot all models/providers support all of these parameters, so please check the API documentation\nfor the specific model and provider you are using.\n\"\"\"\ntemperature\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The temperature to use when calling the model.\"\"\"\ntop_p\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The top_p to use when calling the model.\"\"\"\nfrequency_penalty\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The frequency penalty to use when calling the model.\"\"\"\npresence_penalty\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The presence penalty to use when calling the model.\"\"\"\ntool_choice\n:\nLiteral\n[\n\"auto\"\n,\n\"required\"\n,\n\"none\"\n]\n|\nstr\n|\nNone\n=\nNone\n\"\"\"The tool choice to use when calling the model.\"\"\"\nparallel_tool_calls\n:\nbool\n|\nNone\n=\nNone\n\"\"\"Whether to use parallel tool calls when calling the model.\nDefaults to False if not provided.\"\"\"\ntruncation\n:\nLiteral\n[\n\"auto\"\n,\n\"disabled\"\n]\n|\nNone\n=\nNone\n\"\"\"The truncation strategy to use when calling the model.\"\"\"\nmax_tokens\n:\nint\n|\nNone\n=\nNone\n\"\"\"The maximum number of output tokens to generate.\"\"\"\nreasoning\n:\nReasoning\n|\nNone\n=\nNone\n\"\"\"Configuration options for\n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\"\"\"\nmetadata\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n=\nNone\n\"\"\"Metadata to include with the model response call.\"\"\"\nstore\n:\nbool\n|\nNone\n=\nNone\n\"\"\"Whether to store the generated model response for later retrieval.\nDefaults to True if not provided.\"\"\"\ninclude_usage\n:\nbool\n|\nNone\n=\nNone\n\"\"\"Whether to include usage chunk.\nDefaults to True if not provided.\"\"\"\nextra_query\n:\nQuery\n|\nNone\n=\nNone\n\"\"\"Additional query fields to provide with the request.\nDefaults to None if not provided.\"\"\"\nextra_body\n:\nBody\n|\nNone\n=\nNone\n\"\"\"Additional body fields to provide with the request.\nDefaults to None if not provided.\"\"\"\ndef\nresolve\n(\nself\n,\noverride\n:\nModelSettings\n|\nNone\n)\n->\nModelSettings\n:\n\"\"\"Produce a new ModelSettings by overlaying any non-None values from the\noverride on top of this instance.\"\"\"\nif\noverride\nis\nNone\n:\nreturn\nself\nchanges\n=\n{\nfield\n.\nname\n:\ngetattr\n(\noverride\n,\nfield\n.\nname\n)\nfor\nfield\nin\nfields\n(\nself\n)\nif\ngetattr\n(\noverride\n,\nfield\n.\nname\n)\nis\nnot\nNone\n}\nreturn\nreplace\n(\nself\n,\n**\nchanges\n)\ntemperature\nclass-attribute\ninstance-attribute\ntemperature\n:\nfloat\n|\nNone\n=\nNone\nThe temperature to use when calling the model.\ntop_p\nclass-attribute\ninstance-attribute\ntop_p\n:\nfloat\n|\nNone\n=\nNone\nThe top_p to use when calling the model.\nfrequency_penalty\nclass-attribute\ninstance-attribute\nfrequency_penalty\n:\nfloat\n|\nNone\n=\nNone\nThe frequency penalty to use when calling the model.\npresence_penalty\nclass-attribute\ninstance-attribute\npresence_penalty\n:\nfloat\n|\nNone\n=\nNone\nThe presence penalty to use when calling the model.\ntool_choice\nclass-attribute\ninstance-attribute\ntool_choice\n:\n(\nLiteral\n[\n\"auto\"\n,\n\"required\"\n,\n\"none\"\n]\n|\nstr\n|\nNone\n)\n=\nNone\nThe tool choice to use when calling the model.\nparallel_tool_calls\nclass-attribute\ninstance-attribute\nparallel_tool_calls\n:\nbool\n|\nNone\n=\nNone\nWhether to use parallel tool calls when calling the model.\nDefaults to False if not provided.\ntruncation\nclass-attribute\ninstance-attribute\ntruncation\n:\nLiteral\n[\n'auto'\n,\n'disabled'\n]\n|\nNone\n=\nNone\nThe truncation strategy to use when calling the model.\nmax_tokens\nclass-attribute\ninstance-attribute\nmax_tokens\n:\nint\n|\nNone\n=\nNone\nThe maximum number of output tokens to generate.\nreasoning\nclass-attribute\ninstance-attribute\nreasoning\n:\nReasoning\n|\nNone\n=\nNone\nConfiguration options for\nreasoning models\n.\nmetadata\nclass-attribute\ninstance-attribute\nmetadata\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n=\nNone\nMetadata to include with the model response call.\nstore\nclass-attribute\ninstance-attribute\nstore\n:\nbool\n|\nNone\n=\nNone\nWhether to store the generated model response for later retrieval.\nDefaults to True if not provided.\ninclude_usage\nclass-attribute\ninstance-attribute\ninclude_usage\n:\nbool\n|\nNone\n=\nNone\nWhether to include usage chunk.\nDefaults to True if not provided.\nextra_query\nclass-attribute\ninstance-attribute\nextra_query\n:\nQuery\n|\nNone\n=\nNone\nAdditional query fields to provide with the request.\nDefaults to None if not provided.\nextra_body\nclass-attribute\ninstance-attribute\nextra_body\n:\nBody\n|\nNone\n=\nNone\nAdditional body fields to provide with the request.\nDefaults to None if not provided.\nresolve\nresolve\n(\noverride\n:\nModelSettings\n|\nNone\n)\n->\nModelSettings\nProduce a new ModelSettings by overlaying any non-None values from the\noverride on top of this instance.\nSource code in\nsrc/agents/model_settings.py\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\ndef\nresolve\n(\nself\n,\noverride\n:\nModelSettings\n|\nNone\n)\n->\nModelSettings\n:\n\"\"\"Produce a new ModelSettings by overlaying any non-None values from the\noverride on top of this instance.\"\"\"\nif\noverride\nis\nNone\n:\nreturn\nself\nchanges\n=\n{\nfield\n.\nname\n:\ngetattr\n(\noverride\n,\nfield\n.\nname\n)\nfor\nfield\nin\nfields\n(\nself\n)\nif\ngetattr\n(\noverride\n,\nfield\n.\nname\n)\nis\nnot\nNone\n}\nreturn\nreplace\n(\nself\n,\n**\nchanges\n)",
  "Agent output": "Agent output\nAgentOutputSchema\ndataclass\nAn object that captures the JSON schema of the output, as well as validating/parsing JSON\nproduced by the LLM into the output type.\nSource code in\nsrc/agents/agent_output.py\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n@dataclass\n(\ninit\n=\nFalse\n)\nclass\nAgentOutputSchema\n:\n\"\"\"An object that captures the JSON schema of the output, as well as validating/parsing JSON\nproduced by the LLM into the output type.\n\"\"\"\noutput_type\n:\ntype\n[\nAny\n]\n\"\"\"The type of the output.\"\"\"\n_type_adapter\n:\nTypeAdapter\n[\nAny\n]\n\"\"\"A type adapter that wraps the output type, so that we can validate JSON.\"\"\"\n_is_wrapped\n:\nbool\n\"\"\"Whether the output type is wrapped in a dictionary. This is generally done if the base\noutput type cannot be represented as a JSON Schema object.\n\"\"\"\n_output_schema\n:\ndict\n[\nstr\n,\nAny\n]\n\"\"\"The JSON schema of the output.\"\"\"\nstrict_json_schema\n:\nbool\n\"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\nas it increases the likelihood of correct JSON input.\n\"\"\"\ndef\n__init__\n(\nself\n,\noutput_type\n:\ntype\n[\nAny\n],\nstrict_json_schema\n:\nbool\n=\nTrue\n):\n\"\"\"\nArgs:\noutput_type: The type of the output.\nstrict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\nsetting this to True, as it increases the likelihood of correct JSON input.\n\"\"\"\nself\n.\noutput_type\n=\noutput_type\nself\n.\nstrict_json_schema\n=\nstrict_json_schema\nif\noutput_type\nis\nNone\nor\noutput_type\nis\nstr\n:\nself\n.\n_is_wrapped\n=\nFalse\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\noutput_type\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nreturn\n# We should wrap for things that are not plain text, and for things that would definitely\n# not be a JSON Schema object.\nself\n.\n_is_wrapped\n=\nnot\n_is_subclass_of_base_model_or_dict\n(\noutput_type\n)\nif\nself\n.\n_is_wrapped\n:\nOutputType\n=\nTypedDict\n(\n\"OutputType\"\n,\n{\n_WRAPPER_DICT_KEY\n:\noutput_type\n,\n# type: ignore\n},\n)\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\nOutputType\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nelse\n:\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\noutput_type\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nif\nself\n.\nstrict_json_schema\n:\nself\n.\n_output_schema\n=\nensure_strict_json_schema\n(\nself\n.\n_output_schema\n)\ndef\nis_plain_text\n(\nself\n)\n->\nbool\n:\n\"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\nreturn\nself\n.\noutput_type\nis\nNone\nor\nself\n.\noutput_type\nis\nstr\ndef\njson_schema\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"The JSON schema of the output type.\"\"\"\nif\nself\n.\nis_plain_text\n():\nraise\nUserError\n(\n\"Output type is plain text, so no JSON schema is available\"\n)\nreturn\nself\n.\n_output_schema\ndef\nvalidate_json\n(\nself\n,\njson_str\n:\nstr\n,\npartial\n:\nbool\n=\nFalse\n)\n->\nAny\n:\n\"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\na `ModelBehaviorError` if the JSON is invalid.\n\"\"\"\nvalidated\n=\n_json\n.\nvalidate_json\n(\njson_str\n,\nself\n.\n_type_adapter\n,\npartial\n)\nif\nself\n.\n_is_wrapped\n:\nif\nnot\nisinstance\n(\nvalidated\n,\ndict\n):\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Invalid JSON\"\n,\ndata\n=\n{\n\"details\"\n:\nf\n\"Expected a dict, got\n{\ntype\n(\nvalidated\n)\n}\n\"\n},\n)\n)\nraise\nModelBehaviorError\n(\nf\n\"Expected a dict, got\n{\ntype\n(\nvalidated\n)\n}\nfor JSON:\n{\njson_str\n}\n\"\n)\nif\n_WRAPPER_DICT_KEY\nnot\nin\nvalidated\n:\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Invalid JSON\"\n,\ndata\n=\n{\n\"details\"\n:\nf\n\"Could not find key\n{\n_WRAPPER_DICT_KEY\n}\nin JSON\"\n},\n)\n)\nraise\nModelBehaviorError\n(\nf\n\"Could not find key\n{\n_WRAPPER_DICT_KEY\n}\nin JSON:\n{\njson_str\n}\n\"\n)\nreturn\nvalidated\n[\n_WRAPPER_DICT_KEY\n]\nreturn\nvalidated\ndef\noutput_type_name\n(\nself\n)\n->\nstr\n:\n\"\"\"The name of the output type.\"\"\"\nreturn\n_type_to_str\n(\nself\n.\noutput_type\n)\noutput_type\ninstance-attribute\noutput_type\n:\ntype\n[\nAny\n]\n=\noutput_type\nThe type of the output.\nstrict_json_schema\ninstance-attribute\nstrict_json_schema\n:\nbool\n=\nstrict_json_schema\nWhether the JSON schema is in strict mode. We\nstrongly\nrecommend setting this to True,\nas it increases the likelihood of correct JSON input.\n__init__\n__init__\n(\noutput_type\n:\ntype\n[\nAny\n],\nstrict_json_schema\n:\nbool\n=\nTrue\n)\nParameters:\nName\nType\nDescription\nDefault\noutput_type\ntype\n[\nAny\n]\nThe type of the output.\nrequired\nstrict_json_schema\nbool\nWhether the JSON schema is in strict mode. We\nstrongly\nrecommend\nsetting this to True, as it increases the likelihood of correct JSON input.\nTrue\nSource code in\nsrc/agents/agent_output.py\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\ndef\n__init__\n(\nself\n,\noutput_type\n:\ntype\n[\nAny\n],\nstrict_json_schema\n:\nbool\n=\nTrue\n):\n\"\"\"\nArgs:\noutput_type: The type of the output.\nstrict_json_schema: Whether the JSON schema is in strict mode. We **strongly** recommend\nsetting this to True, as it increases the likelihood of correct JSON input.\n\"\"\"\nself\n.\noutput_type\n=\noutput_type\nself\n.\nstrict_json_schema\n=\nstrict_json_schema\nif\noutput_type\nis\nNone\nor\noutput_type\nis\nstr\n:\nself\n.\n_is_wrapped\n=\nFalse\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\noutput_type\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nreturn\n# We should wrap for things that are not plain text, and for things that would definitely\n# not be a JSON Schema object.\nself\n.\n_is_wrapped\n=\nnot\n_is_subclass_of_base_model_or_dict\n(\noutput_type\n)\nif\nself\n.\n_is_wrapped\n:\nOutputType\n=\nTypedDict\n(\n\"OutputType\"\n,\n{\n_WRAPPER_DICT_KEY\n:\noutput_type\n,\n# type: ignore\n},\n)\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\nOutputType\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nelse\n:\nself\n.\n_type_adapter\n=\nTypeAdapter\n(\noutput_type\n)\nself\n.\n_output_schema\n=\nself\n.\n_type_adapter\n.\njson_schema\n()\nif\nself\n.\nstrict_json_schema\n:\nself\n.\n_output_schema\n=\nensure_strict_json_schema\n(\nself\n.\n_output_schema\n)\nis_plain_text\nis_plain_text\n()\n->\nbool\nWhether the output type is plain text (versus a JSON object).\nSource code in\nsrc/agents/agent_output.py\n76\n77\n78\ndef\nis_plain_text\n(\nself\n)\n->\nbool\n:\n\"\"\"Whether the output type is plain text (versus a JSON object).\"\"\"\nreturn\nself\n.\noutput_type\nis\nNone\nor\nself\n.\noutput_type\nis\nstr\njson_schema\njson_schema\n()\n->\ndict\n[\nstr\n,\nAny\n]\nThe JSON schema of the output type.\nSource code in\nsrc/agents/agent_output.py\n80\n81\n82\n83\n84\ndef\njson_schema\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"The JSON schema of the output type.\"\"\"\nif\nself\n.\nis_plain_text\n():\nraise\nUserError\n(\n\"Output type is plain text, so no JSON schema is available\"\n)\nreturn\nself\n.\n_output_schema\nvalidate_json\nvalidate_json\n(\njson_str\n:\nstr\n,\npartial\n:\nbool\n=\nFalse\n)\n->\nAny\nValidate a JSON string against the output type. Returns the validated object, or raises\na\nModelBehaviorError\nif the JSON is invalid.\nSource code in\nsrc/agents/agent_output.py\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\ndef\nvalidate_json\n(\nself\n,\njson_str\n:\nstr\n,\npartial\n:\nbool\n=\nFalse\n)\n->\nAny\n:\n\"\"\"Validate a JSON string against the output type. Returns the validated object, or raises\na `ModelBehaviorError` if the JSON is invalid.\n\"\"\"\nvalidated\n=\n_json\n.\nvalidate_json\n(\njson_str\n,\nself\n.\n_type_adapter\n,\npartial\n)\nif\nself\n.\n_is_wrapped\n:\nif\nnot\nisinstance\n(\nvalidated\n,\ndict\n):\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Invalid JSON\"\n,\ndata\n=\n{\n\"details\"\n:\nf\n\"Expected a dict, got\n{\ntype\n(\nvalidated\n)\n}\n\"\n},\n)\n)\nraise\nModelBehaviorError\n(\nf\n\"Expected a dict, got\n{\ntype\n(\nvalidated\n)\n}\nfor JSON:\n{\njson_str\n}\n\"\n)\nif\n_WRAPPER_DICT_KEY\nnot\nin\nvalidated\n:\n_error_tracing\n.\nattach_error_to_current_span\n(\nSpanError\n(\nmessage\n=\n\"Invalid JSON\"\n,\ndata\n=\n{\n\"details\"\n:\nf\n\"Could not find key\n{\n_WRAPPER_DICT_KEY\n}\nin JSON\"\n},\n)\n)\nraise\nModelBehaviorError\n(\nf\n\"Could not find key\n{\n_WRAPPER_DICT_KEY\n}\nin JSON:\n{\njson_str\n}\n\"\n)\nreturn\nvalidated\n[\n_WRAPPER_DICT_KEY\n]\nreturn\nvalidated\noutput_type_name\noutput_type_name\n()\n->\nstr\nThe name of the output type.\nSource code in\nsrc/agents/agent_output.py\n116\n117\n118\ndef\noutput_type_name\n(\nself\n)\n->\nstr\n:\n\"\"\"The name of the output type.\"\"\"\nreturn\n_type_to_str\n(\nself\n.\noutput_type\n)",
  "Function schema": "Function schema\nFuncSchema\ndataclass\nCaptures the schema for a python function, in preparation for sending it to an LLM as a tool.\nSource code in\nsrc/agents/function_schema.py\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n@dataclass\nclass\nFuncSchema\n:\n\"\"\"\nCaptures the schema for a python function, in preparation for sending it to an LLM as a tool.\n\"\"\"\nname\n:\nstr\n\"\"\"The name of the function.\"\"\"\ndescription\n:\nstr\n|\nNone\n\"\"\"The description of the function.\"\"\"\nparams_pydantic_model\n:\ntype\n[\nBaseModel\n]\n\"\"\"A Pydantic model that represents the function's parameters.\"\"\"\nparams_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\n\"\"\"The JSON schema for the function's parameters, derived from the Pydantic model.\"\"\"\nsignature\n:\ninspect\n.\nSignature\n\"\"\"The signature of the function.\"\"\"\ntakes_context\n:\nbool\n=\nFalse\n\"\"\"Whether the function takes a RunContextWrapper argument (must be the first argument).\"\"\"\nstrict_json_schema\n:\nbool\n=\nTrue\n\"\"\"Whether the JSON schema is in strict mode. We **strongly** recommend setting this to True,\nas it increases the likelihood of correct JSON input.\"\"\"\ndef\nto_call_args\n(\nself\n,\ndata\n:\nBaseModel\n)\n->\ntuple\n[\nlist\n[\nAny\n],\ndict\n[\nstr\n,\nAny\n]]:\n\"\"\"\nConverts validated data from the Pydantic model into (args, kwargs), suitable for calling\nthe original function.\n\"\"\"\npositional_args\n:\nlist\n[\nAny\n]\n=\n[]\nkeyword_args\n:\ndict\n[\nstr\n,\nAny\n]\n=\n{}\nseen_var_positional\n=\nFalse\n# Use enumerate() so we can skip the first parameter if it's context.\nfor\nidx\n,\n(\nname\n,\nparam\n)\nin\nenumerate\n(\nself\n.\nsignature\n.\nparameters\n.\nitems\n()):\n# If the function takes a RunContextWrapper and this is the first parameter, skip it.\nif\nself\n.\ntakes_context\nand\nidx\n==\n0\n:\ncontinue\nvalue\n=\ngetattr\n(\ndata\n,\nname\n,\nNone\n)\nif\nparam\n.\nkind\n==\nparam\n.\nVAR_POSITIONAL\n:\n# e.g. *args: extend positional args and mark that *args is now seen\npositional_args\n.\nextend\n(\nvalue\nor\n[])\nseen_var_positional\n=\nTrue\nelif\nparam\n.\nkind\n==\nparam\n.\nVAR_KEYWORD\n:\n# e.g. **kwargs handling\nkeyword_args\n.\nupdate\n(\nvalue\nor\n{})\nelif\nparam\n.\nkind\nin\n(\nparam\n.\nPOSITIONAL_ONLY\n,\nparam\n.\nPOSITIONAL_OR_KEYWORD\n):\n# Before *args, add to positional args. After *args, add to keyword args.\nif\nnot\nseen_var_positional\n:\npositional_args\n.\nappend\n(\nvalue\n)\nelse\n:\nkeyword_args\n[\nname\n]\n=\nvalue\nelse\n:\n# For KEYWORD_ONLY parameters, always use keyword args.\nkeyword_args\n[\nname\n]\n=\nvalue\nreturn\npositional_args\n,\nkeyword_args\nname\ninstance-attribute\nname\n:\nstr\nThe name of the function.\ndescription\ninstance-attribute\ndescription\n:\nstr\n|\nNone\nThe description of the function.\nparams_pydantic_model\ninstance-attribute\nparams_pydantic_model\n:\ntype\n[\nBaseModel\n]\nA Pydantic model that represents the function's parameters.\nparams_json_schema\ninstance-attribute\nparams_json_schema\n:\ndict\n[\nstr\n,\nAny\n]\nThe JSON schema for the function's parameters, derived from the Pydantic model.\nsignature\ninstance-attribute\nsignature\n:\nSignature\nThe signature of the function.\ntakes_context\nclass-attribute\ninstance-attribute\ntakes_context\n:\nbool\n=\nFalse\nWhether the function takes a RunContextWrapper argument (must be the first argument).\nstrict_json_schema\nclass-attribute\ninstance-attribute\nstrict_json_schema\n:\nbool\n=\nTrue\nWhether the JSON schema is in strict mode. We\nstrongly\nrecommend setting this to True,\nas it increases the likelihood of correct JSON input.\nto_call_args\nto_call_args\n(\ndata\n:\nBaseModel\n,\n)\n->\ntuple\n[\nlist\n[\nAny\n],\ndict\n[\nstr\n,\nAny\n]]\nConverts validated data from the Pydantic model into (args, kwargs), suitable for calling\nthe original function.\nSource code in\nsrc/agents/function_schema.py\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\ndef\nto_call_args\n(\nself\n,\ndata\n:\nBaseModel\n)\n->\ntuple\n[\nlist\n[\nAny\n],\ndict\n[\nstr\n,\nAny\n]]:\n\"\"\"\nConverts validated data from the Pydantic model into (args, kwargs), suitable for calling\nthe original function.\n\"\"\"\npositional_args\n:\nlist\n[\nAny\n]\n=\n[]\nkeyword_args\n:\ndict\n[\nstr\n,\nAny\n]\n=\n{}\nseen_var_positional\n=\nFalse\n# Use enumerate() so we can skip the first parameter if it's context.\nfor\nidx\n,\n(\nname\n,\nparam\n)\nin\nenumerate\n(\nself\n.\nsignature\n.\nparameters\n.\nitems\n()):\n# If the function takes a RunContextWrapper and this is the first parameter, skip it.\nif\nself\n.\ntakes_context\nand\nidx\n==\n0\n:\ncontinue\nvalue\n=\ngetattr\n(\ndata\n,\nname\n,\nNone\n)\nif\nparam\n.\nkind\n==\nparam\n.\nVAR_POSITIONAL\n:\n# e.g. *args: extend positional args and mark that *args is now seen\npositional_args\n.\nextend\n(\nvalue\nor\n[])\nseen_var_positional\n=\nTrue\nelif\nparam\n.\nkind\n==\nparam\n.\nVAR_KEYWORD\n:\n# e.g. **kwargs handling\nkeyword_args\n.\nupdate\n(\nvalue\nor\n{})\nelif\nparam\n.\nkind\nin\n(\nparam\n.\nPOSITIONAL_ONLY\n,\nparam\n.\nPOSITIONAL_OR_KEYWORD\n):\n# Before *args, add to positional args. After *args, add to keyword args.\nif\nnot\nseen_var_positional\n:\npositional_args\n.\nappend\n(\nvalue\n)\nelse\n:\nkeyword_args\n[\nname\n]\n=\nvalue\nelse\n:\n# For KEYWORD_ONLY parameters, always use keyword args.\nkeyword_args\n[\nname\n]\n=\nvalue\nreturn\npositional_args\n,\nkeyword_args\nFuncDocumentation\ndataclass\nContains metadata about a python function, extracted from its docstring.\nSource code in\nsrc/agents/function_schema.py\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n@dataclass\nclass\nFuncDocumentation\n:\n\"\"\"Contains metadata about a python function, extracted from its docstring.\"\"\"\nname\n:\nstr\n\"\"\"The name of the function, via `__name__`.\"\"\"\ndescription\n:\nstr\n|\nNone\n\"\"\"The description of the function, derived from the docstring.\"\"\"\nparam_descriptions\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\n\"\"\"The parameter descriptions of the function, derived from the docstring.\"\"\"\nname\ninstance-attribute\nname\n:\nstr\nThe name of the function, via\n__name__\n.\ndescription\ninstance-attribute\ndescription\n:\nstr\n|\nNone\nThe description of the function, derived from the docstring.\nparam_descriptions\ninstance-attribute\nparam_descriptions\n:\ndict\n[\nstr\n,\nstr\n]\n|\nNone\nThe parameter descriptions of the function, derived from the docstring.\ngenerate_func_documentation\ngenerate_func_documentation\n(\nfunc\n:\nCallable\n[\n...\n,\nAny\n],\nstyle\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\n)\n->\nFuncDocumentation\nExtracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.\nParameters:\nName\nType\nDescription\nDefault\nfunc\nCallable\n[...,\nAny\n]\nThe function to extract documentation from.\nrequired\nstyle\nDocstringStyle\n| None\nThe style of the docstring to use for parsing. If not provided, we will attempt to\nauto-detect the style.\nNone\nReturns:\nType\nDescription\nFuncDocumentation\nA FuncDocumentation object containing the function's name, description, and parameter\nFuncDocumentation\ndescriptions.\nSource code in\nsrc/agents/function_schema.py\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\ndef\ngenerate_func_documentation\n(\nfunc\n:\nCallable\n[\n...\n,\nAny\n],\nstyle\n:\nDocstringStyle\n|\nNone\n=\nNone\n)\n->\nFuncDocumentation\n:\n\"\"\"\nExtracts metadata from a function docstring, in preparation for sending it to an LLM as a tool.\nArgs:\nfunc: The function to extract documentation from.\nstyle: The style of the docstring to use for parsing. If not provided, we will attempt to\nauto-detect the style.\nReturns:\nA FuncDocumentation object containing the function's name, description, and parameter\ndescriptions.\n\"\"\"\nname\n=\nfunc\n.\n__name__\ndoc\n=\ninspect\n.\ngetdoc\n(\nfunc\n)\nif\nnot\ndoc\n:\nreturn\nFuncDocumentation\n(\nname\n=\nname\n,\ndescription\n=\nNone\n,\nparam_descriptions\n=\nNone\n)\nwith\n_suppress_griffe_logging\n():\ndocstring\n=\nDocstring\n(\ndoc\n,\nlineno\n=\n1\n,\nparser\n=\nstyle\nor\n_detect_docstring_style\n(\ndoc\n))\nparsed\n=\ndocstring\n.\nparse\n()\ndescription\n:\nstr\n|\nNone\n=\nnext\n(\n(\nsection\n.\nvalue\nfor\nsection\nin\nparsed\nif\nsection\n.\nkind\n==\nDocstringSectionKind\n.\ntext\n),\nNone\n)\nparam_descriptions\n:\ndict\n[\nstr\n,\nstr\n]\n=\n{\nparam\n.\nname\n:\nparam\n.\ndescription\nfor\nsection\nin\nparsed\nif\nsection\n.\nkind\n==\nDocstringSectionKind\n.\nparameters\nfor\nparam\nin\nsection\n.\nvalue\n}\nreturn\nFuncDocumentation\n(\nname\n=\nfunc\n.\n__name__\n,\ndescription\n=\ndescription\n,\nparam_descriptions\n=\nparam_descriptions\nor\nNone\n,\n)\nfunction_schema\nfunction_schema\n(\nfunc\n:\nCallable\n[\n...\n,\nAny\n],\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nstrict_json_schema\n:\nbool\n=\nTrue\n,\n)\n->\nFuncSchema\nGiven a python function, extracts a\nFuncSchema\nfrom it, capturing the name, description,\nparameter descriptions, and other metadata.\nParameters:\nName\nType\nDescription\nDefault\nfunc\nCallable\n[...,\nAny\n]\nThe function to extract the schema from.\nrequired\ndocstring_style\nDocstringStyle\n| None\nThe style of the docstring to use for parsing. If not provided, we will\nattempt to auto-detect the style.\nNone\nname_override\nstr\n| None\nIf provided, use this name instead of the function's\n__name__\n.\nNone\ndescription_override\nstr\n| None\nIf provided, use this description instead of the one derived from the\ndocstring.\nNone\nuse_docstring_info\nbool\nIf True, uses the docstring to generate the description and parameter\ndescriptions.\nTrue\nstrict_json_schema\nbool\nWhether the JSON schema is in strict mode. If True, we'll ensure that\nthe schema adheres to the \"strict\" standard the OpenAI API expects. We\nstrongly\nrecommend setting this to True, as it increases the likelihood of the LLM providing\ncorrect JSON input.\nTrue\nReturns:\nType\nDescription\nFuncSchema\nA\nFuncSchema\nobject containing the function's name, description, parameter descriptions,\nFuncSchema\nand other metadata.\nSource code in\nsrc/agents/function_schema.py\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\ndef\nfunction_schema\n(\nfunc\n:\nCallable\n[\n...\n,\nAny\n],\ndocstring_style\n:\nDocstringStyle\n|\nNone\n=\nNone\n,\nname_override\n:\nstr\n|\nNone\n=\nNone\n,\ndescription_override\n:\nstr\n|\nNone\n=\nNone\n,\nuse_docstring_info\n:\nbool\n=\nTrue\n,\nstrict_json_schema\n:\nbool\n=\nTrue\n,\n)\n->\nFuncSchema\n:\n\"\"\"\nGiven a python function, extracts a `FuncSchema` from it, capturing the name, description,\nparameter descriptions, and other metadata.\nArgs:\nfunc: The function to extract the schema from.\ndocstring_style: The style of the docstring to use for parsing. If not provided, we will\nattempt to auto-detect the style.\nname_override: If provided, use this name instead of the function's `__name__`.\ndescription_override: If provided, use this description instead of the one derived from the\ndocstring.\nuse_docstring_info: If True, uses the docstring to generate the description and parameter\ndescriptions.\nstrict_json_schema: Whether the JSON schema is in strict mode. If True, we'll ensure that\nthe schema adheres to the \"strict\" standard the OpenAI API expects. We **strongly**\nrecommend setting this to True, as it increases the likelihood of the LLM providing\ncorrect JSON input.\nReturns:\nA `FuncSchema` object containing the function's name, description, parameter descriptions,\nand other metadata.\n\"\"\"\n# 1. Grab docstring info\nif\nuse_docstring_info\n:\ndoc_info\n=\ngenerate_func_documentation\n(\nfunc\n,\ndocstring_style\n)\nparam_descs\n=\ndoc_info\n.\nparam_descriptions\nor\n{}\nelse\n:\ndoc_info\n=\nNone\nparam_descs\n=\n{}\nfunc_name\n=\nname_override\nor\ndoc_info\n.\nname\nif\ndoc_info\nelse\nfunc\n.\n__name__\n# 2. Inspect function signature and get type hints\nsig\n=\ninspect\n.\nsignature\n(\nfunc\n)\ntype_hints\n=\nget_type_hints\n(\nfunc\n)\nparams\n=\nlist\n(\nsig\n.\nparameters\n.\nitems\n())\ntakes_context\n=\nFalse\nfiltered_params\n=\n[]\nif\nparams\n:\nfirst_name\n,\nfirst_param\n=\nparams\n[\n0\n]\n# Prefer the evaluated type hint if available\nann\n=\ntype_hints\n.\nget\n(\nfirst_name\n,\nfirst_param\n.\nannotation\n)\nif\nann\n!=\ninspect\n.\n_empty\n:\norigin\n=\nget_origin\n(\nann\n)\nor\nann\nif\norigin\nis\nRunContextWrapper\n:\ntakes_context\n=\nTrue\n# Mark that the function takes context\nelse\n:\nfiltered_params\n.\nappend\n((\nfirst_name\n,\nfirst_param\n))\nelse\n:\nfiltered_params\n.\nappend\n((\nfirst_name\n,\nfirst_param\n))\n# For parameters other than the first, raise error if any use RunContextWrapper.\nfor\nname\n,\nparam\nin\nparams\n[\n1\n:]:\nann\n=\ntype_hints\n.\nget\n(\nname\n,\nparam\n.\nannotation\n)\nif\nann\n!=\ninspect\n.\n_empty\n:\norigin\n=\nget_origin\n(\nann\n)\nor\nann\nif\norigin\nis\nRunContextWrapper\n:\nraise\nUserError\n(\nf\n\"RunContextWrapper param found at non-first position in function\"\nf\n\"\n{\nfunc\n.\n__name__\n}\n\"\n)\nfiltered_params\n.\nappend\n((\nname\n,\nparam\n))\n# We will collect field definitions for create_model as a dict:\n#   field_name -> (type_annotation, default_value_or_Field(...))\nfields\n:\ndict\n[\nstr\n,\nAny\n]\n=\n{}\nfor\nname\n,\nparam\nin\nfiltered_params\n:\nann\n=\ntype_hints\n.\nget\n(\nname\n,\nparam\n.\nannotation\n)\ndefault\n=\nparam\n.\ndefault\n# If there's no type hint, assume `Any`\nif\nann\n==\ninspect\n.\n_empty\n:\nann\n=\nAny\n# If a docstring param description exists, use it\nfield_description\n=\nparam_descs\n.\nget\n(\nname\n,\nNone\n)\n# Handle different parameter kinds\nif\nparam\n.\nkind\n==\nparam\n.\nVAR_POSITIONAL\n:\n# e.g. *args: extend positional args\nif\nget_origin\n(\nann\n)\nis\ntuple\n:\n# e.g. def foo(*args: tuple[int, ...]) -> treat as List[int]\nargs_of_tuple\n=\nget_args\n(\nann\n)\nif\nlen\n(\nargs_of_tuple\n)\n==\n2\nand\nargs_of_tuple\n[\n1\n]\nis\nEllipsis\n:\nann\n=\nlist\n[\nargs_of_tuple\n[\n0\n]]\n# type: ignore\nelse\n:\nann\n=\nlist\n[\nAny\n]\nelse\n:\n# If user wrote *args: int, treat as List[int]\nann\n=\nlist\n[\nann\n]\n# type: ignore\n# Default factory to empty list\nfields\n[\nname\n]\n=\n(\nann\n,\nField\n(\ndefault_factory\n=\nlist\n,\ndescription\n=\nfield_description\n),\n# type: ignore\n)\nelif\nparam\n.\nkind\n==\nparam\n.\nVAR_KEYWORD\n:\n# **kwargs handling\nif\nget_origin\n(\nann\n)\nis\ndict\n:\n# e.g. def foo(**kwargs: dict[str, int])\ndict_args\n=\nget_args\n(\nann\n)\nif\nlen\n(\ndict_args\n)\n==\n2\n:\nann\n=\ndict\n[\ndict_args\n[\n0\n],\ndict_args\n[\n1\n]]\n# type: ignore\nelse\n:\nann\n=\ndict\n[\nstr\n,\nAny\n]\nelse\n:\n# e.g. def foo(**kwargs: int) -> Dict[str, int]\nann\n=\ndict\n[\nstr\n,\nann\n]\n# type: ignore\nfields\n[\nname\n]\n=\n(\nann\n,\nField\n(\ndefault_factory\n=\ndict\n,\ndescription\n=\nfield_description\n),\n# type: ignore\n)\nelse\n:\n# Normal parameter\nif\ndefault\n==\ninspect\n.\n_empty\n:\n# Required field\nfields\n[\nname\n]\n=\n(\nann\n,\nField\n(\n...\n,\ndescription\n=\nfield_description\n),\n)\nelse\n:\n# Parameter with a default value\nfields\n[\nname\n]\n=\n(\nann\n,\nField\n(\ndefault\n=\ndefault\n,\ndescription\n=\nfield_description\n),\n)\n# 3. Dynamically build a Pydantic model\ndynamic_model\n=\ncreate_model\n(\nf\n\"\n{\nfunc_name\n}\n_args\"\n,\n__base__\n=\nBaseModel\n,\n**\nfields\n)\n# 4. Build JSON schema from that model\njson_schema\n=\ndynamic_model\n.\nmodel_json_schema\n()\nif\nstrict_json_schema\n:\njson_schema\n=\nensure_strict_json_schema\n(\njson_schema\n)\n# 5. Return as a FuncSchema dataclass\nreturn\nFuncSchema\n(\nname\n=\nfunc_name\n,\ndescription\n=\ndescription_override\nor\ndoc_info\n.\ndescription\nif\ndoc_info\nelse\nNone\n,\nparams_pydantic_model\n=\ndynamic_model\n,\nparams_json_schema\n=\njson_schema\n,\nsignature\n=\nsig\n,\ntakes_context\n=\ntakes_context\n,\nstrict_json_schema\n=\nstrict_json_schema\n,\n)",
  "Model interface": "Model interface\nModelTracing\nBases:\nEnum\nSource code in\nsrc/agents/models/interface.py\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nclass\nModelTracing\n(\nenum\n.\nEnum\n):\nDISABLED\n=\n0\n\"\"\"Tracing is disabled entirely.\"\"\"\nENABLED\n=\n1\n\"\"\"Tracing is enabled, and all data is included.\"\"\"\nENABLED_WITHOUT_DATA\n=\n2\n\"\"\"Tracing is enabled, but inputs/outputs are not included.\"\"\"\ndef\nis_disabled\n(\nself\n)\n->\nbool\n:\nreturn\nself\n==\nModelTracing\n.\nDISABLED\ndef\ninclude_data\n(\nself\n)\n->\nbool\n:\nreturn\nself\n==\nModelTracing\n.\nENABLED\nDISABLED\nclass-attribute\ninstance-attribute\nDISABLED\n=\n0\nTracing is disabled entirely.\nENABLED\nclass-attribute\ninstance-attribute\nENABLED\n=\n1\nTracing is enabled, and all data is included.\nENABLED_WITHOUT_DATA\nclass-attribute\ninstance-attribute\nENABLED_WITHOUT_DATA\n=\n2\nTracing is enabled, but inputs/outputs are not included.\nModel\nBases:\nABC\nThe base interface for calling an LLM.\nSource code in\nsrc/agents/models/interface.py\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\nclass\nModel\n(\nabc\n.\nABC\n):\n\"\"\"The base interface for calling an LLM.\"\"\"\n@abc\n.\nabstractmethod\nasync\ndef\nget_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\n\"\"\"Get a response from the model.\nArgs:\nsystem_instructions: The system instructions to use.\ninput: The input items to the model, in OpenAI Responses format.\nmodel_settings: The model settings to use.\ntools: The tools available to the model.\noutput_schema: The output schema to use.\nhandoffs: The handoffs available to the model.\ntracing: Tracing configuration.\nprevious_response_id: the ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nReturns:\nThe full model response.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]:\n\"\"\"Stream a response from the model.\nArgs:\nsystem_instructions: The system instructions to use.\ninput: The input items to the model, in OpenAI Responses format.\nmodel_settings: The model settings to use.\ntools: The tools available to the model.\noutput_schema: The output schema to use.\nhandoffs: The handoffs available to the model.\ntracing: Tracing configuration.\nprevious_response_id: the ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nReturns:\nAn iterator of response stream events, in OpenAI Responses format.\n\"\"\"\npass\nget_response\nabstractmethod\nasync\nget_response\n(\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\nGet a response from the model.\nParameters:\nName\nType\nDescription\nDefault\nsystem_instructions\nstr\n| None\nThe system instructions to use.\nrequired\ninput\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe input items to the model, in OpenAI Responses format.\nrequired\nmodel_settings\nModelSettings\nThe model settings to use.\nrequired\ntools\nlist\n[\nTool\n]\nThe tools available to the model.\nrequired\noutput_schema\nAgentOutputSchema\n| None\nThe output schema to use.\nrequired\nhandoffs\nlist\n[\nHandoff\n]\nThe handoffs available to the model.\nrequired\ntracing\nModelTracing\nTracing configuration.\nrequired\nprevious_response_id\nstr\n| None\nthe ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nrequired\nReturns:\nType\nDescription\nModelResponse\nThe full model response.\nSource code in\nsrc/agents/models/interface.py\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n@abc\n.\nabstractmethod\nasync\ndef\nget_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\n\"\"\"Get a response from the model.\nArgs:\nsystem_instructions: The system instructions to use.\ninput: The input items to the model, in OpenAI Responses format.\nmodel_settings: The model settings to use.\ntools: The tools available to the model.\noutput_schema: The output schema to use.\nhandoffs: The handoffs available to the model.\ntracing: Tracing configuration.\nprevious_response_id: the ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nReturns:\nThe full model response.\n\"\"\"\npass\nstream_response\nabstractmethod\nstream_response\n(\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]\nStream a response from the model.\nParameters:\nName\nType\nDescription\nDefault\nsystem_instructions\nstr\n| None\nThe system instructions to use.\nrequired\ninput\nstr\n|\nlist\n[\nTResponseInputItem\n]\nThe input items to the model, in OpenAI Responses format.\nrequired\nmodel_settings\nModelSettings\nThe model settings to use.\nrequired\ntools\nlist\n[\nTool\n]\nThe tools available to the model.\nrequired\noutput_schema\nAgentOutputSchema\n| None\nThe output schema to use.\nrequired\nhandoffs\nlist\n[\nHandoff\n]\nThe handoffs available to the model.\nrequired\ntracing\nModelTracing\nTracing configuration.\nrequired\nprevious_response_id\nstr\n| None\nthe ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nrequired\nReturns:\nType\nDescription\nAsyncIterator\n[\nTResponseStreamEvent\n]\nAn iterator of response stream events, in OpenAI Responses format.\nSource code in\nsrc/agents/models/interface.py\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n@abc\n.\nabstractmethod\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]:\n\"\"\"Stream a response from the model.\nArgs:\nsystem_instructions: The system instructions to use.\ninput: The input items to the model, in OpenAI Responses format.\nmodel_settings: The model settings to use.\ntools: The tools available to the model.\noutput_schema: The output schema to use.\nhandoffs: The handoffs available to the model.\ntracing: Tracing configuration.\nprevious_response_id: the ID of the previous response. Generally not used by the model,\nexcept for the OpenAI Responses API.\nReturns:\nAn iterator of response stream events, in OpenAI Responses format.\n\"\"\"\npass\nModelProvider\nBases:\nABC\nThe base interface for a model provider.\nModel provider is responsible for looking up Models by name.\nSource code in\nsrc/agents/models/interface.py\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\nclass\nModelProvider\n(\nabc\n.\nABC\n):\n\"\"\"The base interface for a model provider.\nModel provider is responsible for looking up Models by name.\n\"\"\"\n@abc\n.\nabstractmethod\ndef\nget_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nModel\n:\n\"\"\"Get a model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe model.\n\"\"\"\nget_model\nabstractmethod\nget_model\n(\nmodel_name\n:\nstr\n|\nNone\n)\n->\nModel\nGet a model by name.\nParameters:\nName\nType\nDescription\nDefault\nmodel_name\nstr\n| None\nThe name of the model to get.\nrequired\nReturns:\nType\nDescription\nModel\nThe model.\nSource code in\nsrc/agents/models/interface.py\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n@abc\n.\nabstractmethod\ndef\nget_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nModel\n:\n\"\"\"Get a model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe model.\n\"\"\"",
  "OpenAI Chat Completions model": "OpenAI Chat Completions model\nOpenAIChatCompletionsModel\nBases:\nModel\nSource code in\nsrc/agents/models/openai_chatcompletions.py\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\nclass\nOpenAIChatCompletionsModel\n(\nModel\n):\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n|\nChatModel\n,\nopenai_client\n:\nAsyncOpenAI\n,\n)\n->\nNone\n:\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\ndef\n_non_null_or_not_given\n(\nself\n,\nvalue\n:\nAny\n)\n->\nAny\n:\nreturn\nvalue\nif\nvalue\nis\nnot\nNone\nelse\nNOT_GIVEN\nasync\ndef\nget_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\nwith\ngeneration_span\n(\nmodel\n=\nstr\n(\nself\n.\nmodel\n),\nmodel_config\n=\ndataclasses\n.\nasdict\n(\nmodel_settings\n)\n|\n{\n\"base_url\"\n:\nstr\n(\nself\n.\n_client\n.\nbase_url\n)},\ndisabled\n=\ntracing\n.\nis_disabled\n(),\n)\nas\nspan_generation\n:\nresponse\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nspan_generation\n,\ntracing\n,\nstream\n=\nFalse\n,\n)\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Received model response\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"LLM resp:\n\\n\n{\njson\n.\ndumps\n(\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\nmodel_dump\n(),\nindent\n=\n2\n)\n}\n\\n\n\"\n)\nusage\n=\n(\nUsage\n(\nrequests\n=\n1\n,\ninput_tokens\n=\nresponse\n.\nusage\n.\nprompt_tokens\n,\noutput_tokens\n=\nresponse\n.\nusage\n.\ncompletion_tokens\n,\ntotal_tokens\n=\nresponse\n.\nusage\n.\ntotal_tokens\n,\n)\nif\nresponse\n.\nusage\nelse\nUsage\n()\n)\nif\ntracing\n.\ninclude_data\n():\nspan_generation\n.\nspan_data\n.\noutput\n=\n[\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\nmodel_dump\n()]\nspan_generation\n.\nspan_data\n.\nusage\n=\n{\n\"input_tokens\"\n:\nusage\n.\ninput_tokens\n,\n\"output_tokens\"\n:\nusage\n.\noutput_tokens\n,\n}\nitems\n=\nConverter\n.\nmessage_to_output_items\n(\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n)\nreturn\nModelResponse\n(\noutput\n=\nitems\n,\nusage\n=\nusage\n,\nresponse_id\n=\nNone\n,\n)\nasync\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]:\n\"\"\"\nYields a partial message as it is generated, as well as the usage information.\n\"\"\"\nwith\ngeneration_span\n(\nmodel\n=\nstr\n(\nself\n.\nmodel\n),\nmodel_config\n=\ndataclasses\n.\nasdict\n(\nmodel_settings\n)\n|\n{\n\"base_url\"\n:\nstr\n(\nself\n.\n_client\n.\nbase_url\n)},\ndisabled\n=\ntracing\n.\nis_disabled\n(),\n)\nas\nspan_generation\n:\nresponse\n,\nstream\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nspan_generation\n,\ntracing\n,\nstream\n=\nTrue\n,\n)\nfinal_response\n:\nResponse\n|\nNone\n=\nNone\nasync\nfor\nchunk\nin\nChatCmplStreamHandler\n.\nhandle_stream\n(\nresponse\n,\nstream\n):\nyield\nchunk\nif\nchunk\n.\ntype\n==\n\"response.completed\"\n:\nfinal_response\n=\nchunk\n.\nresponse\nif\ntracing\n.\ninclude_data\n()\nand\nfinal_response\n:\nspan_generation\n.\nspan_data\n.\noutput\n=\n[\nfinal_response\n.\nmodel_dump\n()]\nif\nfinal_response\nand\nfinal_response\n.\nusage\n:\nspan_generation\n.\nspan_data\n.\nusage\n=\n{\n\"input_tokens\"\n:\nfinal_response\n.\nusage\n.\ninput_tokens\n,\n\"output_tokens\"\n:\nfinal_response\n.\nusage\n.\noutput_tokens\n,\n}\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nLiteral\n[\nTrue\n],\n)\n->\ntuple\n[\nResponse\n,\nAsyncStream\n[\nChatCompletionChunk\n]]:\n...\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nLiteral\n[\nFalse\n],\n)\n->\nChatCompletion\n:\n...\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nbool\n=\nFalse\n,\n)\n->\nChatCompletion\n|\ntuple\n[\nResponse\n,\nAsyncStream\n[\nChatCompletionChunk\n]]:\nconverted_messages\n=\nConverter\n.\nitems_to_messages\n(\ninput\n)\nif\nsystem_instructions\n:\nconverted_messages\n.\ninsert\n(\n0\n,\n{\n\"content\"\n:\nsystem_instructions\n,\n\"role\"\n:\n\"system\"\n,\n},\n)\nif\ntracing\n.\ninclude_data\n():\nspan\n.\nspan_data\n.\ninput\n=\nconverted_messages\nparallel_tool_calls\n=\n(\nTrue\nif\nmodel_settings\n.\nparallel_tool_calls\nand\ntools\nand\nlen\n(\ntools\n)\n>\n0\nelse\nFalse\nif\nmodel_settings\n.\nparallel_tool_calls\nis\nFalse\nelse\nNOT_GIVEN\n)\ntool_choice\n=\nConverter\n.\nconvert_tool_choice\n(\nmodel_settings\n.\ntool_choice\n)\nresponse_format\n=\nConverter\n.\nconvert_response_format\n(\noutput_schema\n)\nconverted_tools\n=\n[\nConverter\n.\ntool_to_openai\n(\ntool\n)\nfor\ntool\nin\ntools\n]\nif\ntools\nelse\n[]\nfor\nhandoff\nin\nhandoffs\n:\nconverted_tools\n.\nappend\n(\nConverter\n.\nconvert_handoff_tool\n(\nhandoff\n))\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Calling LLM\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"\n{\njson\n.\ndumps\n(\nconverted_messages\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Tools:\n\\n\n{\njson\n.\ndumps\n(\nconverted_tools\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Stream:\n{\nstream\n}\n\\n\n\"\nf\n\"Tool choice:\n{\ntool_choice\n}\n\\n\n\"\nf\n\"Response format:\n{\nresponse_format\n}\n\\n\n\"\n)\nreasoning_effort\n=\nmodel_settings\n.\nreasoning\n.\neffort\nif\nmodel_settings\n.\nreasoning\nelse\nNone\nstore\n=\nChatCmplHelpers\n.\nget_store_param\n(\nself\n.\n_get_client\n(),\nmodel_settings\n)\nstream_options\n=\nChatCmplHelpers\n.\nget_stream_options_param\n(\nself\n.\n_get_client\n(),\nmodel_settings\n,\nstream\n=\nstream\n)\nret\n=\nawait\nself\n.\n_get_client\n()\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\nself\n.\nmodel\n,\nmessages\n=\nconverted_messages\n,\ntools\n=\nconverted_tools\nor\nNOT_GIVEN\n,\ntemperature\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\ntemperature\n),\ntop_p\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\ntop_p\n),\nfrequency_penalty\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nfrequency_penalty\n),\npresence_penalty\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\npresence_penalty\n),\nmax_tokens\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nmax_tokens\n),\ntool_choice\n=\ntool_choice\n,\nresponse_format\n=\nresponse_format\n,\nparallel_tool_calls\n=\nparallel_tool_calls\n,\nstream\n=\nstream\n,\nstream_options\n=\nself\n.\n_non_null_or_not_given\n(\nstream_options\n),\nstore\n=\nself\n.\n_non_null_or_not_given\n(\nstore\n),\nreasoning_effort\n=\nself\n.\n_non_null_or_not_given\n(\nreasoning_effort\n),\nextra_headers\n=\nHEADERS\n,\nextra_query\n=\nmodel_settings\n.\nextra_query\n,\nextra_body\n=\nmodel_settings\n.\nextra_body\n,\nmetadata\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nmetadata\n),\n)\nif\nisinstance\n(\nret\n,\nChatCompletion\n):\nreturn\nret\nresponse\n=\nResponse\n(\nid\n=\nFAKE_RESPONSES_ID\n,\ncreated_at\n=\ntime\n.\ntime\n(),\nmodel\n=\nself\n.\nmodel\n,\nobject\n=\n\"response\"\n,\noutput\n=\n[],\ntool_choice\n=\ncast\n(\nLiteral\n[\n\"auto\"\n,\n\"required\"\n,\n\"none\"\n],\ntool_choice\n)\nif\ntool_choice\n!=\nNOT_GIVEN\nelse\n\"auto\"\n,\ntop_p\n=\nmodel_settings\n.\ntop_p\n,\ntemperature\n=\nmodel_settings\n.\ntemperature\n,\ntools\n=\n[],\nparallel_tool_calls\n=\nparallel_tool_calls\nor\nFalse\n,\nreasoning\n=\nmodel_settings\n.\nreasoning\n,\n)\nreturn\nresponse\n,\nret\ndef\n_get_client\n(\nself\n)\n->\nAsyncOpenAI\n:\nif\nself\n.\n_client\nis\nNone\n:\nself\n.\n_client\n=\nAsyncOpenAI\n()\nreturn\nself\n.\n_client\nstream_response\nasync\nstream_response\n(\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]\nYields a partial message as it is generated, as well as the usage information.\nSource code in\nsrc/agents/models/openai_chatcompletions.py\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\nasync\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]:\n\"\"\"\nYields a partial message as it is generated, as well as the usage information.\n\"\"\"\nwith\ngeneration_span\n(\nmodel\n=\nstr\n(\nself\n.\nmodel\n),\nmodel_config\n=\ndataclasses\n.\nasdict\n(\nmodel_settings\n)\n|\n{\n\"base_url\"\n:\nstr\n(\nself\n.\n_client\n.\nbase_url\n)},\ndisabled\n=\ntracing\n.\nis_disabled\n(),\n)\nas\nspan_generation\n:\nresponse\n,\nstream\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nspan_generation\n,\ntracing\n,\nstream\n=\nTrue\n,\n)\nfinal_response\n:\nResponse\n|\nNone\n=\nNone\nasync\nfor\nchunk\nin\nChatCmplStreamHandler\n.\nhandle_stream\n(\nresponse\n,\nstream\n):\nyield\nchunk\nif\nchunk\n.\ntype\n==\n\"response.completed\"\n:\nfinal_response\n=\nchunk\n.\nresponse\nif\ntracing\n.\ninclude_data\n()\nand\nfinal_response\n:\nspan_generation\n.\nspan_data\n.\noutput\n=\n[\nfinal_response\n.\nmodel_dump\n()]\nif\nfinal_response\nand\nfinal_response\n.\nusage\n:\nspan_generation\n.\nspan_data\n.\nusage\n=\n{\n\"input_tokens\"\n:\nfinal_response\n.\nusage\n.\ninput_tokens\n,\n\"output_tokens\"\n:\nfinal_response\n.\nusage\n.\noutput_tokens\n,\n}",
  "OpenAI Responses model": "OpenAI Responses model\nOpenAIResponsesModel\nBases:\nModel\nImplementation of\nModel\nthat uses the OpenAI Responses API.\nSource code in\nsrc/agents/models/openai_responses.py\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\nclass\nOpenAIResponsesModel\n(\nModel\n):\n\"\"\"\nImplementation of `Model` that uses the OpenAI Responses API.\n\"\"\"\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n|\nChatModel\n,\nopenai_client\n:\nAsyncOpenAI\n,\n)\n->\nNone\n:\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\ndef\n_non_null_or_not_given\n(\nself\n,\nvalue\n:\nAny\n)\n->\nAny\n:\nreturn\nvalue\nif\nvalue\nis\nnot\nNone\nelse\nNOT_GIVEN\nasync\ndef\nget_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\nwith\nresponse_span\n(\ndisabled\n=\ntracing\n.\nis_disabled\n())\nas\nspan_response\n:\ntry\n:\nresponse\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nprevious_response_id\n,\nstream\n=\nFalse\n,\n)\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"LLM responded\"\n)\nelse\n:\nlogger\n.\ndebug\n(\n\"LLM resp:\n\\n\n\"\nf\n\"\n{\njson\n.\ndumps\n([\nx\n.\nmodel_dump\n()\nfor\nx\nin\nresponse\n.\noutput\n],\nindent\n=\n2\n)\n}\n\\n\n\"\n)\nusage\n=\n(\nUsage\n(\nrequests\n=\n1\n,\ninput_tokens\n=\nresponse\n.\nusage\n.\ninput_tokens\n,\noutput_tokens\n=\nresponse\n.\nusage\n.\noutput_tokens\n,\ntotal_tokens\n=\nresponse\n.\nusage\n.\ntotal_tokens\n,\n)\nif\nresponse\n.\nusage\nelse\nUsage\n()\n)\nif\ntracing\n.\ninclude_data\n():\nspan_response\n.\nspan_data\n.\nresponse\n=\nresponse\nspan_response\n.\nspan_data\n.\ninput\n=\ninput\nexcept\nException\nas\ne\n:\nspan_response\n.\nset_error\n(\nSpanError\n(\nmessage\n=\n\"Error getting response\"\n,\ndata\n=\n{\n\"error\"\n:\nstr\n(\ne\n)\nif\ntracing\n.\ninclude_data\n()\nelse\ne\n.\n__class__\n.\n__name__\n,\n},\n)\n)\nrequest_id\n=\ne\n.\nrequest_id\nif\nisinstance\n(\ne\n,\nAPIStatusError\n)\nelse\nNone\nlogger\n.\nerror\n(\nf\n\"Error getting response:\n{\ne\n}\n. (request_id:\n{\nrequest_id\n}\n)\"\n)\nraise\nreturn\nModelResponse\n(\noutput\n=\nresponse\n.\noutput\n,\nusage\n=\nusage\n,\nresponse_id\n=\nresponse\n.\nid\n,\n)\nasync\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nResponseStreamEvent\n]:\n\"\"\"\nYields a partial message as it is generated, as well as the usage information.\n\"\"\"\nwith\nresponse_span\n(\ndisabled\n=\ntracing\n.\nis_disabled\n())\nas\nspan_response\n:\ntry\n:\nstream\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nprevious_response_id\n,\nstream\n=\nTrue\n,\n)\nfinal_response\n:\nResponse\n|\nNone\n=\nNone\nasync\nfor\nchunk\nin\nstream\n:\nif\nisinstance\n(\nchunk\n,\nResponseCompletedEvent\n):\nfinal_response\n=\nchunk\n.\nresponse\nyield\nchunk\nif\nfinal_response\nand\ntracing\n.\ninclude_data\n():\nspan_response\n.\nspan_data\n.\nresponse\n=\nfinal_response\nspan_response\n.\nspan_data\n.\ninput\n=\ninput\nexcept\nException\nas\ne\n:\nspan_response\n.\nset_error\n(\nSpanError\n(\nmessage\n=\n\"Error streaming response\"\n,\ndata\n=\n{\n\"error\"\n:\nstr\n(\ne\n)\nif\ntracing\n.\ninclude_data\n()\nelse\ne\n.\n__class__\n.\n__name__\n,\n},\n)\n)\nlogger\n.\nerror\n(\nf\n\"Error streaming response:\n{\ne\n}\n\"\n)\nraise\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nprevious_response_id\n:\nstr\n|\nNone\n,\nstream\n:\nLiteral\n[\nTrue\n],\n)\n->\nAsyncStream\n[\nResponseStreamEvent\n]:\n...\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nprevious_response_id\n:\nstr\n|\nNone\n,\nstream\n:\nLiteral\n[\nFalse\n],\n)\n->\nResponse\n:\n...\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nprevious_response_id\n:\nstr\n|\nNone\n,\nstream\n:\nLiteral\n[\nTrue\n]\n|\nLiteral\n[\nFalse\n]\n=\nFalse\n,\n)\n->\nResponse\n|\nAsyncStream\n[\nResponseStreamEvent\n]:\nlist_input\n=\nItemHelpers\n.\ninput_to_new_input_list\n(\ninput\n)\nparallel_tool_calls\n=\n(\nTrue\nif\nmodel_settings\n.\nparallel_tool_calls\nand\ntools\nand\nlen\n(\ntools\n)\n>\n0\nelse\nFalse\nif\nmodel_settings\n.\nparallel_tool_calls\nis\nFalse\nelse\nNOT_GIVEN\n)\ntool_choice\n=\nConverter\n.\nconvert_tool_choice\n(\nmodel_settings\n.\ntool_choice\n)\nconverted_tools\n=\nConverter\n.\nconvert_tools\n(\ntools\n,\nhandoffs\n)\nresponse_format\n=\nConverter\n.\nget_response_format\n(\noutput_schema\n)\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Calling LLM\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Calling LLM\n{\nself\n.\nmodel\n}\nwith input:\n\\n\n\"\nf\n\"\n{\njson\n.\ndumps\n(\nlist_input\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Tools:\n\\n\n{\njson\n.\ndumps\n(\nconverted_tools\n.\ntools\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Stream:\n{\nstream\n}\n\\n\n\"\nf\n\"Tool choice:\n{\ntool_choice\n}\n\\n\n\"\nf\n\"Response format:\n{\nresponse_format\n}\n\\n\n\"\nf\n\"Previous response id:\n{\nprevious_response_id\n}\n\\n\n\"\n)\nreturn\nawait\nself\n.\n_client\n.\nresponses\n.\ncreate\n(\nprevious_response_id\n=\nself\n.\n_non_null_or_not_given\n(\nprevious_response_id\n),\ninstructions\n=\nself\n.\n_non_null_or_not_given\n(\nsystem_instructions\n),\nmodel\n=\nself\n.\nmodel\n,\ninput\n=\nlist_input\n,\ninclude\n=\nconverted_tools\n.\nincludes\n,\ntools\n=\nconverted_tools\n.\ntools\n,\ntemperature\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\ntemperature\n),\ntop_p\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\ntop_p\n),\ntruncation\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\ntruncation\n),\nmax_output_tokens\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nmax_tokens\n),\ntool_choice\n=\ntool_choice\n,\nparallel_tool_calls\n=\nparallel_tool_calls\n,\nstream\n=\nstream\n,\nextra_headers\n=\n_HEADERS\n,\nextra_query\n=\nmodel_settings\n.\nextra_query\n,\nextra_body\n=\nmodel_settings\n.\nextra_body\n,\ntext\n=\nresponse_format\n,\nstore\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nstore\n),\nreasoning\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nreasoning\n),\nmetadata\n=\nself\n.\n_non_null_or_not_given\n(\nmodel_settings\n.\nmetadata\n),\n)\ndef\n_get_client\n(\nself\n)\n->\nAsyncOpenAI\n:\nif\nself\n.\n_client\nis\nNone\n:\nself\n.\n_client\n=\nAsyncOpenAI\n()\nreturn\nself\n.\n_client\nstream_response\nasync\nstream_response\n(\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nResponseStreamEvent\n]\nYields a partial message as it is generated, as well as the usage information.\nSource code in\nsrc/agents/models/openai_responses.py\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\nasync\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nResponseStreamEvent\n]:\n\"\"\"\nYields a partial message as it is generated, as well as the usage information.\n\"\"\"\nwith\nresponse_span\n(\ndisabled\n=\ntracing\n.\nis_disabled\n())\nas\nspan_response\n:\ntry\n:\nstream\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nprevious_response_id\n,\nstream\n=\nTrue\n,\n)\nfinal_response\n:\nResponse\n|\nNone\n=\nNone\nasync\nfor\nchunk\nin\nstream\n:\nif\nisinstance\n(\nchunk\n,\nResponseCompletedEvent\n):\nfinal_response\n=\nchunk\n.\nresponse\nyield\nchunk\nif\nfinal_response\nand\ntracing\n.\ninclude_data\n():\nspan_response\n.\nspan_data\n.\nresponse\n=\nfinal_response\nspan_response\n.\nspan_data\n.\ninput\n=\ninput\nexcept\nException\nas\ne\n:\nspan_response\n.\nset_error\n(\nSpanError\n(\nmessage\n=\n\"Error streaming response\"\n,\ndata\n=\n{\n\"error\"\n:\nstr\n(\ne\n)\nif\ntracing\n.\ninclude_data\n()\nelse\ne\n.\n__class__\n.\n__name__\n,\n},\n)\n)\nlogger\n.\nerror\n(\nf\n\"Error streaming response:\n{\ne\n}\n\"\n)\nraise\nConverter\nSource code in\nsrc/agents/models/openai_responses.py\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\nclass\nConverter\n:\n@classmethod\ndef\nconvert_tool_choice\n(\ncls\n,\ntool_choice\n:\nLiteral\n[\n\"auto\"\n,\n\"required\"\n,\n\"none\"\n]\n|\nstr\n|\nNone\n)\n->\nresponse_create_params\n.\nToolChoice\n|\nNotGiven\n:\nif\ntool_choice\nis\nNone\n:\nreturn\nNOT_GIVEN\nelif\ntool_choice\n==\n\"required\"\n:\nreturn\n\"required\"\nelif\ntool_choice\n==\n\"auto\"\n:\nreturn\n\"auto\"\nelif\ntool_choice\n==\n\"none\"\n:\nreturn\n\"none\"\nelif\ntool_choice\n==\n\"file_search\"\n:\nreturn\n{\n\"type\"\n:\n\"file_search\"\n,\n}\nelif\ntool_choice\n==\n\"web_search_preview\"\n:\nreturn\n{\n\"type\"\n:\n\"web_search_preview\"\n,\n}\nelif\ntool_choice\n==\n\"computer_use_preview\"\n:\nreturn\n{\n\"type\"\n:\n\"computer_use_preview\"\n,\n}\nelse\n:\nreturn\n{\n\"type\"\n:\n\"function\"\n,\n\"name\"\n:\ntool_choice\n,\n}\n@classmethod\ndef\nget_response_format\n(\ncls\n,\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n)\n->\nResponseTextConfigParam\n|\nNotGiven\n:\nif\noutput_schema\nis\nNone\nor\noutput_schema\n.\nis_plain_text\n():\nreturn\nNOT_GIVEN\nelse\n:\nreturn\n{\n\"format\"\n:\n{\n\"type\"\n:\n\"json_schema\"\n,\n\"name\"\n:\n\"final_output\"\n,\n\"schema\"\n:\noutput_schema\n.\njson_schema\n(),\n\"strict\"\n:\noutput_schema\n.\nstrict_json_schema\n,\n}\n}\n@classmethod\ndef\nconvert_tools\n(\ncls\n,\ntools\n:\nlist\n[\nTool\n],\nhandoffs\n:\nlist\n[\nHandoff\n[\nAny\n]],\n)\n->\nConvertedTools\n:\nconverted_tools\n:\nlist\n[\nToolParam\n]\n=\n[]\nincludes\n:\nlist\n[\nIncludeLiteral\n]\n=\n[]\ncomputer_tools\n=\n[\ntool\nfor\ntool\nin\ntools\nif\nisinstance\n(\ntool\n,\nComputerTool\n)]\nif\nlen\n(\ncomputer_tools\n)\n>\n1\n:\nraise\nUserError\n(\nf\n\"You can only provide one computer tool. Got\n{\nlen\n(\ncomputer_tools\n)\n}\n\"\n)\nfor\ntool\nin\ntools\n:\nconverted_tool\n,\ninclude\n=\ncls\n.\n_convert_tool\n(\ntool\n)\nconverted_tools\n.\nappend\n(\nconverted_tool\n)\nif\ninclude\n:\nincludes\n.\nappend\n(\ninclude\n)\nfor\nhandoff\nin\nhandoffs\n:\nconverted_tools\n.\nappend\n(\ncls\n.\n_convert_handoff_tool\n(\nhandoff\n))\nreturn\nConvertedTools\n(\ntools\n=\nconverted_tools\n,\nincludes\n=\nincludes\n)\n@classmethod\ndef\n_convert_tool\n(\ncls\n,\ntool\n:\nTool\n)\n->\ntuple\n[\nToolParam\n,\nIncludeLiteral\n|\nNone\n]:\n\"\"\"Returns converted tool and includes\"\"\"\nif\nisinstance\n(\ntool\n,\nFunctionTool\n):\nconverted_tool\n:\nToolParam\n=\n{\n\"name\"\n:\ntool\n.\nname\n,\n\"parameters\"\n:\ntool\n.\nparams_json_schema\n,\n\"strict\"\n:\ntool\n.\nstrict_json_schema\n,\n\"type\"\n:\n\"function\"\n,\n\"description\"\n:\ntool\n.\ndescription\n,\n}\nincludes\n:\nIncludeLiteral\n|\nNone\n=\nNone\nelif\nisinstance\n(\ntool\n,\nWebSearchTool\n):\nws\n:\nWebSearchToolParam\n=\n{\n\"type\"\n:\n\"web_search_preview\"\n,\n\"user_location\"\n:\ntool\n.\nuser_location\n,\n\"search_context_size\"\n:\ntool\n.\nsearch_context_size\n,\n}\nconverted_tool\n=\nws\nincludes\n=\nNone\nelif\nisinstance\n(\ntool\n,\nFileSearchTool\n):\nconverted_tool\n=\n{\n\"type\"\n:\n\"file_search\"\n,\n\"vector_store_ids\"\n:\ntool\n.\nvector_store_ids\n,\n}\nif\ntool\n.\nmax_num_results\n:\nconverted_tool\n[\n\"max_num_results\"\n]\n=\ntool\n.\nmax_num_results\nif\ntool\n.\nranking_options\n:\nconverted_tool\n[\n\"ranking_options\"\n]\n=\ntool\n.\nranking_options\nif\ntool\n.\nfilters\n:\nconverted_tool\n[\n\"filters\"\n]\n=\ntool\n.\nfilters\nincludes\n=\n\"file_search_call.results\"\nif\ntool\n.\ninclude_search_results\nelse\nNone\nelif\nisinstance\n(\ntool\n,\nComputerTool\n):\nconverted_tool\n=\n{\n\"type\"\n:\n\"computer_use_preview\"\n,\n\"environment\"\n:\ntool\n.\ncomputer\n.\nenvironment\n,\n\"display_width\"\n:\ntool\n.\ncomputer\n.\ndimensions\n[\n0\n],\n\"display_height\"\n:\ntool\n.\ncomputer\n.\ndimensions\n[\n1\n],\n}\nincludes\n=\nNone\nelse\n:\nraise\nUserError\n(\nf\n\"Unknown tool type:\n{\ntype\n(\ntool\n)\n}\n, tool\"\n)\nreturn\nconverted_tool\n,\nincludes\n@classmethod\ndef\n_convert_handoff_tool\n(\ncls\n,\nhandoff\n:\nHandoff\n)\n->\nToolParam\n:\nreturn\n{\n\"name\"\n:\nhandoff\n.\ntool_name\n,\n\"parameters\"\n:\nhandoff\n.\ninput_json_schema\n,\n\"strict\"\n:\nhandoff\n.\nstrict_json_schema\n,\n\"type\"\n:\n\"function\"\n,\n\"description\"\n:\nhandoff\n.\ntool_description\n,\n}",
  "MCP Servers": "MCP Servers\nMCPServer\nBases:\nABC\nBase class for Model Context Protocol servers.\nSource code in\nsrc/agents/mcp/server.py\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\nclass\nMCPServer\n(\nabc\n.\nABC\n):\n\"\"\"Base class for Model Context Protocol servers.\"\"\"\n@abc\n.\nabstractmethod\nasync\ndef\nconnect\n(\nself\n):\n\"\"\"Connect to the server. For example, this might mean spawning a subprocess or\nopening a network connection. The server is expected to remain connected until\n`cleanup()` is called.\n\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\nname\n(\nself\n)\n->\nstr\n:\n\"\"\"A readable name for the server.\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\ncleanup\n(\nself\n):\n\"\"\"Cleanup the server. For example, this might mean closing a subprocess or\nclosing a network connection.\n\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\nlist_tools\n(\nself\n)\n->\nlist\n[\nMCPTool\n]:\n\"\"\"List the tools available on the server.\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\ncall_tool\n(\nself\n,\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\n:\n\"\"\"Invoke a tool on the server.\"\"\"\npass\nname\nabstractmethod\nproperty\nname\n:\nstr\nA readable name for the server.\nconnect\nabstractmethod\nasync\nconnect\n()\nConnect to the server. For example, this might mean spawning a subprocess or\nopening a network connection. The server is expected to remain connected until\ncleanup()\nis called.\nSource code in\nsrc/agents/mcp/server.py\n22\n23\n24\n25\n26\n27\n28\n@abc\n.\nabstractmethod\nasync\ndef\nconnect\n(\nself\n):\n\"\"\"Connect to the server. For example, this might mean spawning a subprocess or\nopening a network connection. The server is expected to remain connected until\n`cleanup()` is called.\n\"\"\"\npass\ncleanup\nabstractmethod\nasync\ncleanup\n()\nCleanup the server. For example, this might mean closing a subprocess or\nclosing a network connection.\nSource code in\nsrc/agents/mcp/server.py\n36\n37\n38\n39\n40\n41\n@abc\n.\nabstractmethod\nasync\ndef\ncleanup\n(\nself\n):\n\"\"\"Cleanup the server. For example, this might mean closing a subprocess or\nclosing a network connection.\n\"\"\"\npass\nlist_tools\nabstractmethod\nasync\nlist_tools\n()\n->\nlist\n[\nTool\n]\nList the tools available on the server.\nSource code in\nsrc/agents/mcp/server.py\n43\n44\n45\n46\n@abc\n.\nabstractmethod\nasync\ndef\nlist_tools\n(\nself\n)\n->\nlist\n[\nMCPTool\n]:\n\"\"\"List the tools available on the server.\"\"\"\npass\ncall_tool\nabstractmethod\nasync\ncall_tool\n(\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\nInvoke a tool on the server.\nSource code in\nsrc/agents/mcp/server.py\n48\n49\n50\n51\n@abc\n.\nabstractmethod\nasync\ndef\ncall_tool\n(\nself\n,\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\n:\n\"\"\"Invoke a tool on the server.\"\"\"\npass\nMCPServerStdioParams\nBases:\nTypedDict\nMirrors\nmcp.client.stdio.StdioServerParameters\n, but lets you pass params without another\nimport.\nSource code in\nsrc/agents/mcp/server.py\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\nclass\nMCPServerStdioParams\n(\nTypedDict\n):\n\"\"\"Mirrors `mcp.client.stdio.StdioServerParameters`, but lets you pass params without another\nimport.\n\"\"\"\ncommand\n:\nstr\n\"\"\"The executable to run to start the server. For example, `python` or `node`.\"\"\"\nargs\n:\nNotRequired\n[\nlist\n[\nstr\n]]\n\"\"\"Command line args to pass to the `command` executable. For example, `['foo.py']` or\n`['server.js', '--port', '8080']`.\"\"\"\nenv\n:\nNotRequired\n[\ndict\n[\nstr\n,\nstr\n]]\n\"\"\"The environment variables to set for the server. .\"\"\"\ncwd\n:\nNotRequired\n[\nstr\n|\nPath\n]\n\"\"\"The working directory to use when spawning the process.\"\"\"\nencoding\n:\nNotRequired\n[\nstr\n]\n\"\"\"The text encoding used when sending/receiving messages to the server. Defaults to `utf-8`.\"\"\"\nencoding_error_handler\n:\nNotRequired\n[\nLiteral\n[\n\"strict\"\n,\n\"ignore\"\n,\n\"replace\"\n]]\n\"\"\"The text encoding error handler. Defaults to `strict`.\nSee https://docs.python.org/3/library/codecs.html#codec-base-classes for\nexplanations of possible values.\n\"\"\"\ncommand\ninstance-attribute\ncommand\n:\nstr\nThe executable to run to start the server. For example,\npython\nor\nnode\n.\nargs\ninstance-attribute\nargs\n:\nNotRequired\n[\nlist\n[\nstr\n]]\nCommand line args to pass to the\ncommand\nexecutable. For example,\n['foo.py']\nor\n['server.js', '--port', '8080']\n.\nenv\ninstance-attribute\nenv\n:\nNotRequired\n[\ndict\n[\nstr\n,\nstr\n]]\nThe environment variables to set for the server. .\ncwd\ninstance-attribute\ncwd\n:\nNotRequired\n[\nstr\n|\nPath\n]\nThe working directory to use when spawning the process.\nencoding\ninstance-attribute\nencoding\n:\nNotRequired\n[\nstr\n]\nThe text encoding used when sending/receiving messages to the server. Defaults to\nutf-8\n.\nencoding_error_handler\ninstance-attribute\nencoding_error_handler\n:\nNotRequired\n[\nLiteral\n[\n\"strict\"\n,\n\"ignore\"\n,\n\"replace\"\n]\n]\nThe text encoding error handler. Defaults to\nstrict\n.\nSee https://docs.python.org/3/library/codecs.html#codec-base-classes for\nexplanations of possible values.\nMCPServerStdio\nBases:\n_MCPServerWithClientSession\nMCP server implementation that uses the stdio transport. See the [spec]\n(https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for\ndetails.\nSource code in\nsrc/agents/mcp/server.py\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\nclass\nMCPServerStdio\n(\n_MCPServerWithClientSession\n):\n\"\"\"MCP server implementation that uses the stdio transport. See the [spec]\n(https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) for\ndetails.\n\"\"\"\ndef\n__init__\n(\nself\n,\nparams\n:\nMCPServerStdioParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new MCP server based on the stdio transport.\nArgs:\nparams: The params that configure the server. This includes the command to run to\nstart the server, the args to pass to the command, the environment variables to\nset for the server, the working directory to use when spawning the process, and\nthe text encoding used when sending/receiving messages to the server.\ncache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\ncached and only fetched from the server once. If `False`, the tools list will be\nfetched from the server on each call to `list_tools()`. The cache can be\ninvalidated by calling `invalidate_tools_cache()`. You should set this to `True`\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nname: A readable name for the server. If not provided, we'll create one from the\ncommand.\n\"\"\"\nsuper\n()\n.\n__init__\n(\ncache_tools_list\n)\nself\n.\nparams\n=\nStdioServerParameters\n(\ncommand\n=\nparams\n[\n\"command\"\n],\nargs\n=\nparams\n.\nget\n(\n\"args\"\n,\n[]),\nenv\n=\nparams\n.\nget\n(\n\"env\"\n),\ncwd\n=\nparams\n.\nget\n(\n\"cwd\"\n),\nencoding\n=\nparams\n.\nget\n(\n\"encoding\"\n,\n\"utf-8\"\n),\nencoding_error_handler\n=\nparams\n.\nget\n(\n\"encoding_error_handler\"\n,\n\"strict\"\n),\n)\nself\n.\n_name\n=\nname\nor\nf\n\"stdio:\n{\nself\n.\nparams\n.\ncommand\n}\n\"\ndef\ncreate_streams\n(\nself\n,\n)\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]:\n\"\"\"Create the streams for the server.\"\"\"\nreturn\nstdio_client\n(\nself\n.\nparams\n)\n@property\ndef\nname\n(\nself\n)\n->\nstr\n:\n\"\"\"A readable name for the server.\"\"\"\nreturn\nself\n.\n_name\nname\nproperty\nname\n:\nstr\nA readable name for the server.\n__init__\n__init__\n(\nparams\n:\nMCPServerStdioParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\nCreate a new MCP server based on the stdio transport.\nParameters:\nName\nType\nDescription\nDefault\nparams\nMCPServerStdioParams\nThe params that configure the server. This includes the command to run to\nstart the server, the args to pass to the command, the environment variables to\nset for the server, the working directory to use when spawning the process, and\nthe text encoding used when sending/receiving messages to the server.\nrequired\ncache_tools_list\nbool\nWhether to cache the tools list. If\nTrue\n, the tools list will be\ncached and only fetched from the server once. If\nFalse\n, the tools list will be\nfetched from the server on each call to\nlist_tools()\n. The cache can be\ninvalidated by calling\ninvalidate_tools_cache()\n. You should set this to\nTrue\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nFalse\nname\nstr\n| None\nA readable name for the server. If not provided, we'll create one from the\ncommand.\nNone\nSource code in\nsrc/agents/mcp/server.py\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\ndef\n__init__\n(\nself\n,\nparams\n:\nMCPServerStdioParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new MCP server based on the stdio transport.\nArgs:\nparams: The params that configure the server. This includes the command to run to\nstart the server, the args to pass to the command, the environment variables to\nset for the server, the working directory to use when spawning the process, and\nthe text encoding used when sending/receiving messages to the server.\ncache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\ncached and only fetched from the server once. If `False`, the tools list will be\nfetched from the server on each call to `list_tools()`. The cache can be\ninvalidated by calling `invalidate_tools_cache()`. You should set this to `True`\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nname: A readable name for the server. If not provided, we'll create one from the\ncommand.\n\"\"\"\nsuper\n()\n.\n__init__\n(\ncache_tools_list\n)\nself\n.\nparams\n=\nStdioServerParameters\n(\ncommand\n=\nparams\n[\n\"command\"\n],\nargs\n=\nparams\n.\nget\n(\n\"args\"\n,\n[]),\nenv\n=\nparams\n.\nget\n(\n\"env\"\n),\ncwd\n=\nparams\n.\nget\n(\n\"cwd\"\n),\nencoding\n=\nparams\n.\nget\n(\n\"encoding\"\n,\n\"utf-8\"\n),\nencoding_error_handler\n=\nparams\n.\nget\n(\n\"encoding_error_handler\"\n,\n\"strict\"\n),\n)\nself\n.\n_name\n=\nname\nor\nf\n\"stdio:\n{\nself\n.\nparams\n.\ncommand\n}\n\"\ncreate_streams\ncreate_streams\n()\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]\nCreate the streams for the server.\nSource code in\nsrc/agents/mcp/server.py\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\ndef\ncreate_streams\n(\nself\n,\n)\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]:\n\"\"\"Create the streams for the server.\"\"\"\nreturn\nstdio_client\n(\nself\n.\nparams\n)\nconnect\nasync\nconnect\n()\nConnect to the server.\nSource code in\nsrc/agents/mcp/server.py\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\nasync\ndef\nconnect\n(\nself\n):\n\"\"\"Connect to the server.\"\"\"\ntry\n:\ntransport\n=\nawait\nself\n.\nexit_stack\n.\nenter_async_context\n(\nself\n.\ncreate_streams\n())\nread\n,\nwrite\n=\ntransport\nsession\n=\nawait\nself\n.\nexit_stack\n.\nenter_async_context\n(\nClientSession\n(\nread\n,\nwrite\n))\nawait\nsession\n.\ninitialize\n()\nself\n.\nsession\n=\nsession\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error initializing MCP server:\n{\ne\n}\n\"\n)\nawait\nself\n.\ncleanup\n()\nraise\ncleanup\nasync\ncleanup\n()\nCleanup the server.\nSource code in\nsrc/agents/mcp/server.py\n135\n136\n137\n138\n139\n140\n141\n142\nasync\ndef\ncleanup\n(\nself\n):\n\"\"\"Cleanup the server.\"\"\"\nasync\nwith\nself\n.\n_cleanup_lock\n:\ntry\n:\nawait\nself\n.\nexit_stack\n.\naclose\n()\nself\n.\nsession\n=\nNone\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error cleaning up server:\n{\ne\n}\n\"\n)\nlist_tools\nasync\nlist_tools\n()\n->\nlist\n[\nTool\n]\nList the tools available on the server.\nSource code in\nsrc/agents/mcp/server.py\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\nasync\ndef\nlist_tools\n(\nself\n)\n->\nlist\n[\nMCPTool\n]:\n\"\"\"List the tools available on the server.\"\"\"\nif\nnot\nself\n.\nsession\n:\nraise\nUserError\n(\n\"Server not initialized. Make sure you call `connect()` first.\"\n)\n# Return from cache if caching is enabled, we have tools, and the cache is not dirty\nif\nself\n.\ncache_tools_list\nand\nnot\nself\n.\n_cache_dirty\nand\nself\n.\n_tools_list\n:\nreturn\nself\n.\n_tools_list\n# Reset the cache dirty to False\nself\n.\n_cache_dirty\n=\nFalse\n# Fetch the tools from the server\nself\n.\n_tools_list\n=\n(\nawait\nself\n.\nsession\n.\nlist_tools\n())\n.\ntools\nreturn\nself\n.\n_tools_list\ncall_tool\nasync\ncall_tool\n(\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\nInvoke a tool on the server.\nSource code in\nsrc/agents/mcp/server.py\n128\n129\n130\n131\n132\n133\nasync\ndef\ncall_tool\n(\nself\n,\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\n:\n\"\"\"Invoke a tool on the server.\"\"\"\nif\nnot\nself\n.\nsession\n:\nraise\nUserError\n(\n\"Server not initialized. Make sure you call `connect()` first.\"\n)\nreturn\nawait\nself\n.\nsession\n.\ncall_tool\n(\ntool_name\n,\narguments\n)\ninvalidate_tools_cache\ninvalidate_tools_cache\n()\nInvalidate the tools cache.\nSource code in\nsrc/agents/mcp/server.py\n95\n96\n97\ndef\ninvalidate_tools_cache\n(\nself\n):\n\"\"\"Invalidate the tools cache.\"\"\"\nself\n.\n_cache_dirty\n=\nTrue\nMCPServerSseParams\nBases:\nTypedDict\nMirrors the params in\nmcp.client.sse.sse_client\n.\nSource code in\nsrc/agents/mcp/server.py\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\nclass\nMCPServerSseParams\n(\nTypedDict\n):\n\"\"\"Mirrors the params in`mcp.client.sse.sse_client`.\"\"\"\nurl\n:\nstr\n\"\"\"The URL of the server.\"\"\"\nheaders\n:\nNotRequired\n[\ndict\n[\nstr\n,\nstr\n]]\n\"\"\"The headers to send to the server.\"\"\"\ntimeout\n:\nNotRequired\n[\nfloat\n]\n\"\"\"The timeout for the HTTP request. Defaults to 5 seconds.\"\"\"\nsse_read_timeout\n:\nNotRequired\n[\nfloat\n]\n\"\"\"The timeout for the SSE connection, in seconds. Defaults to 5 minutes.\"\"\"\nurl\ninstance-attribute\nurl\n:\nstr\nThe URL of the server.\nheaders\ninstance-attribute\nheaders\n:\nNotRequired\n[\ndict\n[\nstr\n,\nstr\n]]\nThe headers to send to the server.\ntimeout\ninstance-attribute\ntimeout\n:\nNotRequired\n[\nfloat\n]\nThe timeout for the HTTP request. Defaults to 5 seconds.\nsse_read_timeout\ninstance-attribute\nsse_read_timeout\n:\nNotRequired\n[\nfloat\n]\nThe timeout for the SSE connection, in seconds. Defaults to 5 minutes.\nMCPServerSse\nBases:\n_MCPServerWithClientSession\nMCP server implementation that uses the HTTP with SSE transport. See the [spec]\n(https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse)\nfor details.\nSource code in\nsrc/agents/mcp/server.py\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\nclass\nMCPServerSse\n(\n_MCPServerWithClientSession\n):\n\"\"\"MCP server implementation that uses the HTTP with SSE transport. See the [spec]\n(https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse)\nfor details.\n\"\"\"\ndef\n__init__\n(\nself\n,\nparams\n:\nMCPServerSseParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new MCP server based on the HTTP with SSE transport.\nArgs:\nparams: The params that configure the server. This includes the URL of the server,\nthe headers to send to the server, the timeout for the HTTP request, and the\ntimeout for the SSE connection.\ncache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\ncached and only fetched from the server once. If `False`, the tools list will be\nfetched from the server on each call to `list_tools()`. The cache can be\ninvalidated by calling `invalidate_tools_cache()`. You should set this to `True`\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nname: A readable name for the server. If not provided, we'll create one from the\nURL.\n\"\"\"\nsuper\n()\n.\n__init__\n(\ncache_tools_list\n)\nself\n.\nparams\n=\nparams\nself\n.\n_name\n=\nname\nor\nf\n\"sse:\n{\nself\n.\nparams\n[\n'url'\n]\n}\n\"\ndef\ncreate_streams\n(\nself\n,\n)\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]:\n\"\"\"Create the streams for the server.\"\"\"\nreturn\nsse_client\n(\nurl\n=\nself\n.\nparams\n[\n\"url\"\n],\nheaders\n=\nself\n.\nparams\n.\nget\n(\n\"headers\"\n,\nNone\n),\ntimeout\n=\nself\n.\nparams\n.\nget\n(\n\"timeout\"\n,\n5\n),\nsse_read_timeout\n=\nself\n.\nparams\n.\nget\n(\n\"sse_read_timeout\"\n,\n60\n*\n5\n),\n)\n@property\ndef\nname\n(\nself\n)\n->\nstr\n:\n\"\"\"A readable name for the server.\"\"\"\nreturn\nself\n.\n_name\nname\nproperty\nname\n:\nstr\nA readable name for the server.\n__init__\n__init__\n(\nparams\n:\nMCPServerSseParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n)\nCreate a new MCP server based on the HTTP with SSE transport.\nParameters:\nName\nType\nDescription\nDefault\nparams\nMCPServerSseParams\nThe params that configure the server. This includes the URL of the server,\nthe headers to send to the server, the timeout for the HTTP request, and the\ntimeout for the SSE connection.\nrequired\ncache_tools_list\nbool\nWhether to cache the tools list. If\nTrue\n, the tools list will be\ncached and only fetched from the server once. If\nFalse\n, the tools list will be\nfetched from the server on each call to\nlist_tools()\n. The cache can be\ninvalidated by calling\ninvalidate_tools_cache()\n. You should set this to\nTrue\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nFalse\nname\nstr\n| None\nA readable name for the server. If not provided, we'll create one from the\nURL.\nNone\nSource code in\nsrc/agents/mcp/server.py\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\ndef\n__init__\n(\nself\n,\nparams\n:\nMCPServerSseParams\n,\ncache_tools_list\n:\nbool\n=\nFalse\n,\nname\n:\nstr\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new MCP server based on the HTTP with SSE transport.\nArgs:\nparams: The params that configure the server. This includes the URL of the server,\nthe headers to send to the server, the timeout for the HTTP request, and the\ntimeout for the SSE connection.\ncache_tools_list: Whether to cache the tools list. If `True`, the tools list will be\ncached and only fetched from the server once. If `False`, the tools list will be\nfetched from the server on each call to `list_tools()`. The cache can be\ninvalidated by calling `invalidate_tools_cache()`. You should set this to `True`\nif you know the server will not change its tools list, because it can drastically\nimprove latency (by avoiding a round-trip to the server every time).\nname: A readable name for the server. If not provided, we'll create one from the\nURL.\n\"\"\"\nsuper\n()\n.\n__init__\n(\ncache_tools_list\n)\nself\n.\nparams\n=\nparams\nself\n.\n_name\n=\nname\nor\nf\n\"sse:\n{\nself\n.\nparams\n[\n'url'\n]\n}\n\"\ncreate_streams\ncreate_streams\n()\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]\nCreate the streams for the server.\nSource code in\nsrc/agents/mcp/server.py\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\ndef\ncreate_streams\n(\nself\n,\n)\n->\nAbstractAsyncContextManager\n[\ntuple\n[\nMemoryObjectReceiveStream\n[\nJSONRPCMessage\n|\nException\n],\nMemoryObjectSendStream\n[\nJSONRPCMessage\n],\n]\n]:\n\"\"\"Create the streams for the server.\"\"\"\nreturn\nsse_client\n(\nurl\n=\nself\n.\nparams\n[\n\"url\"\n],\nheaders\n=\nself\n.\nparams\n.\nget\n(\n\"headers\"\n,\nNone\n),\ntimeout\n=\nself\n.\nparams\n.\nget\n(\n\"timeout\"\n,\n5\n),\nsse_read_timeout\n=\nself\n.\nparams\n.\nget\n(\n\"sse_read_timeout\"\n,\n60\n*\n5\n),\n)\nconnect\nasync\nconnect\n()\nConnect to the server.\nSource code in\nsrc/agents/mcp/server.py\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\nasync\ndef\nconnect\n(\nself\n):\n\"\"\"Connect to the server.\"\"\"\ntry\n:\ntransport\n=\nawait\nself\n.\nexit_stack\n.\nenter_async_context\n(\nself\n.\ncreate_streams\n())\nread\n,\nwrite\n=\ntransport\nsession\n=\nawait\nself\n.\nexit_stack\n.\nenter_async_context\n(\nClientSession\n(\nread\n,\nwrite\n))\nawait\nsession\n.\ninitialize\n()\nself\n.\nsession\n=\nsession\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error initializing MCP server:\n{\ne\n}\n\"\n)\nawait\nself\n.\ncleanup\n()\nraise\ncleanup\nasync\ncleanup\n()\nCleanup the server.\nSource code in\nsrc/agents/mcp/server.py\n135\n136\n137\n138\n139\n140\n141\n142\nasync\ndef\ncleanup\n(\nself\n):\n\"\"\"Cleanup the server.\"\"\"\nasync\nwith\nself\n.\n_cleanup_lock\n:\ntry\n:\nawait\nself\n.\nexit_stack\n.\naclose\n()\nself\n.\nsession\n=\nNone\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error cleaning up server:\n{\ne\n}\n\"\n)\nlist_tools\nasync\nlist_tools\n()\n->\nlist\n[\nTool\n]\nList the tools available on the server.\nSource code in\nsrc/agents/mcp/server.py\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\nasync\ndef\nlist_tools\n(\nself\n)\n->\nlist\n[\nMCPTool\n]:\n\"\"\"List the tools available on the server.\"\"\"\nif\nnot\nself\n.\nsession\n:\nraise\nUserError\n(\n\"Server not initialized. Make sure you call `connect()` first.\"\n)\n# Return from cache if caching is enabled, we have tools, and the cache is not dirty\nif\nself\n.\ncache_tools_list\nand\nnot\nself\n.\n_cache_dirty\nand\nself\n.\n_tools_list\n:\nreturn\nself\n.\n_tools_list\n# Reset the cache dirty to False\nself\n.\n_cache_dirty\n=\nFalse\n# Fetch the tools from the server\nself\n.\n_tools_list\n=\n(\nawait\nself\n.\nsession\n.\nlist_tools\n())\n.\ntools\nreturn\nself\n.\n_tools_list\ncall_tool\nasync\ncall_tool\n(\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\nInvoke a tool on the server.\nSource code in\nsrc/agents/mcp/server.py\n128\n129\n130\n131\n132\n133\nasync\ndef\ncall_tool\n(\nself\n,\ntool_name\n:\nstr\n,\narguments\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n)\n->\nCallToolResult\n:\n\"\"\"Invoke a tool on the server.\"\"\"\nif\nnot\nself\n.\nsession\n:\nraise\nUserError\n(\n\"Server not initialized. Make sure you call `connect()` first.\"\n)\nreturn\nawait\nself\n.\nsession\n.\ncall_tool\n(\ntool_name\n,\narguments\n)\ninvalidate_tools_cache\ninvalidate_tools_cache\n()\nInvalidate the tools cache.\nSource code in\nsrc/agents/mcp/server.py\n95\n96\n97\ndef\ninvalidate_tools_cache\n(\nself\n):\n\"\"\"Invalidate the tools cache.\"\"\"\nself\n.\n_cache_dirty\n=\nTrue",
  "MCP Util": "MCP Util\nMCPUtil\nSet of utilities for interop between MCP and Agents SDK tools.\nSource code in\nsrc/agents/mcp/util.py\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\nclass\nMCPUtil\n:\n\"\"\"Set of utilities for interop between MCP and Agents SDK tools.\"\"\"\n@classmethod\nasync\ndef\nget_all_function_tools\n(\ncls\n,\nservers\n:\nlist\n[\n\"MCPServer\"\n],\nconvert_schemas_to_strict\n:\nbool\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Get all function tools from a list of MCP servers.\"\"\"\ntools\n=\n[]\ntool_names\n:\nset\n[\nstr\n]\n=\nset\n()\nfor\nserver\nin\nservers\n:\nserver_tools\n=\nawait\ncls\n.\nget_function_tools\n(\nserver\n,\nconvert_schemas_to_strict\n)\nserver_tool_names\n=\n{\ntool\n.\nname\nfor\ntool\nin\nserver_tools\n}\nif\nlen\n(\nserver_tool_names\n&\ntool_names\n)\n>\n0\n:\nraise\nUserError\n(\nf\n\"Duplicate tool names found across MCP servers: \"\nf\n\"\n{\nserver_tool_names\n&\ntool_names\n}\n\"\n)\ntool_names\n.\nupdate\n(\nserver_tool_names\n)\ntools\n.\nextend\n(\nserver_tools\n)\nreturn\ntools\n@classmethod\nasync\ndef\nget_function_tools\n(\ncls\n,\nserver\n:\n\"MCPServer\"\n,\nconvert_schemas_to_strict\n:\nbool\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Get all function tools from a single MCP server.\"\"\"\nwith\nmcp_tools_span\n(\nserver\n=\nserver\n.\nname\n)\nas\nspan\n:\ntools\n=\nawait\nserver\n.\nlist_tools\n()\nspan\n.\nspan_data\n.\nresult\n=\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n]\nreturn\n[\ncls\n.\nto_function_tool\n(\ntool\n,\nserver\n,\nconvert_schemas_to_strict\n)\nfor\ntool\nin\ntools\n]\n@classmethod\ndef\nto_function_tool\n(\ncls\n,\ntool\n:\n\"MCPTool\"\n,\nserver\n:\n\"MCPServer\"\n,\nconvert_schemas_to_strict\n:\nbool\n)\n->\nFunctionTool\n:\n\"\"\"Convert an MCP tool to an Agents SDK function tool.\"\"\"\ninvoke_func\n=\nfunctools\n.\npartial\n(\ncls\n.\ninvoke_mcp_tool\n,\nserver\n,\ntool\n)\nschema\n,\nis_strict\n=\ntool\n.\ninputSchema\n,\nFalse\n# MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\nif\n\"properties\"\nnot\nin\nschema\n:\nschema\n[\n\"properties\"\n]\n=\n{}\nif\nconvert_schemas_to_strict\n:\ntry\n:\nschema\n=\nensure_strict_json_schema\n(\nschema\n)\nis_strict\n=\nTrue\nexcept\nException\nas\ne\n:\nlogger\n.\ninfo\n(\nf\n\"Error converting MCP schema to strict mode:\n{\ne\n}\n\"\n)\nreturn\nFunctionTool\n(\nname\n=\ntool\n.\nname\n,\ndescription\n=\ntool\n.\ndescription\nor\n\"\"\n,\nparams_json_schema\n=\nschema\n,\non_invoke_tool\n=\ninvoke_func\n,\nstrict_json_schema\n=\nis_strict\n,\n)\n@classmethod\nasync\ndef\ninvoke_mcp_tool\n(\ncls\n,\nserver\n:\n\"MCPServer\"\n,\ntool\n:\n\"MCPTool\"\n,\ncontext\n:\nRunContextWrapper\n[\nAny\n],\ninput_json\n:\nstr\n)\n->\nstr\n:\n\"\"\"Invoke an MCP tool and return the result as a string.\"\"\"\ntry\n:\njson_data\n:\ndict\n[\nstr\n,\nAny\n]\n=\njson\n.\nloads\n(\ninput_json\n)\nif\ninput_json\nelse\n{}\nexcept\nException\nas\ne\n:\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n:\n{\ninput_json\n}\n\"\n)\nraise\nModelBehaviorError\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n:\n{\ninput_json\n}\n\"\n)\nfrom\ne\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking MCP tool\n{\ntool\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking MCP tool\n{\ntool\n.\nname\n}\nwith input\n{\ninput_json\n}\n\"\n)\ntry\n:\nresult\n=\nawait\nserver\n.\ncall_tool\n(\ntool\n.\nname\n,\njson_data\n)\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error invoking MCP tool\n{\ntool\n.\nname\n}\n:\n{\ne\n}\n\"\n)\nraise\nAgentsException\n(\nf\n\"Error invoking MCP tool\n{\ntool\n.\nname\n}\n:\n{\ne\n}\n\"\n)\nfrom\ne\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"MCP tool\n{\ntool\n.\nname\n}\ncompleted.\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"MCP tool\n{\ntool\n.\nname\n}\nreturned\n{\nresult\n}\n\"\n)\n# The MCP tool result is a list of content items, whereas OpenAI tool outputs are a single\n# string. We'll try to convert.\nif\nlen\n(\nresult\n.\ncontent\n)\n==\n1\n:\ntool_output\n=\nresult\n.\ncontent\n[\n0\n]\n.\nmodel_dump_json\n()\nelif\nlen\n(\nresult\n.\ncontent\n)\n>\n1\n:\ntool_output\n=\njson\n.\ndumps\n([\nitem\n.\nmodel_dump\n()\nfor\nitem\nin\nresult\n.\ncontent\n])\nelse\n:\nlogger\n.\nerror\n(\nf\n\"Errored MCP tool result:\n{\nresult\n}\n\"\n)\ntool_output\n=\n\"Error running tool.\"\ncurrent_span\n=\nget_current_span\n()\nif\ncurrent_span\n:\nif\nisinstance\n(\ncurrent_span\n.\nspan_data\n,\nFunctionSpanData\n):\ncurrent_span\n.\nspan_data\n.\noutput\n=\ntool_output\ncurrent_span\n.\nspan_data\n.\nmcp_data\n=\n{\n\"server\"\n:\nserver\n.\nname\n,\n}\nelse\n:\nlogger\n.\nwarning\n(\nf\n\"Current span is not a FunctionSpanData, skipping tool output:\n{\ncurrent_span\n}\n\"\n)\nreturn\ntool_output\nget_all_function_tools\nasync\nclassmethod\nget_all_function_tools\n(\nservers\n:\nlist\n[\nMCPServer\n],\nconvert_schemas_to_strict\n:\nbool\n,\n)\n->\nlist\n[\nTool\n]\nGet all function tools from a list of MCP servers.\nSource code in\nsrc/agents/mcp/util.py\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n@classmethod\nasync\ndef\nget_all_function_tools\n(\ncls\n,\nservers\n:\nlist\n[\n\"MCPServer\"\n],\nconvert_schemas_to_strict\n:\nbool\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Get all function tools from a list of MCP servers.\"\"\"\ntools\n=\n[]\ntool_names\n:\nset\n[\nstr\n]\n=\nset\n()\nfor\nserver\nin\nservers\n:\nserver_tools\n=\nawait\ncls\n.\nget_function_tools\n(\nserver\n,\nconvert_schemas_to_strict\n)\nserver_tool_names\n=\n{\ntool\n.\nname\nfor\ntool\nin\nserver_tools\n}\nif\nlen\n(\nserver_tool_names\n&\ntool_names\n)\n>\n0\n:\nraise\nUserError\n(\nf\n\"Duplicate tool names found across MCP servers: \"\nf\n\"\n{\nserver_tool_names\n&\ntool_names\n}\n\"\n)\ntool_names\n.\nupdate\n(\nserver_tool_names\n)\ntools\n.\nextend\n(\nserver_tools\n)\nreturn\ntools\nget_function_tools\nasync\nclassmethod\nget_function_tools\n(\nserver\n:\nMCPServer\n,\nconvert_schemas_to_strict\n:\nbool\n)\n->\nlist\n[\nTool\n]\nGet all function tools from a single MCP server.\nSource code in\nsrc/agents/mcp/util.py\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n@classmethod\nasync\ndef\nget_function_tools\n(\ncls\n,\nserver\n:\n\"MCPServer\"\n,\nconvert_schemas_to_strict\n:\nbool\n)\n->\nlist\n[\nTool\n]:\n\"\"\"Get all function tools from a single MCP server.\"\"\"\nwith\nmcp_tools_span\n(\nserver\n=\nserver\n.\nname\n)\nas\nspan\n:\ntools\n=\nawait\nserver\n.\nlist_tools\n()\nspan\n.\nspan_data\n.\nresult\n=\n[\ntool\n.\nname\nfor\ntool\nin\ntools\n]\nreturn\n[\ncls\n.\nto_function_tool\n(\ntool\n,\nserver\n,\nconvert_schemas_to_strict\n)\nfor\ntool\nin\ntools\n]\nto_function_tool\nclassmethod\nto_function_tool\n(\ntool\n:\nTool\n,\nserver\n:\nMCPServer\n,\nconvert_schemas_to_strict\n:\nbool\n,\n)\n->\nFunctionTool\nConvert an MCP tool to an Agents SDK function tool.\nSource code in\nsrc/agents/mcp/util.py\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n@classmethod\ndef\nto_function_tool\n(\ncls\n,\ntool\n:\n\"MCPTool\"\n,\nserver\n:\n\"MCPServer\"\n,\nconvert_schemas_to_strict\n:\nbool\n)\n->\nFunctionTool\n:\n\"\"\"Convert an MCP tool to an Agents SDK function tool.\"\"\"\ninvoke_func\n=\nfunctools\n.\npartial\n(\ncls\n.\ninvoke_mcp_tool\n,\nserver\n,\ntool\n)\nschema\n,\nis_strict\n=\ntool\n.\ninputSchema\n,\nFalse\n# MCP spec doesn't require the inputSchema to have `properties`, but OpenAI spec does.\nif\n\"properties\"\nnot\nin\nschema\n:\nschema\n[\n\"properties\"\n]\n=\n{}\nif\nconvert_schemas_to_strict\n:\ntry\n:\nschema\n=\nensure_strict_json_schema\n(\nschema\n)\nis_strict\n=\nTrue\nexcept\nException\nas\ne\n:\nlogger\n.\ninfo\n(\nf\n\"Error converting MCP schema to strict mode:\n{\ne\n}\n\"\n)\nreturn\nFunctionTool\n(\nname\n=\ntool\n.\nname\n,\ndescription\n=\ntool\n.\ndescription\nor\n\"\"\n,\nparams_json_schema\n=\nschema\n,\non_invoke_tool\n=\ninvoke_func\n,\nstrict_json_schema\n=\nis_strict\n,\n)\ninvoke_mcp_tool\nasync\nclassmethod\ninvoke_mcp_tool\n(\nserver\n:\nMCPServer\n,\ntool\n:\nTool\n,\ncontext\n:\nRunContextWrapper\n[\nAny\n],\ninput_json\n:\nstr\n,\n)\n->\nstr\nInvoke an MCP tool and return the result as a string.\nSource code in\nsrc/agents/mcp/util.py\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n@classmethod\nasync\ndef\ninvoke_mcp_tool\n(\ncls\n,\nserver\n:\n\"MCPServer\"\n,\ntool\n:\n\"MCPTool\"\n,\ncontext\n:\nRunContextWrapper\n[\nAny\n],\ninput_json\n:\nstr\n)\n->\nstr\n:\n\"\"\"Invoke an MCP tool and return the result as a string.\"\"\"\ntry\n:\njson_data\n:\ndict\n[\nstr\n,\nAny\n]\n=\njson\n.\nloads\n(\ninput_json\n)\nif\ninput_json\nelse\n{}\nexcept\nException\nas\ne\n:\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n:\n{\ninput_json\n}\n\"\n)\nraise\nModelBehaviorError\n(\nf\n\"Invalid JSON input for tool\n{\ntool\n.\nname\n}\n:\n{\ninput_json\n}\n\"\n)\nfrom\ne\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking MCP tool\n{\ntool\n.\nname\n}\n\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Invoking MCP tool\n{\ntool\n.\nname\n}\nwith input\n{\ninput_json\n}\n\"\n)\ntry\n:\nresult\n=\nawait\nserver\n.\ncall_tool\n(\ntool\n.\nname\n,\njson_data\n)\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error invoking MCP tool\n{\ntool\n.\nname\n}\n:\n{\ne\n}\n\"\n)\nraise\nAgentsException\n(\nf\n\"Error invoking MCP tool\n{\ntool\n.\nname\n}\n:\n{\ne\n}\n\"\n)\nfrom\ne\nif\n_debug\n.\nDONT_LOG_TOOL_DATA\n:\nlogger\n.\ndebug\n(\nf\n\"MCP tool\n{\ntool\n.\nname\n}\ncompleted.\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"MCP tool\n{\ntool\n.\nname\n}\nreturned\n{\nresult\n}\n\"\n)\n# The MCP tool result is a list of content items, whereas OpenAI tool outputs are a single\n# string. We'll try to convert.\nif\nlen\n(\nresult\n.\ncontent\n)\n==\n1\n:\ntool_output\n=\nresult\n.\ncontent\n[\n0\n]\n.\nmodel_dump_json\n()\nelif\nlen\n(\nresult\n.\ncontent\n)\n>\n1\n:\ntool_output\n=\njson\n.\ndumps\n([\nitem\n.\nmodel_dump\n()\nfor\nitem\nin\nresult\n.\ncontent\n])\nelse\n:\nlogger\n.\nerror\n(\nf\n\"Errored MCP tool result:\n{\nresult\n}\n\"\n)\ntool_output\n=\n\"Error running tool.\"\ncurrent_span\n=\nget_current_span\n()\nif\ncurrent_span\n:\nif\nisinstance\n(\ncurrent_span\n.\nspan_data\n,\nFunctionSpanData\n):\ncurrent_span\n.\nspan_data\n.\noutput\n=\ntool_output\ncurrent_span\n.\nspan_data\n.\nmcp_data\n=\n{\n\"server\"\n:\nserver\n.\nname\n,\n}\nelse\n:\nlogger\n.\nwarning\n(\nf\n\"Current span is not a FunctionSpanData, skipping tool output:\n{\ncurrent_span\n}\n\"\n)\nreturn\ntool_output",
  "Tracing module": "Tracing module\nTracingProcessor\nBases:\nABC\nInterface for processing spans.\nSource code in\nsrc/agents/tracing/processor_interface.py\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\nclass\nTracingProcessor\n(\nabc\n.\nABC\n):\n\"\"\"Interface for processing spans.\"\"\"\n@abc\n.\nabstractmethod\ndef\non_trace_start\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is started.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_trace_end\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is finished.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_span_start\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is started.\nArgs:\nspan: The span that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_span_end\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is finished. Should not block or raise exceptions.\nArgs:\nspan: The span that finished.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"Called when the application stops.\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nforce_flush\n(\nself\n)\n->\nNone\n:\n\"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\npass\non_trace_start\nabstractmethod\non_trace_start\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is started.\nParameters:\nName\nType\nDescription\nDefault\ntrace\nTrace\nThe trace that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n12\n13\n14\n15\n16\n17\n18\n19\n@abc\n.\nabstractmethod\ndef\non_trace_start\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is started.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\non_trace_end\nabstractmethod\non_trace_end\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is finished.\nParameters:\nName\nType\nDescription\nDefault\ntrace\nTrace\nThe trace that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n21\n22\n23\n24\n25\n26\n27\n28\n@abc\n.\nabstractmethod\ndef\non_trace_end\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is finished.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\non_span_start\nabstractmethod\non_span_start\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is started.\nParameters:\nName\nType\nDescription\nDefault\nspan\nSpan\n[\nAny\n]\nThe span that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n30\n31\n32\n33\n34\n35\n36\n37\n@abc\n.\nabstractmethod\ndef\non_span_start\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is started.\nArgs:\nspan: The span that started.\n\"\"\"\npass\non_span_end\nabstractmethod\non_span_end\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is finished. Should not block or raise exceptions.\nParameters:\nName\nType\nDescription\nDefault\nspan\nSpan\n[\nAny\n]\nThe span that finished.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n39\n40\n41\n42\n43\n44\n45\n46\n@abc\n.\nabstractmethod\ndef\non_span_end\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is finished. Should not block or raise exceptions.\nArgs:\nspan: The span that finished.\n\"\"\"\npass\nshutdown\nabstractmethod\nshutdown\n()\n->\nNone\nCalled when the application stops.\nSource code in\nsrc/agents/tracing/processor_interface.py\n48\n49\n50\n51\n@abc\n.\nabstractmethod\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"Called when the application stops.\"\"\"\npass\nforce_flush\nabstractmethod\nforce_flush\n()\n->\nNone\nForces an immediate flush of all queued spans/traces.\nSource code in\nsrc/agents/tracing/processor_interface.py\n53\n54\n55\n56\n@abc\n.\nabstractmethod\ndef\nforce_flush\n(\nself\n)\n->\nNone\n:\n\"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\npass\nAgentSpanData\nBases:\nSpanData\nRepresents an Agent Span in the trace.\nIncludes name, handoffs, tools, and output type.\nSource code in\nsrc/agents/tracing/span_data.py\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\nclass\nAgentSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents an Agent Span in the trace.\nIncludes name, handoffs, tools, and output type.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"handoffs\"\n,\n\"tools\"\n,\n\"output_type\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\nname\n=\nname\nself\n.\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nhandoffs\nself\n.\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\ntools\nself\n.\noutput_type\n:\nstr\n|\nNone\n=\noutput_type\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"agent\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"handoffs\"\n:\nself\n.\nhandoffs\n,\n\"tools\"\n:\nself\n.\ntools\n,\n\"output_type\"\n:\nself\n.\noutput_type\n,\n}\nCustomSpanData\nBases:\nSpanData\nRepresents a Custom Span in the trace.\nIncludes name and data property bag.\nSource code in\nsrc/agents/tracing/span_data.py\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\nclass\nCustomSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Custom Span in the trace.\nIncludes name and data property bag.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"data\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]):\nself\n.\nname\n=\nname\nself\n.\ndata\n=\ndata\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"custom\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"data\"\n:\nself\n.\ndata\n,\n}\nFunctionSpanData\nBases:\nSpanData\nRepresents a Function Span in the trace.\nIncludes input, output and MCP data (if applicable).\nSource code in\nsrc/agents/tracing/span_data.py\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nclass\nFunctionSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Function Span in the trace.\nIncludes input, output and MCP data (if applicable).\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"input\"\n,\n\"output\"\n,\n\"mcp_data\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n,\noutput\n:\nAny\n|\nNone\n,\nmcp_data\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\nname\n=\nname\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\nmcp_data\n=\nmcp_data\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"function\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\nstr\n(\nself\n.\noutput\n)\nif\nself\n.\noutput\nelse\nNone\n,\n\"mcp_data\"\n:\nself\n.\nmcp_data\n,\n}\nGenerationSpanData\nBases:\nSpanData\nRepresents a Generation Span in the trace.\nIncludes input, output, model, model configuration, and usage.\nSource code in\nsrc/agents/tracing/span_data.py\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\nclass\nGenerationSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Generation Span in the trace.\nIncludes input, output, model, model configuration, and usage.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n\"usage\"\n,\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\nself\n.\nusage\n=\nusage\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"generation\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\nself\n.\noutput\n,\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n\"usage\"\n:\nself\n.\nusage\n,\n}\nGuardrailSpanData\nBases:\nSpanData\nRepresents a Guardrail Span in the trace.\nIncludes name and triggered status.\nSource code in\nsrc/agents/tracing/span_data.py\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\nclass\nGuardrailSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Guardrail Span in the trace.\nIncludes name and triggered status.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"triggered\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n):\nself\n.\nname\n=\nname\nself\n.\ntriggered\n=\ntriggered\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"guardrail\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"triggered\"\n:\nself\n.\ntriggered\n,\n}\nHandoffSpanData\nBases:\nSpanData\nRepresents a Handoff Span in the trace.\nIncludes source and destination agents.\nSource code in\nsrc/agents/tracing/span_data.py\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\nclass\nHandoffSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Handoff Span in the trace.\nIncludes source and destination agents.\n\"\"\"\n__slots__\n=\n(\n\"from_agent\"\n,\n\"to_agent\"\n)\ndef\n__init__\n(\nself\n,\nfrom_agent\n:\nstr\n|\nNone\n,\nto_agent\n:\nstr\n|\nNone\n):\nself\n.\nfrom_agent\n=\nfrom_agent\nself\n.\nto_agent\n=\nto_agent\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"handoff\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"from_agent\"\n:\nself\n.\nfrom_agent\n,\n\"to_agent\"\n:\nself\n.\nto_agent\n,\n}\nMCPListToolsSpanData\nBases:\nSpanData\nRepresents an MCP List Tools Span in the trace.\nIncludes server and result.\nSource code in\nsrc/agents/tracing/span_data.py\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\nclass\nMCPListToolsSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents an MCP List Tools Span in the trace.\nIncludes server and result.\n\"\"\"\n__slots__\n=\n(\n\"server\"\n,\n\"result\"\n,\n)\ndef\n__init__\n(\nself\n,\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n):\nself\n.\nserver\n=\nserver\nself\n.\nresult\n=\nresult\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"mcp_tools\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"server\"\n:\nself\n.\nserver\n,\n\"result\"\n:\nself\n.\nresult\n,\n}\nResponseSpanData\nBases:\nSpanData\nRepresents a Response Span in the trace.\nIncludes response and input.\nSource code in\nsrc/agents/tracing/span_data.py\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\nclass\nResponseSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Response Span in the trace.\nIncludes response and input.\n\"\"\"\n__slots__\n=\n(\n\"response\"\n,\n\"input\"\n)\ndef\n__init__\n(\nself\n,\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nResponseInputItemParam\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\n:\nself\n.\nresponse\n=\nresponse\n# This is not used by the OpenAI trace processors, but is useful for other tracing\n# processor implementations\nself\n.\ninput\n=\ninput\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"response\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"response_id\"\n:\nself\n.\nresponse\n.\nid\nif\nself\n.\nresponse\nelse\nNone\n,\n}\nSpanData\nBases:\nABC\nRepresents span data in the trace.\nSource code in\nsrc/agents/tracing/span_data.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nclass\nSpanData\n(\nabc\n.\nABC\n):\n\"\"\"\nRepresents span data in the trace.\n\"\"\"\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Export the span data as a dictionary.\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\ntype\n(\nself\n)\n->\nstr\n:\n\"\"\"Return the type of the span.\"\"\"\npass\ntype\nabstractmethod\nproperty\ntype\n:\nstr\nReturn the type of the span.\nexport\nabstractmethod\nexport\n()\n->\ndict\n[\nstr\n,\nAny\n]\nExport the span data as a dictionary.\nSource code in\nsrc/agents/tracing/span_data.py\n16\n17\n18\n19\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Export the span data as a dictionary.\"\"\"\npass\nSpeechGroupSpanData\nBases:\nSpanData\nRepresents a Speech Group Span in the trace.\nSource code in\nsrc/agents/tracing/span_data.py\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\nclass\nSpeechGroupSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Speech Group Span in the trace.\n\"\"\"\n__slots__\n=\n\"input\"\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"speech-group\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n}\nSpeechSpanData\nBases:\nSpanData\nRepresents a Speech Span in the trace.\nIncludes input, output, model, model configuration, and first content timestamp.\nSource code in\nsrc/agents/tracing/span_data.py\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\nclass\nSpeechSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Speech Span in the trace.\nIncludes input, output, model, model configuration, and first content timestamp.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n\"first_content_at\"\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\noutput_format\n=\noutput_format\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\nself\n.\nfirst_content_at\n=\nfirst_content_at\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"speech\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\n{\n\"data\"\n:\nself\n.\noutput\nor\n\"\"\n,\n\"format\"\n:\nself\n.\noutput_format\n,\n},\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n\"first_content_at\"\n:\nself\n.\nfirst_content_at\n,\n}\nTranscriptionSpanData\nBases:\nSpanData\nRepresents a Transcription Span in the trace.\nIncludes input, output, model, and model configuration.\nSource code in\nsrc/agents/tracing/span_data.py\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\nclass\nTranscriptionSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Transcription Span in the trace.\nIncludes input, output, model, and model configuration.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\ninput_format\n=\ninput_format\nself\n.\noutput\n=\noutput\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"transcription\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\n{\n\"data\"\n:\nself\n.\ninput\nor\n\"\"\n,\n\"format\"\n:\nself\n.\ninput_format\n,\n},\n\"output\"\n:\nself\n.\noutput\n,\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n}\nSpan\nBases:\nABC\n,\nGeneric\n[\nTSpanData\n]\nSource code in\nsrc/agents/tracing/spans.py\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nclass\nSpan\n(\nabc\n.\nABC\n,\nGeneric\n[\nTSpanData\n]):\n@property\n@abc\n.\nabstractmethod\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nspan_id\n(\nself\n)\n->\nstr\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nspan_data\n(\nself\n)\n->\nTSpanData\n:\npass\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the span.\nArgs:\nmark_as_current: If true, the span will be marked as the current span.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\n\"\"\"\nFinish the span.\nArgs:\nreset_current: If true, the span will be reset as the current span.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\n__enter__\n(\nself\n)\n->\nSpan\n[\nTSpanData\n]:\npass\n@abc\n.\nabstractmethod\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\npass\n@property\n@abc\n.\nabstractmethod\ndef\nparent_id\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\n@abc\n.\nabstractmethod\ndef\nset_error\n(\nself\n,\nerror\n:\nSpanError\n)\n->\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nerror\n(\nself\n)\n->\nSpanError\n|\nNone\n:\npass\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nstarted_at\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nended_at\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\nstart\nabstractmethod\nstart\n(\nmark_as_current\n:\nbool\n=\nFalse\n)\nStart the span.\nParameters:\nName\nType\nDescription\nDefault\nmark_as_current\nbool\nIf true, the span will be marked as the current span.\nFalse\nSource code in\nsrc/agents/tracing/spans.py\n39\n40\n41\n42\n43\n44\n45\n46\n47\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the span.\nArgs:\nmark_as_current: If true, the span will be marked as the current span.\n\"\"\"\npass\nfinish\nabstractmethod\nfinish\n(\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\nFinish the span.\nParameters:\nName\nType\nDescription\nDefault\nreset_current\nbool\nIf true, the span will be reset as the current span.\nFalse\nSource code in\nsrc/agents/tracing/spans.py\n49\n50\n51\n52\n53\n54\n55\n56\n57\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\n\"\"\"\nFinish the span.\nArgs:\nreset_current: If true, the span will be reset as the current span.\n\"\"\"\npass\nTrace\nA trace is the root level object that tracing creates. It represents a logical \"workflow\".\nSource code in\nsrc/agents/tracing/traces.py\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\nclass\nTrace\n:\n\"\"\"\nA trace is the root level object that tracing creates. It represents a logical \"workflow\".\n\"\"\"\n@abc\n.\nabstractmethod\ndef\n__enter__\n(\nself\n)\n->\nTrace\n:\npass\n@abc\n.\nabstractmethod\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\npass\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the trace.\nArgs:\nmark_as_current: If true, the trace will be marked as the current trace.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nFinish the trace.\nArgs:\nreset_current: If true, the trace will be reset as the current trace.\n\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\n\"\"\"\nThe trace ID.\n\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\nname\n(\nself\n)\n->\nstr\n:\n\"\"\"\nThe name of the workflow being traced.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\n\"\"\"\nExport the trace as a dictionary.\n\"\"\"\npass\ntrace_id\nabstractmethod\nproperty\ntrace_id\n:\nstr\nThe trace ID.\nname\nabstractmethod\nproperty\nname\n:\nstr\nThe name of the workflow being traced.\nstart\nabstractmethod\nstart\n(\nmark_as_current\n:\nbool\n=\nFalse\n)\nStart the trace.\nParameters:\nName\nType\nDescription\nDefault\nmark_as_current\nbool\nIf true, the trace will be marked as the current trace.\nFalse\nSource code in\nsrc/agents/tracing/traces.py\n26\n27\n28\n29\n30\n31\n32\n33\n34\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the trace.\nArgs:\nmark_as_current: If true, the trace will be marked as the current trace.\n\"\"\"\npass\nfinish\nabstractmethod\nfinish\n(\nreset_current\n:\nbool\n=\nFalse\n)\nFinish the trace.\nParameters:\nName\nType\nDescription\nDefault\nreset_current\nbool\nIf true, the trace will be reset as the current trace.\nFalse\nSource code in\nsrc/agents/tracing/traces.py\n36\n37\n38\n39\n40\n41\n42\n43\n44\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nFinish the trace.\nArgs:\nreset_current: If true, the trace will be reset as the current trace.\n\"\"\"\npass\nexport\nabstractmethod\nexport\n()\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nExport the trace as a dictionary.\nSource code in\nsrc/agents/tracing/traces.py\n62\n63\n64\n65\n66\n67\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\n\"\"\"\nExport the trace as a dictionary.\n\"\"\"\npass\nagent_span\nagent_span\n(\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nAgentSpanData\n]\nCreate a new agent span. The span will not be started automatically, you should either do\nwith agent_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the agent.\nrequired\nhandoffs\nlist\n[\nstr\n] | None\nOptional list of agent names to which this agent could hand off control.\nNone\ntools\nlist\n[\nstr\n] | None\nOptional list of tool names available to this agent.\nNone\noutput_type\nstr\n| None\nOptional name of the output type produced by the agent.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nAgentSpanData\n]\nThe newly created agent span.\nSource code in\nsrc/agents/tracing/create.py\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\ndef\nagent_span\n(\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nAgentSpanData\n]:\n\"\"\"Create a new agent span. The span will not be started automatically, you should either do\n`with agent_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the agent.\nhandoffs: Optional list of agent names to which this agent could hand off control.\ntools: Optional list of tool names available to this agent.\noutput_type: Optional name of the output type produced by the agent.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created agent span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nAgentSpanData\n(\nname\n=\nname\n,\nhandoffs\n=\nhandoffs\n,\ntools\n=\ntools\n,\noutput_type\n=\noutput_type\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ncustom_span\ncustom_span\n(\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nCustomSpanData\n]\nCreate a new custom span, to which you can add your own metadata. The span will not be\nstarted automatically, you should either do\nwith custom_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the custom span.\nrequired\ndata\ndict\n[\nstr\n,\nAny\n] | None\nArbitrary structured data to associate with the span.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nCustomSpanData\n]\nThe newly created custom span.\nSource code in\nsrc/agents/tracing/create.py\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\ndef\ncustom_span\n(\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nCustomSpanData\n]:\n\"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\nstarted automatically, you should either do `with custom_span() ...` or call\n`span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the custom span.\ndata: Arbitrary structured data to associate with the span.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created custom span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nCustomSpanData\n(\nname\n=\nname\n,\ndata\n=\ndata\nor\n{}),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nfunction_span\nfunction_span\n(\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nFunctionSpanData\n]\nCreate a new function span. The span will not be started automatically, you should either do\nwith function_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the function.\nrequired\ninput\nstr\n| None\nThe input to the function.\nNone\noutput\nstr\n| None\nThe output of the function.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nFunctionSpanData\n]\nThe newly created function span.\nSource code in\nsrc/agents/tracing/create.py\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\ndef\nfunction_span\n(\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nFunctionSpanData\n]:\n\"\"\"Create a new function span. The span will not be started automatically, you should either do\n`with function_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the function.\ninput: The input to the function.\noutput: The output of the function.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created function span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nFunctionSpanData\n(\nname\n=\nname\n,\ninput\n=\ninput\n,\noutput\n=\noutput\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ngeneration_span\ngeneration_span\n(\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGenerationSpanData\n]\nCreate a new generation span. The span will not be started automatically, you should either\ndo\nwith generation_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nThis span captures the details of a model generation, including the\ninput message sequence, any generated outputs, the model name and\nconfiguration, and usage data. If you only need to capture a model\nresponse identifier, use\nresponse_span()\ninstead.\nParameters:\nName\nType\nDescription\nDefault\ninput\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]] | None\nThe sequence of input messages sent to the model.\nNone\noutput\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]] | None\nThe sequence of output messages received from the model.\nNone\nmodel\nstr\n| None\nThe model identifier used for the generation.\nNone\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nusage\ndict\n[\nstr\n,\nAny\n] | None\nA dictionary of usage information (input tokens, output tokens, etc.).\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nGenerationSpanData\n]\nThe newly created generation span.\nSource code in\nsrc/agents/tracing/create.py\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\ndef\ngeneration_span\n(\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGenerationSpanData\n]:\n\"\"\"Create a new generation span. The span will not be started automatically, you should either\ndo `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\nThis span captures the details of a model generation, including the\ninput message sequence, any generated outputs, the model name and\nconfiguration, and usage data. If you only need to capture a model\nresponse identifier, use `response_span()` instead.\nArgs:\ninput: The sequence of input messages sent to the model.\noutput: The sequence of output messages received from the model.\nmodel: The model identifier used for the generation.\nmodel_config: The model configuration (hyperparameters) used.\nusage: A dictionary of usage information (input tokens, output tokens, etc.).\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created generation span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nGenerationSpanData\n(\ninput\n=\ninput\n,\noutput\n=\noutput\n,\nmodel\n=\nmodel\n,\nmodel_config\n=\nmodel_config\n,\nusage\n=\nusage\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nget_current_span\nget_current_span\n()\n->\nSpan\n[\nAny\n]\n|\nNone\nReturns the currently active span, if present.\nSource code in\nsrc/agents/tracing/create.py\n79\n80\n81\ndef\nget_current_span\n()\n->\nSpan\n[\nAny\n]\n|\nNone\n:\n\"\"\"Returns the currently active span, if present.\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\nget_current_span\n()\nget_current_trace\nget_current_trace\n()\n->\nTrace\n|\nNone\nReturns the currently active trace, if present.\nSource code in\nsrc/agents/tracing/create.py\n74\n75\n76\ndef\nget_current_trace\n()\n->\nTrace\n|\nNone\n:\n\"\"\"Returns the currently active trace, if present.\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\nget_current_trace\n()\nguardrail_span\nguardrail_span\n(\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGuardrailSpanData\n]\nCreate a new guardrail span. The span will not be started automatically, you should either\ndo\nwith guardrail_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the guardrail.\nrequired\ntriggered\nbool\nWhether the guardrail was triggered.\nFalse\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\ndef\nguardrail_span\n(\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGuardrailSpanData\n]:\n\"\"\"Create a new guardrail span. The span will not be started automatically, you should either\ndo `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the guardrail.\ntriggered: Whether the guardrail was triggered.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nGuardrailSpanData\n(\nname\n=\nname\n,\ntriggered\n=\ntriggered\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nhandoff_span\nhandoff_span\n(\nfrom_agent\n:\nstr\n|\nNone\n=\nNone\n,\nto_agent\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nHandoffSpanData\n]\nCreate a new handoff span. The span will not be started automatically, you should either do\nwith handoff_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nfrom_agent\nstr\n| None\nThe name of the agent that is handing off.\nNone\nto_agent\nstr\n| None\nThe name of the agent that is receiving the handoff.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nHandoffSpanData\n]\nThe newly created handoff span.\nSource code in\nsrc/agents/tracing/create.py\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\ndef\nhandoff_span\n(\nfrom_agent\n:\nstr\n|\nNone\n=\nNone\n,\nto_agent\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nHandoffSpanData\n]:\n\"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n`with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nfrom_agent: The name of the agent that is handing off.\nto_agent: The name of the agent that is receiving the handoff.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created handoff span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nHandoffSpanData\n(\nfrom_agent\n=\nfrom_agent\n,\nto_agent\n=\nto_agent\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nmcp_tools_span\nmcp_tools_span\n(\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nMCPListToolsSpanData\n]\nCreate a new MCP list tools span. The span will not be started automatically, you should\neither do\nwith mcp_tools_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nserver\nstr\n| None\nThe name of the MCP server.\nNone\nresult\nlist\n[\nstr\n] | None\nThe result of the MCP list tools call.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\ndef\nmcp_tools_span\n(\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nMCPListToolsSpanData\n]:\n\"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\neither do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nserver: The name of the MCP server.\nresult: The result of the MCP list tools call.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nMCPListToolsSpanData\n(\nserver\n=\nserver\n,\nresult\n=\nresult\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nresponse_span\nresponse_span\n(\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nResponseSpanData\n]\nCreate a new response span. The span will not be started automatically, you should either do\nwith response_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nresponse\nResponse\n| None\nThe OpenAI Response object.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\ndef\nresponse_span\n(\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nResponseSpanData\n]:\n\"\"\"Create a new response span. The span will not be started automatically, you should either do\n`with response_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nresponse: The OpenAI Response object.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nResponseSpanData\n(\nresponse\n=\nresponse\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nspeech_group_span\nspeech_group_span\n(\ninput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechGroupSpanData\n]\nCreate a new speech group span. The span will not be started automatically, you should\neither do\nwith speech_group_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\ninput\nstr\n| None\nThe input text used for the speech request.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\ndef\nspeech_group_span\n(\ninput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechGroupSpanData\n]:\n\"\"\"Create a new speech group span. The span will not be started automatically, you should\neither do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\ninput: The input text used for the speech request.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nSpeechGroupSpanData\n(\ninput\n=\ninput\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nspeech_span\nspeech_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechSpanData\n]\nCreate a new speech span. The span will not be started automatically, you should either do\nwith speech_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\n| None\nThe name of the model used for the text-to-speech.\nNone\ninput\nstr\n| None\nThe text input of the text-to-speech.\nNone\noutput\nstr\n| None\nThe audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\nNone\noutput_format\nstr\n| None\nThe format of the audio output (defaults to \"pcm\").\n'pcm'\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nfirst_content_at\nstr\n| None\nThe time of the first byte of the audio output.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\ndef\nspeech_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechSpanData\n]:\n\"\"\"Create a new speech span. The span will not be started automatically, you should either do\n`with speech_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nmodel: The name of the model used for the text-to-speech.\ninput: The text input of the text-to-speech.\noutput: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\noutput_format: The format of the audio output (defaults to \"pcm\").\nmodel_config: The model configuration (hyperparameters) used.\nfirst_content_at: The time of the first byte of the audio output.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nSpeechSpanData\n(\nmodel\n=\nmodel\n,\ninput\n=\ninput\n,\noutput\n=\noutput\n,\noutput_format\n=\noutput_format\n,\nmodel_config\n=\nmodel_config\n,\nfirst_content_at\n=\nfirst_content_at\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ntrace\ntrace\n(\nworkflow_name\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\nCreate a new trace. The trace will not be started automatically; you should either use\nit as a context manager (\nwith trace(...):\n) or call\ntrace.start()\n+\ntrace.finish()\nmanually.\nIn addition to the workflow name and optional grouping identifier, you can provide\nan arbitrary metadata dictionary to attach additional user-defined information to\nthe trace.\nParameters:\nName\nType\nDescription\nDefault\nworkflow_name\nstr\nThe name of the logical app or workflow. For example, you might provide\n\"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\nrequired\ntrace_id\nstr\n| None\nThe ID of the trace. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_trace_id()\nto generate a trace ID, to guarantee that IDs are\ncorrectly formatted.\nNone\ngroup_id\nstr\n| None\nOptional grouping identifier to link multiple traces from the same conversation\nor process. For instance, you might use a chat thread ID.\nNone\nmetadata\ndict\n[\nstr\n,\nAny\n] | None\nOptional dictionary of additional metadata to attach to the trace.\nNone\ndisabled\nbool\nIf True, we will return a Trace but the Trace will not be recorded. This will\nnot be checked if there's an existing trace and\neven_if_trace_running\nis True.\nFalse\nReturns:\nType\nDescription\nTrace\nThe newly created trace object.\nSource code in\nsrc/agents/tracing/create.py\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\ndef\ntrace\n(\nworkflow_name\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\n:\n\"\"\"\nCreate a new trace. The trace will not be started automatically; you should either use\nit as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\nmanually.\nIn addition to the workflow name and optional grouping identifier, you can provide\nan arbitrary metadata dictionary to attach additional user-defined information to\nthe trace.\nArgs:\nworkflow_name: The name of the logical app or workflow. For example, you might provide\n\"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\ntrace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\ncorrectly formatted.\ngroup_id: Optional grouping identifier to link multiple traces from the same conversation\nor process. For instance, you might use a chat thread ID.\nmetadata: Optional dictionary of additional metadata to attach to the trace.\ndisabled: If True, we will return a Trace but the Trace will not be recorded. This will\nnot be checked if there's an existing trace and `even_if_trace_running` is True.\nReturns:\nThe newly created trace object.\n\"\"\"\ncurrent_trace\n=\nGLOBAL_TRACE_PROVIDER\n.\nget_current_trace\n()\nif\ncurrent_trace\n:\nlogger\n.\nwarning\n(\n\"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n)\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_trace\n(\nname\n=\nworkflow_name\n,\ntrace_id\n=\ntrace_id\n,\ngroup_id\n=\ngroup_id\n,\nmetadata\n=\nmetadata\n,\ndisabled\n=\ndisabled\n,\n)\ntranscription_span\ntranscription_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTranscriptionSpanData\n]\nCreate a new transcription span. The span will not be started automatically, you should\neither do\nwith transcription_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\n| None\nThe name of the model used for the speech-to-text.\nNone\ninput\nstr\n| None\nThe audio input of the speech-to-text transcription, as a base64 encoded string of\naudio bytes.\nNone\ninput_format\nstr\n| None\nThe format of the audio input (defaults to \"pcm\").\n'pcm'\noutput\nstr\n| None\nThe output of the speech-to-text transcription.\nNone\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nTranscriptionSpanData\n]\nThe newly created speech-to-text span.\nSource code in\nsrc/agents/tracing/create.py\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\ndef\ntranscription_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTranscriptionSpanData\n]:\n\"\"\"Create a new transcription span. The span will not be started automatically, you should\neither do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nmodel: The name of the model used for the speech-to-text.\ninput: The audio input of the speech-to-text transcription, as a base64 encoded string of\naudio bytes.\ninput_format: The format of the audio input (defaults to \"pcm\").\noutput: The output of the speech-to-text transcription.\nmodel_config: The model configuration (hyperparameters) used.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created speech-to-text span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nTranscriptionSpanData\n(\ninput\n=\ninput\n,\ninput_format\n=\ninput_format\n,\noutput\n=\noutput\n,\nmodel\n=\nmodel\n,\nmodel_config\n=\nmodel_config\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ngen_span_id\ngen_span_id\n()\n->\nstr\nGenerates a new span ID.\nSource code in\nsrc/agents/tracing/util.py\n15\n16\n17\ndef\ngen_span_id\n()\n->\nstr\n:\n\"\"\"Generates a new span ID.\"\"\"\nreturn\nf\n\"span_\n{\nuuid\n.\nuuid4\n()\n.\nhex\n[:\n24\n]\n}\n\"\ngen_trace_id\ngen_trace_id\n()\n->\nstr\nGenerates a new trace ID.\nSource code in\nsrc/agents/tracing/util.py\n10\n11\n12\ndef\ngen_trace_id\n()\n->\nstr\n:\n\"\"\"Generates a new trace ID.\"\"\"\nreturn\nf\n\"trace_\n{\nuuid\n.\nuuid4\n()\n.\nhex\n}\n\"\nadd_trace_processor\nadd_trace_processor\n(\nspan_processor\n:\nTracingProcessor\n,\n)\n->\nNone\nAdds a new trace processor. This processor will receive all traces/spans.\nSource code in\nsrc/agents/tracing/__init__.py\n79\n80\n81\n82\n83\ndef\nadd_trace_processor\n(\nspan_processor\n:\nTracingProcessor\n)\n->\nNone\n:\n\"\"\"\nAdds a new trace processor. This processor will receive all traces/spans.\n\"\"\"\nGLOBAL_TRACE_PROVIDER\n.\nregister_processor\n(\nspan_processor\n)\nset_trace_processors\nset_trace_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n],\n)\n->\nNone\nSet the list of trace processors. This will replace the current list of processors.\nSource code in\nsrc/agents/tracing/__init__.py\n86\n87\n88\n89\n90\ndef\nset_trace_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n])\n->\nNone\n:\n\"\"\"\nSet the list of trace processors. This will replace the current list of processors.\n\"\"\"\nGLOBAL_TRACE_PROVIDER\n.\nset_processors\n(\nprocessors\n)\nset_tracing_disabled\nset_tracing_disabled\n(\ndisabled\n:\nbool\n)\n->\nNone\nSet whether tracing is globally disabled.\nSource code in\nsrc/agents/tracing/__init__.py\n93\n94\n95\n96\n97\ndef\nset_tracing_disabled\n(\ndisabled\n:\nbool\n)\n->\nNone\n:\n\"\"\"\nSet whether tracing is globally disabled.\n\"\"\"\nGLOBAL_TRACE_PROVIDER\n.\nset_disabled\n(\ndisabled\n)\nset_tracing_export_api_key\nset_tracing_export_api_key\n(\napi_key\n:\nstr\n)\n->\nNone\nSet the OpenAI API key for the backend exporter.\nSource code in\nsrc/agents/tracing/__init__.py\n100\n101\n102\n103\n104\ndef\nset_tracing_export_api_key\n(\napi_key\n:\nstr\n)\n->\nNone\n:\n\"\"\"\nSet the OpenAI API key for the backend exporter.\n\"\"\"\ndefault_exporter\n()\n.\nset_api_key\n(\napi_key\n)",
  "Creating traces/spans": "Creating traces/spans\ntrace\ntrace\n(\nworkflow_name\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\nCreate a new trace. The trace will not be started automatically; you should either use\nit as a context manager (\nwith trace(...):\n) or call\ntrace.start()\n+\ntrace.finish()\nmanually.\nIn addition to the workflow name and optional grouping identifier, you can provide\nan arbitrary metadata dictionary to attach additional user-defined information to\nthe trace.\nParameters:\nName\nType\nDescription\nDefault\nworkflow_name\nstr\nThe name of the logical app or workflow. For example, you might provide\n\"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\nrequired\ntrace_id\nstr\n| None\nThe ID of the trace. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_trace_id()\nto generate a trace ID, to guarantee that IDs are\ncorrectly formatted.\nNone\ngroup_id\nstr\n| None\nOptional grouping identifier to link multiple traces from the same conversation\nor process. For instance, you might use a chat thread ID.\nNone\nmetadata\ndict\n[\nstr\n,\nAny\n] | None\nOptional dictionary of additional metadata to attach to the trace.\nNone\ndisabled\nbool\nIf True, we will return a Trace but the Trace will not be recorded. This will\nnot be checked if there's an existing trace and\neven_if_trace_running\nis True.\nFalse\nReturns:\nType\nDescription\nTrace\nThe newly created trace object.\nSource code in\nsrc/agents/tracing/create.py\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\ndef\ntrace\n(\nworkflow_name\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\n:\n\"\"\"\nCreate a new trace. The trace will not be started automatically; you should either use\nit as a context manager (`with trace(...):`) or call `trace.start()` + `trace.finish()`\nmanually.\nIn addition to the workflow name and optional grouping identifier, you can provide\nan arbitrary metadata dictionary to attach additional user-defined information to\nthe trace.\nArgs:\nworkflow_name: The name of the logical app or workflow. For example, you might provide\n\"code_bot\" for a coding agent, or \"customer_support_agent\" for a customer support agent.\ntrace_id: The ID of the trace. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_trace_id()` to generate a trace ID, to guarantee that IDs are\ncorrectly formatted.\ngroup_id: Optional grouping identifier to link multiple traces from the same conversation\nor process. For instance, you might use a chat thread ID.\nmetadata: Optional dictionary of additional metadata to attach to the trace.\ndisabled: If True, we will return a Trace but the Trace will not be recorded. This will\nnot be checked if there's an existing trace and `even_if_trace_running` is True.\nReturns:\nThe newly created trace object.\n\"\"\"\ncurrent_trace\n=\nGLOBAL_TRACE_PROVIDER\n.\nget_current_trace\n()\nif\ncurrent_trace\n:\nlogger\n.\nwarning\n(\n\"Trace already exists. Creating a new trace, but this is probably a mistake.\"\n)\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_trace\n(\nname\n=\nworkflow_name\n,\ntrace_id\n=\ntrace_id\n,\ngroup_id\n=\ngroup_id\n,\nmetadata\n=\nmetadata\n,\ndisabled\n=\ndisabled\n,\n)\nget_current_trace\nget_current_trace\n()\n->\nTrace\n|\nNone\nReturns the currently active trace, if present.\nSource code in\nsrc/agents/tracing/create.py\n74\n75\n76\ndef\nget_current_trace\n()\n->\nTrace\n|\nNone\n:\n\"\"\"Returns the currently active trace, if present.\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\nget_current_trace\n()\nget_current_span\nget_current_span\n()\n->\nSpan\n[\nAny\n]\n|\nNone\nReturns the currently active span, if present.\nSource code in\nsrc/agents/tracing/create.py\n79\n80\n81\ndef\nget_current_span\n()\n->\nSpan\n[\nAny\n]\n|\nNone\n:\n\"\"\"Returns the currently active span, if present.\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\nget_current_span\n()\nagent_span\nagent_span\n(\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nAgentSpanData\n]\nCreate a new agent span. The span will not be started automatically, you should either do\nwith agent_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the agent.\nrequired\nhandoffs\nlist\n[\nstr\n] | None\nOptional list of agent names to which this agent could hand off control.\nNone\ntools\nlist\n[\nstr\n] | None\nOptional list of tool names available to this agent.\nNone\noutput_type\nstr\n| None\nOptional name of the output type produced by the agent.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nAgentSpanData\n]\nThe newly created agent span.\nSource code in\nsrc/agents/tracing/create.py\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\ndef\nagent_span\n(\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nAgentSpanData\n]:\n\"\"\"Create a new agent span. The span will not be started automatically, you should either do\n`with agent_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the agent.\nhandoffs: Optional list of agent names to which this agent could hand off control.\ntools: Optional list of tool names available to this agent.\noutput_type: Optional name of the output type produced by the agent.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created agent span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nAgentSpanData\n(\nname\n=\nname\n,\nhandoffs\n=\nhandoffs\n,\ntools\n=\ntools\n,\noutput_type\n=\noutput_type\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nfunction_span\nfunction_span\n(\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nFunctionSpanData\n]\nCreate a new function span. The span will not be started automatically, you should either do\nwith function_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the function.\nrequired\ninput\nstr\n| None\nThe input to the function.\nNone\noutput\nstr\n| None\nThe output of the function.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nFunctionSpanData\n]\nThe newly created function span.\nSource code in\nsrc/agents/tracing/create.py\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\ndef\nfunction_span\n(\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nFunctionSpanData\n]:\n\"\"\"Create a new function span. The span will not be started automatically, you should either do\n`with function_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the function.\ninput: The input to the function.\noutput: The output of the function.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created function span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nFunctionSpanData\n(\nname\n=\nname\n,\ninput\n=\ninput\n,\noutput\n=\noutput\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ngeneration_span\ngeneration_span\n(\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGenerationSpanData\n]\nCreate a new generation span. The span will not be started automatically, you should either\ndo\nwith generation_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nThis span captures the details of a model generation, including the\ninput message sequence, any generated outputs, the model name and\nconfiguration, and usage data. If you only need to capture a model\nresponse identifier, use\nresponse_span()\ninstead.\nParameters:\nName\nType\nDescription\nDefault\ninput\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]] | None\nThe sequence of input messages sent to the model.\nNone\noutput\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]] | None\nThe sequence of output messages received from the model.\nNone\nmodel\nstr\n| None\nThe model identifier used for the generation.\nNone\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nusage\ndict\n[\nstr\n,\nAny\n] | None\nA dictionary of usage information (input tokens, output tokens, etc.).\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nGenerationSpanData\n]\nThe newly created generation span.\nSource code in\nsrc/agents/tracing/create.py\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\ndef\ngeneration_span\n(\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGenerationSpanData\n]:\n\"\"\"Create a new generation span. The span will not be started automatically, you should either\ndo `with generation_span() ...` or call `span.start()` + `span.finish()` manually.\nThis span captures the details of a model generation, including the\ninput message sequence, any generated outputs, the model name and\nconfiguration, and usage data. If you only need to capture a model\nresponse identifier, use `response_span()` instead.\nArgs:\ninput: The sequence of input messages sent to the model.\noutput: The sequence of output messages received from the model.\nmodel: The model identifier used for the generation.\nmodel_config: The model configuration (hyperparameters) used.\nusage: A dictionary of usage information (input tokens, output tokens, etc.).\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created generation span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nGenerationSpanData\n(\ninput\n=\ninput\n,\noutput\n=\noutput\n,\nmodel\n=\nmodel\n,\nmodel_config\n=\nmodel_config\n,\nusage\n=\nusage\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nresponse_span\nresponse_span\n(\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nResponseSpanData\n]\nCreate a new response span. The span will not be started automatically, you should either do\nwith response_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nresponse\nResponse\n| None\nThe OpenAI Response object.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\ndef\nresponse_span\n(\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nResponseSpanData\n]:\n\"\"\"Create a new response span. The span will not be started automatically, you should either do\n`with response_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nresponse: The OpenAI Response object.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nResponseSpanData\n(\nresponse\n=\nresponse\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nhandoff_span\nhandoff_span\n(\nfrom_agent\n:\nstr\n|\nNone\n=\nNone\n,\nto_agent\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nHandoffSpanData\n]\nCreate a new handoff span. The span will not be started automatically, you should either do\nwith handoff_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nfrom_agent\nstr\n| None\nThe name of the agent that is handing off.\nNone\nto_agent\nstr\n| None\nThe name of the agent that is receiving the handoff.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nHandoffSpanData\n]\nThe newly created handoff span.\nSource code in\nsrc/agents/tracing/create.py\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\ndef\nhandoff_span\n(\nfrom_agent\n:\nstr\n|\nNone\n=\nNone\n,\nto_agent\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nHandoffSpanData\n]:\n\"\"\"Create a new handoff span. The span will not be started automatically, you should either do\n`with handoff_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nfrom_agent: The name of the agent that is handing off.\nto_agent: The name of the agent that is receiving the handoff.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created handoff span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nHandoffSpanData\n(\nfrom_agent\n=\nfrom_agent\n,\nto_agent\n=\nto_agent\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ncustom_span\ncustom_span\n(\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nCustomSpanData\n]\nCreate a new custom span, to which you can add your own metadata. The span will not be\nstarted automatically, you should either do\nwith custom_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the custom span.\nrequired\ndata\ndict\n[\nstr\n,\nAny\n] | None\nArbitrary structured data to associate with the span.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nCustomSpanData\n]\nThe newly created custom span.\nSource code in\nsrc/agents/tracing/create.py\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\ndef\ncustom_span\n(\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nCustomSpanData\n]:\n\"\"\"Create a new custom span, to which you can add your own metadata. The span will not be\nstarted automatically, you should either do `with custom_span() ...` or call\n`span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the custom span.\ndata: Arbitrary structured data to associate with the span.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created custom span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nCustomSpanData\n(\nname\n=\nname\n,\ndata\n=\ndata\nor\n{}),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nguardrail_span\nguardrail_span\n(\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGuardrailSpanData\n]\nCreate a new guardrail span. The span will not be started automatically, you should either\ndo\nwith guardrail_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nname\nstr\nThe name of the guardrail.\nrequired\ntriggered\nbool\nWhether the guardrail was triggered.\nFalse\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\ndef\nguardrail_span\n(\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nGuardrailSpanData\n]:\n\"\"\"Create a new guardrail span. The span will not be started automatically, you should either\ndo `with guardrail_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nname: The name of the guardrail.\ntriggered: Whether the guardrail was triggered.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nGuardrailSpanData\n(\nname\n=\nname\n,\ntriggered\n=\ntriggered\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\ntranscription_span\ntranscription_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTranscriptionSpanData\n]\nCreate a new transcription span. The span will not be started automatically, you should\neither do\nwith transcription_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\n| None\nThe name of the model used for the speech-to-text.\nNone\ninput\nstr\n| None\nThe audio input of the speech-to-text transcription, as a base64 encoded string of\naudio bytes.\nNone\ninput_format\nstr\n| None\nThe format of the audio input (defaults to \"pcm\").\n'pcm'\noutput\nstr\n| None\nThe output of the speech-to-text transcription.\nNone\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nReturns:\nType\nDescription\nSpan\n[\nTranscriptionSpanData\n]\nThe newly created speech-to-text span.\nSource code in\nsrc/agents/tracing/create.py\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\ndef\ntranscription_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTranscriptionSpanData\n]:\n\"\"\"Create a new transcription span. The span will not be started automatically, you should\neither do `with transcription_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nmodel: The name of the model used for the speech-to-text.\ninput: The audio input of the speech-to-text transcription, as a base64 encoded string of\naudio bytes.\ninput_format: The format of the audio input (defaults to \"pcm\").\noutput: The output of the speech-to-text transcription.\nmodel_config: The model configuration (hyperparameters) used.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\nReturns:\nThe newly created speech-to-text span.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nTranscriptionSpanData\n(\ninput\n=\ninput\n,\ninput_format\n=\ninput_format\n,\noutput\n=\noutput\n,\nmodel\n=\nmodel\n,\nmodel_config\n=\nmodel_config\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nspeech_span\nspeech_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechSpanData\n]\nCreate a new speech span. The span will not be started automatically, you should either do\nwith speech_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\n| None\nThe name of the model used for the text-to-speech.\nNone\ninput\nstr\n| None\nThe text input of the text-to-speech.\nNone\noutput\nstr\n| None\nThe audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\nNone\noutput_format\nstr\n| None\nThe format of the audio output (defaults to \"pcm\").\n'pcm'\nmodel_config\nMapping\n[\nstr\n,\nAny\n] | None\nThe model configuration (hyperparameters) used.\nNone\nfirst_content_at\nstr\n| None\nThe time of the first byte of the audio output.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\ndef\nspeech_span\n(\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechSpanData\n]:\n\"\"\"Create a new speech span. The span will not be started automatically, you should either do\n`with speech_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nmodel: The name of the model used for the text-to-speech.\ninput: The text input of the text-to-speech.\noutput: The audio output of the text-to-speech as base64 encoded string of PCM audio bytes.\noutput_format: The format of the audio output (defaults to \"pcm\").\nmodel_config: The model configuration (hyperparameters) used.\nfirst_content_at: The time of the first byte of the audio output.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nSpeechSpanData\n(\nmodel\n=\nmodel\n,\ninput\n=\ninput\n,\noutput\n=\noutput\n,\noutput_format\n=\noutput_format\n,\nmodel_config\n=\nmodel_config\n,\nfirst_content_at\n=\nfirst_content_at\n,\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nspeech_group_span\nspeech_group_span\n(\ninput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechGroupSpanData\n]\nCreate a new speech group span. The span will not be started automatically, you should\neither do\nwith speech_group_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\ninput\nstr\n| None\nThe input text used for the speech request.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\ndef\nspeech_group_span\n(\ninput\n:\nstr\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nSpeechGroupSpanData\n]:\n\"\"\"Create a new speech group span. The span will not be started automatically, you should\neither do `with speech_group_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\ninput: The input text used for the speech request.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nSpeechGroupSpanData\n(\ninput\n=\ninput\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)\nmcp_tools_span\nmcp_tools_span\n(\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nMCPListToolsSpanData\n]\nCreate a new MCP list tools span. The span will not be started automatically, you should\neither do\nwith mcp_tools_span() ...\nor call\nspan.start()\n+\nspan.finish()\nmanually.\nParameters:\nName\nType\nDescription\nDefault\nserver\nstr\n| None\nThe name of the MCP server.\nNone\nresult\nlist\n[\nstr\n] | None\nThe result of the MCP list tools call.\nNone\nspan_id\nstr\n| None\nThe ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using\nutil.gen_span_id()\nto generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nNone\nparent\nTrace\n|\nSpan\n[\nAny\n] | None\nThe parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\nNone\ndisabled\nbool\nIf True, we will return a Span but the Span will not be recorded.\nFalse\nSource code in\nsrc/agents/tracing/create.py\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\ndef\nmcp_tools_span\n(\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nMCPListToolsSpanData\n]:\n\"\"\"Create a new MCP list tools span. The span will not be started automatically, you should\neither do `with mcp_tools_span() ...` or call `span.start()` + `span.finish()` manually.\nArgs:\nserver: The name of the MCP server.\nresult: The result of the MCP list tools call.\nspan_id: The ID of the span. Optional. If not provided, we will generate an ID. We\nrecommend using `util.gen_span_id()` to generate a span ID, to guarantee that IDs are\ncorrectly formatted.\nparent: The parent span or trace. If not provided, we will automatically use the current\ntrace/span as the parent.\ndisabled: If True, we will return a Span but the Span will not be recorded.\n\"\"\"\nreturn\nGLOBAL_TRACE_PROVIDER\n.\ncreate_span\n(\nspan_data\n=\nMCPListToolsSpanData\n(\nserver\n=\nserver\n,\nresult\n=\nresult\n),\nspan_id\n=\nspan_id\n,\nparent\n=\nparent\n,\ndisabled\n=\ndisabled\n,\n)",
  "Traces": "Traces\nTrace\nA trace is the root level object that tracing creates. It represents a logical \"workflow\".\nSource code in\nsrc/agents/tracing/traces.py\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\nclass\nTrace\n:\n\"\"\"\nA trace is the root level object that tracing creates. It represents a logical \"workflow\".\n\"\"\"\n@abc\n.\nabstractmethod\ndef\n__enter__\n(\nself\n)\n->\nTrace\n:\npass\n@abc\n.\nabstractmethod\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\npass\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the trace.\nArgs:\nmark_as_current: If true, the trace will be marked as the current trace.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nFinish the trace.\nArgs:\nreset_current: If true, the trace will be reset as the current trace.\n\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\n\"\"\"\nThe trace ID.\n\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\nname\n(\nself\n)\n->\nstr\n:\n\"\"\"\nThe name of the workflow being traced.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\n\"\"\"\nExport the trace as a dictionary.\n\"\"\"\npass\ntrace_id\nabstractmethod\nproperty\ntrace_id\n:\nstr\nThe trace ID.\nname\nabstractmethod\nproperty\nname\n:\nstr\nThe name of the workflow being traced.\nstart\nabstractmethod\nstart\n(\nmark_as_current\n:\nbool\n=\nFalse\n)\nStart the trace.\nParameters:\nName\nType\nDescription\nDefault\nmark_as_current\nbool\nIf true, the trace will be marked as the current trace.\nFalse\nSource code in\nsrc/agents/tracing/traces.py\n26\n27\n28\n29\n30\n31\n32\n33\n34\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the trace.\nArgs:\nmark_as_current: If true, the trace will be marked as the current trace.\n\"\"\"\npass\nfinish\nabstractmethod\nfinish\n(\nreset_current\n:\nbool\n=\nFalse\n)\nFinish the trace.\nParameters:\nName\nType\nDescription\nDefault\nreset_current\nbool\nIf true, the trace will be reset as the current trace.\nFalse\nSource code in\nsrc/agents/tracing/traces.py\n36\n37\n38\n39\n40\n41\n42\n43\n44\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nFinish the trace.\nArgs:\nreset_current: If true, the trace will be reset as the current trace.\n\"\"\"\npass\nexport\nabstractmethod\nexport\n()\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\nExport the trace as a dictionary.\nSource code in\nsrc/agents/tracing/traces.py\n62\n63\n64\n65\n66\n67\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\n\"\"\"\nExport the trace as a dictionary.\n\"\"\"\npass\nNoOpTrace\nBases:\nTrace\nA no-op trace that will not be recorded.\nSource code in\nsrc/agents/tracing/traces.py\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\nclass\nNoOpTrace\n(\nTrace\n):\n\"\"\"\nA no-op trace that will not be recorded.\n\"\"\"\ndef\n__init__\n(\nself\n):\nself\n.\n_started\n=\nFalse\nself\n.\n_prev_context_token\n:\ncontextvars\n.\nToken\n[\nTrace\n|\nNone\n]\n|\nNone\n=\nNone\ndef\n__enter__\n(\nself\n)\n->\nTrace\n:\nif\nself\n.\n_started\n:\nif\nnot\nself\n.\n_prev_context_token\n:\nlogger\n.\nerror\n(\n\"Trace already started but no context token set\"\n)\nreturn\nself\nself\n.\n_started\n=\nTrue\nself\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nreturn\nself\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\nself\n.\nfinish\n(\nreset_current\n=\nTrue\n)\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\nif\nmark_as_current\n:\nself\n.\n_prev_context_token\n=\nScope\n.\nset_current_trace\n(\nself\n)\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\nif\nreset_current\nand\nself\n.\n_prev_context_token\nis\nnot\nNone\n:\nScope\n.\nreset_current_trace\n(\nself\n.\n_prev_context_token\n)\nself\n.\n_prev_context_token\n=\nNone\n@property\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\nreturn\n\"no-op\"\n@property\ndef\nname\n(\nself\n)\n->\nstr\n:\nreturn\n\"no-op\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nreturn\nNone\nTraceImpl\nBases:\nTrace\nA trace that will be recorded by the tracing library.\nSource code in\nsrc/agents/tracing/traces.py\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\nclass\nTraceImpl\n(\nTrace\n):\n\"\"\"\nA trace that will be recorded by the tracing library.\n\"\"\"\n__slots__\n=\n(\n\"_name\"\n,\n\"_trace_id\"\n,\n\"group_id\"\n,\n\"metadata\"\n,\n\"_prev_context_token\"\n,\n\"_processor\"\n,\n\"_started\"\n,\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n,\nprocessor\n:\nTracingProcessor\n,\n):\nself\n.\n_name\n=\nname\nself\n.\n_trace_id\n=\ntrace_id\nor\nutil\n.\ngen_trace_id\n()\nself\n.\ngroup_id\n=\ngroup_id\nself\n.\nmetadata\n=\nmetadata\nself\n.\n_prev_context_token\n:\ncontextvars\n.\nToken\n[\nTrace\n|\nNone\n]\n|\nNone\n=\nNone\nself\n.\n_processor\n=\nprocessor\nself\n.\n_started\n=\nFalse\n@property\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\n_trace_id\n@property\ndef\nname\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\n_name\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\nif\nself\n.\n_started\n:\nreturn\nself\n.\n_started\n=\nTrue\nself\n.\n_processor\n.\non_trace_start\n(\nself\n)\nif\nmark_as_current\n:\nself\n.\n_prev_context_token\n=\nScope\n.\nset_current_trace\n(\nself\n)\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n):\nif\nnot\nself\n.\n_started\n:\nreturn\nself\n.\n_processor\n.\non_trace_end\n(\nself\n)\nif\nreset_current\nand\nself\n.\n_prev_context_token\nis\nnot\nNone\n:\nScope\n.\nreset_current_trace\n(\nself\n.\n_prev_context_token\n)\nself\n.\n_prev_context_token\n=\nNone\ndef\n__enter__\n(\nself\n)\n->\nTrace\n:\nif\nself\n.\n_started\n:\nif\nnot\nself\n.\n_prev_context_token\n:\nlogger\n.\nerror\n(\n\"Trace already started but no context token set\"\n)\nreturn\nself\nself\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nreturn\nself\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\nself\n.\nfinish\n(\nreset_current\n=\nexc_type\nis\nnot\nGeneratorExit\n)\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nreturn\n{\n\"object\"\n:\n\"trace\"\n,\n\"id\"\n:\nself\n.\ntrace_id\n,\n\"workflow_name\"\n:\nself\n.\nname\n,\n\"group_id\"\n:\nself\n.\ngroup_id\n,\n\"metadata\"\n:\nself\n.\nmetadata\n,\n}",
  "Spans": "Spans\nSpan\nBases:\nABC\n,\nGeneric\n[\nTSpanData\n]\nSource code in\nsrc/agents/tracing/spans.py\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nclass\nSpan\n(\nabc\n.\nABC\n,\nGeneric\n[\nTSpanData\n]):\n@property\n@abc\n.\nabstractmethod\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nspan_id\n(\nself\n)\n->\nstr\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nspan_data\n(\nself\n)\n->\nTSpanData\n:\npass\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the span.\nArgs:\nmark_as_current: If true, the span will be marked as the current span.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\n\"\"\"\nFinish the span.\nArgs:\nreset_current: If true, the span will be reset as the current span.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\n__enter__\n(\nself\n)\n->\nSpan\n[\nTSpanData\n]:\npass\n@abc\n.\nabstractmethod\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\npass\n@property\n@abc\n.\nabstractmethod\ndef\nparent_id\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\n@abc\n.\nabstractmethod\ndef\nset_error\n(\nself\n,\nerror\n:\nSpanError\n)\n->\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nerror\n(\nself\n)\n->\nSpanError\n|\nNone\n:\npass\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nstarted_at\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\n@property\n@abc\n.\nabstractmethod\ndef\nended_at\n(\nself\n)\n->\nstr\n|\nNone\n:\npass\nstart\nabstractmethod\nstart\n(\nmark_as_current\n:\nbool\n=\nFalse\n)\nStart the span.\nParameters:\nName\nType\nDescription\nDefault\nmark_as_current\nbool\nIf true, the span will be marked as the current span.\nFalse\nSource code in\nsrc/agents/tracing/spans.py\n39\n40\n41\n42\n43\n44\n45\n46\n47\n@abc\n.\nabstractmethod\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\n\"\"\"\nStart the span.\nArgs:\nmark_as_current: If true, the span will be marked as the current span.\n\"\"\"\npass\nfinish\nabstractmethod\nfinish\n(\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\nFinish the span.\nParameters:\nName\nType\nDescription\nDefault\nreset_current\nbool\nIf true, the span will be reset as the current span.\nFalse\nSource code in\nsrc/agents/tracing/spans.py\n49\n50\n51\n52\n53\n54\n55\n56\n57\n@abc\n.\nabstractmethod\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\n\"\"\"\nFinish the span.\nArgs:\nreset_current: If true, the span will be reset as the current span.\n\"\"\"\npass\nNoOpSpan\nBases:\nSpan\n[\nTSpanData\n]\nSource code in\nsrc/agents/tracing/spans.py\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\nclass\nNoOpSpan\n(\nSpan\n[\nTSpanData\n]):\n__slots__\n=\n(\n\"_span_data\"\n,\n\"_prev_span_token\"\n)\ndef\n__init__\n(\nself\n,\nspan_data\n:\nTSpanData\n):\nself\n.\n_span_data\n=\nspan_data\nself\n.\n_prev_span_token\n:\ncontextvars\n.\nToken\n[\nSpan\n[\nTSpanData\n]\n|\nNone\n]\n|\nNone\n=\nNone\n@property\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\nreturn\n\"no-op\"\n@property\ndef\nspan_id\n(\nself\n)\n->\nstr\n:\nreturn\n\"no-op\"\n@property\ndef\nspan_data\n(\nself\n)\n->\nTSpanData\n:\nreturn\nself\n.\n_span_data\n@property\ndef\nparent_id\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nNone\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\nif\nmark_as_current\n:\nself\n.\n_prev_span_token\n=\nScope\n.\nset_current_span\n(\nself\n)\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\nif\nreset_current\nand\nself\n.\n_prev_span_token\nis\nnot\nNone\n:\nScope\n.\nreset_current_span\n(\nself\n.\n_prev_span_token\n)\nself\n.\n_prev_span_token\n=\nNone\ndef\n__enter__\n(\nself\n)\n->\nSpan\n[\nTSpanData\n]:\nself\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nreturn\nself\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\nreset_current\n=\nTrue\nif\nexc_type\nis\nGeneratorExit\n:\nlogger\n.\ndebug\n(\n\"GeneratorExit, skipping span reset\"\n)\nreset_current\n=\nFalse\nself\n.\nfinish\n(\nreset_current\n=\nreset_current\n)\ndef\nset_error\n(\nself\n,\nerror\n:\nSpanError\n)\n->\nNone\n:\npass\n@property\ndef\nerror\n(\nself\n)\n->\nSpanError\n|\nNone\n:\nreturn\nNone\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nreturn\nNone\n@property\ndef\nstarted_at\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nNone\n@property\ndef\nended_at\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nNone\nSpanImpl\nBases:\nSpan\n[\nTSpanData\n]\nSource code in\nsrc/agents/tracing/spans.py\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\nclass\nSpanImpl\n(\nSpan\n[\nTSpanData\n]):\n__slots__\n=\n(\n\"_trace_id\"\n,\n\"_span_id\"\n,\n\"_parent_id\"\n,\n\"_started_at\"\n,\n\"_ended_at\"\n,\n\"_error\"\n,\n\"_prev_span_token\"\n,\n\"_processor\"\n,\n\"_span_data\"\n,\n)\ndef\n__init__\n(\nself\n,\ntrace_id\n:\nstr\n,\nspan_id\n:\nstr\n|\nNone\n,\nparent_id\n:\nstr\n|\nNone\n,\nprocessor\n:\nTracingProcessor\n,\nspan_data\n:\nTSpanData\n,\n):\nself\n.\n_trace_id\n=\ntrace_id\nself\n.\n_span_id\n=\nspan_id\nor\nutil\n.\ngen_span_id\n()\nself\n.\n_parent_id\n=\nparent_id\nself\n.\n_started_at\n:\nstr\n|\nNone\n=\nNone\nself\n.\n_ended_at\n:\nstr\n|\nNone\n=\nNone\nself\n.\n_processor\n=\nprocessor\nself\n.\n_error\n:\nSpanError\n|\nNone\n=\nNone\nself\n.\n_prev_span_token\n:\ncontextvars\n.\nToken\n[\nSpan\n[\nTSpanData\n]\n|\nNone\n]\n|\nNone\n=\nNone\nself\n.\n_span_data\n=\nspan_data\n@property\ndef\ntrace_id\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\n_trace_id\n@property\ndef\nspan_id\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\n_span_id\n@property\ndef\nspan_data\n(\nself\n)\n->\nTSpanData\n:\nreturn\nself\n.\n_span_data\n@property\ndef\nparent_id\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nself\n.\n_parent_id\ndef\nstart\n(\nself\n,\nmark_as_current\n:\nbool\n=\nFalse\n):\nif\nself\n.\nstarted_at\nis\nnot\nNone\n:\nlogger\n.\nwarning\n(\n\"Span already started\"\n)\nreturn\nself\n.\n_started_at\n=\nutil\n.\ntime_iso\n()\nself\n.\n_processor\n.\non_span_start\n(\nself\n)\nif\nmark_as_current\n:\nself\n.\n_prev_span_token\n=\nScope\n.\nset_current_span\n(\nself\n)\ndef\nfinish\n(\nself\n,\nreset_current\n:\nbool\n=\nFalse\n)\n->\nNone\n:\nif\nself\n.\nended_at\nis\nnot\nNone\n:\nlogger\n.\nwarning\n(\n\"Span already finished\"\n)\nreturn\nself\n.\n_ended_at\n=\nutil\n.\ntime_iso\n()\nself\n.\n_processor\n.\non_span_end\n(\nself\n)\nif\nreset_current\nand\nself\n.\n_prev_span_token\nis\nnot\nNone\n:\nScope\n.\nreset_current_span\n(\nself\n.\n_prev_span_token\n)\nself\n.\n_prev_span_token\n=\nNone\ndef\n__enter__\n(\nself\n)\n->\nSpan\n[\nTSpanData\n]:\nself\n.\nstart\n(\nmark_as_current\n=\nTrue\n)\nreturn\nself\ndef\n__exit__\n(\nself\n,\nexc_type\n,\nexc_val\n,\nexc_tb\n):\nreset_current\n=\nTrue\nif\nexc_type\nis\nGeneratorExit\n:\nlogger\n.\ndebug\n(\n\"GeneratorExit, skipping span reset\"\n)\nreset_current\n=\nFalse\nself\n.\nfinish\n(\nreset_current\n=\nreset_current\n)\ndef\nset_error\n(\nself\n,\nerror\n:\nSpanError\n)\n->\nNone\n:\nself\n.\n_error\n=\nerror\n@property\ndef\nerror\n(\nself\n)\n->\nSpanError\n|\nNone\n:\nreturn\nself\n.\n_error\n@property\ndef\nstarted_at\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nself\n.\n_started_at\n@property\ndef\nended_at\n(\nself\n)\n->\nstr\n|\nNone\n:\nreturn\nself\n.\n_ended_at\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n:\nreturn\n{\n\"object\"\n:\n\"trace.span\"\n,\n\"id\"\n:\nself\n.\nspan_id\n,\n\"trace_id\"\n:\nself\n.\ntrace_id\n,\n\"parent_id\"\n:\nself\n.\n_parent_id\n,\n\"started_at\"\n:\nself\n.\n_started_at\n,\n\"ended_at\"\n:\nself\n.\n_ended_at\n,\n\"span_data\"\n:\nself\n.\nspan_data\n.\nexport\n(),\n\"error\"\n:\nself\n.\n_error\n,\n}",
  "Processor interface": "Processor interface\nTracingProcessor\nBases:\nABC\nInterface for processing spans.\nSource code in\nsrc/agents/tracing/processor_interface.py\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\nclass\nTracingProcessor\n(\nabc\n.\nABC\n):\n\"\"\"Interface for processing spans.\"\"\"\n@abc\n.\nabstractmethod\ndef\non_trace_start\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is started.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_trace_end\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is finished.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_span_start\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is started.\nArgs:\nspan: The span that started.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\non_span_end\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is finished. Should not block or raise exceptions.\nArgs:\nspan: The span that finished.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"Called when the application stops.\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nforce_flush\n(\nself\n)\n->\nNone\n:\n\"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\npass\non_trace_start\nabstractmethod\non_trace_start\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is started.\nParameters:\nName\nType\nDescription\nDefault\ntrace\nTrace\nThe trace that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n12\n13\n14\n15\n16\n17\n18\n19\n@abc\n.\nabstractmethod\ndef\non_trace_start\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is started.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\non_trace_end\nabstractmethod\non_trace_end\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is finished.\nParameters:\nName\nType\nDescription\nDefault\ntrace\nTrace\nThe trace that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n21\n22\n23\n24\n25\n26\n27\n28\n@abc\n.\nabstractmethod\ndef\non_trace_end\n(\nself\n,\ntrace\n:\n\"Trace\"\n)\n->\nNone\n:\n\"\"\"Called when a trace is finished.\nArgs:\ntrace: The trace that started.\n\"\"\"\npass\non_span_start\nabstractmethod\non_span_start\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is started.\nParameters:\nName\nType\nDescription\nDefault\nspan\nSpan\n[\nAny\n]\nThe span that started.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n30\n31\n32\n33\n34\n35\n36\n37\n@abc\n.\nabstractmethod\ndef\non_span_start\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is started.\nArgs:\nspan: The span that started.\n\"\"\"\npass\non_span_end\nabstractmethod\non_span_end\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is finished. Should not block or raise exceptions.\nParameters:\nName\nType\nDescription\nDefault\nspan\nSpan\n[\nAny\n]\nThe span that finished.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n39\n40\n41\n42\n43\n44\n45\n46\n@abc\n.\nabstractmethod\ndef\non_span_end\n(\nself\n,\nspan\n:\n\"Span[Any]\"\n)\n->\nNone\n:\n\"\"\"Called when a span is finished. Should not block or raise exceptions.\nArgs:\nspan: The span that finished.\n\"\"\"\npass\nshutdown\nabstractmethod\nshutdown\n()\n->\nNone\nCalled when the application stops.\nSource code in\nsrc/agents/tracing/processor_interface.py\n48\n49\n50\n51\n@abc\n.\nabstractmethod\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"Called when the application stops.\"\"\"\npass\nforce_flush\nabstractmethod\nforce_flush\n()\n->\nNone\nForces an immediate flush of all queued spans/traces.\nSource code in\nsrc/agents/tracing/processor_interface.py\n53\n54\n55\n56\n@abc\n.\nabstractmethod\ndef\nforce_flush\n(\nself\n)\n->\nNone\n:\n\"\"\"Forces an immediate flush of all queued spans/traces.\"\"\"\npass\nTracingExporter\nBases:\nABC\nExports traces and spans. For example, could log them or send them to a backend.\nSource code in\nsrc/agents/tracing/processor_interface.py\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\nclass\nTracingExporter\n(\nabc\n.\nABC\n):\n\"\"\"Exports traces and spans. For example, could log them or send them to a backend.\"\"\"\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n,\nitems\n:\nlist\n[\n\"Trace | Span[Any]\"\n])\n->\nNone\n:\n\"\"\"Exports a list of traces and spans.\nArgs:\nitems: The items to export.\n\"\"\"\npass\nexport\nabstractmethod\nexport\n(\nitems\n:\nlist\n[\nTrace\n|\nSpan\n[\nAny\n]])\n->\nNone\nExports a list of traces and spans.\nParameters:\nName\nType\nDescription\nDefault\nitems\nlist\n[\nTrace\n|\nSpan\n[\nAny\n]]\nThe items to export.\nrequired\nSource code in\nsrc/agents/tracing/processor_interface.py\n62\n63\n64\n65\n66\n67\n68\n69\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n,\nitems\n:\nlist\n[\n\"Trace | Span[Any]\"\n])\n->\nNone\n:\n\"\"\"Exports a list of traces and spans.\nArgs:\nitems: The items to export.\n\"\"\"\npass",
  "Processors": "Processors\nConsoleSpanExporter\nBases:\nTracingExporter\nPrints the traces and spans to the console.\nSource code in\nsrc/agents/tracing/processors.py\n19\n20\n21\n22\n23\n24\n25\n26\n27\nclass\nConsoleSpanExporter\n(\nTracingExporter\n):\n\"\"\"Prints the traces and spans to the console.\"\"\"\ndef\nexport\n(\nself\n,\nitems\n:\nlist\n[\nTrace\n|\nSpan\n[\nAny\n]])\n->\nNone\n:\nfor\nitem\nin\nitems\n:\nif\nisinstance\n(\nitem\n,\nTrace\n):\nprint\n(\nf\n\"[Exporter] Export trace_id=\n{\nitem\n.\ntrace_id\n}\n, name=\n{\nitem\n.\nname\n}\n, \"\n)\nelse\n:\nprint\n(\nf\n\"[Exporter] Export span:\n{\nitem\n.\nexport\n()\n}\n\"\n)\nBackendSpanExporter\nBases:\nTracingExporter\nSource code in\nsrc/agents/tracing/processors.py\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\nclass\nBackendSpanExporter\n(\nTracingExporter\n):\ndef\n__init__\n(\nself\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\nendpoint\n:\nstr\n=\n\"https://api.openai.com/v1/traces/ingest\"\n,\nmax_retries\n:\nint\n=\n3\n,\nbase_delay\n:\nfloat\n=\n1.0\n,\nmax_delay\n:\nfloat\n=\n30.0\n,\n):\n\"\"\"\nArgs:\napi_key: The API key for the \"Authorization\" header. Defaults to\n`os.environ[\"OPENAI_API_KEY\"]` if not provided.\norganization: The OpenAI organization to use. Defaults to\n`os.environ[\"OPENAI_ORG_ID\"]` if not provided.\nproject: The OpenAI project to use. Defaults to\n`os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\nendpoint: The HTTP endpoint to which traces/spans are posted.\nmax_retries: Maximum number of retries upon failures.\nbase_delay: Base delay (in seconds) for the first backoff.\nmax_delay: Maximum delay (in seconds) for backoff growth.\n\"\"\"\nself\n.\n_api_key\n=\napi_key\nself\n.\n_organization\n=\norganization\nself\n.\n_project\n=\nproject\nself\n.\nendpoint\n=\nendpoint\nself\n.\nmax_retries\n=\nmax_retries\nself\n.\nbase_delay\n=\nbase_delay\nself\n.\nmax_delay\n=\nmax_delay\n# Keep a client open for connection pooling across multiple export calls\nself\n.\n_client\n=\nhttpx\n.\nClient\n(\ntimeout\n=\nhttpx\n.\nTimeout\n(\ntimeout\n=\n60\n,\nconnect\n=\n5.0\n))\ndef\nset_api_key\n(\nself\n,\napi_key\n:\nstr\n):\n\"\"\"Set the OpenAI API key for the exporter.\nArgs:\napi_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\nclient.\n\"\"\"\n# We're specifically setting the underlying cached property as well\nself\n.\n_api_key\n=\napi_key\nself\n.\napi_key\n=\napi_key\n@cached_property\ndef\napi_key\n(\nself\n):\nreturn\nself\n.\n_api_key\nor\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_API_KEY\"\n)\n@cached_property\ndef\norganization\n(\nself\n):\nreturn\nself\n.\n_organization\nor\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_ORG_ID\"\n)\n@cached_property\ndef\nproject\n(\nself\n):\nreturn\nself\n.\n_project\nor\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_PROJECT_ID\"\n)\ndef\nexport\n(\nself\n,\nitems\n:\nlist\n[\nTrace\n|\nSpan\n[\nAny\n]])\n->\nNone\n:\nif\nnot\nitems\n:\nreturn\nif\nnot\nself\n.\napi_key\n:\nlogger\n.\nwarning\n(\n\"OPENAI_API_KEY is not set, skipping trace export\"\n)\nreturn\ndata\n=\n[\nitem\n.\nexport\n()\nfor\nitem\nin\nitems\nif\nitem\n.\nexport\n()]\npayload\n=\n{\n\"data\"\n:\ndata\n}\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Bearer\n{\nself\n.\napi_key\n}\n\"\n,\n\"Content-Type\"\n:\n\"application/json\"\n,\n\"OpenAI-Beta\"\n:\n\"traces=v1\"\n,\n}\n# Exponential backoff loop\nattempt\n=\n0\ndelay\n=\nself\n.\nbase_delay\nwhile\nTrue\n:\nattempt\n+=\n1\ntry\n:\nresponse\n=\nself\n.\n_client\n.\npost\n(\nurl\n=\nself\n.\nendpoint\n,\nheaders\n=\nheaders\n,\njson\n=\npayload\n)\n# If the response is successful, break out of the loop\nif\nresponse\n.\nstatus_code\n<\n300\n:\nlogger\n.\ndebug\n(\nf\n\"Exported\n{\nlen\n(\nitems\n)\n}\nitems\"\n)\nreturn\n# If the response is a client error (4xx), we wont retry\nif\n400\n<=\nresponse\n.\nstatus_code\n<\n500\n:\nlogger\n.\nerror\n(\nf\n\"[non-fatal] Tracing client error\n{\nresponse\n.\nstatus_code\n}\n:\n{\nresponse\n.\ntext\n}\n\"\n)\nreturn\n# For 5xx or other unexpected codes, treat it as transient and retry\nlogger\n.\nwarning\n(\nf\n\"[non-fatal] Tracing: server error\n{\nresponse\n.\nstatus_code\n}\n, retrying.\"\n)\nexcept\nhttpx\n.\nRequestError\nas\nexc\n:\n# Network or other I/O error, we'll retry\nlogger\n.\nwarning\n(\nf\n\"[non-fatal] Tracing: request failed:\n{\nexc\n}\n\"\n)\n# If we reach here, we need to retry or give up\nif\nattempt\n>=\nself\n.\nmax_retries\n:\nlogger\n.\nerror\n(\n\"[non-fatal] Tracing: max retries reached, giving up on this batch.\"\n)\nreturn\n# Exponential backoff + jitter\nsleep_time\n=\ndelay\n+\nrandom\n.\nuniform\n(\n0\n,\n0.1\n*\ndelay\n)\n# 10% jitter\ntime\n.\nsleep\n(\nsleep_time\n)\ndelay\n=\nmin\n(\ndelay\n*\n2\n,\nself\n.\nmax_delay\n)\ndef\nclose\n(\nself\n):\n\"\"\"Close the underlying HTTP client.\"\"\"\nself\n.\n_client\n.\nclose\n()\n__init__\n__init__\n(\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\nendpoint\n:\nstr\n=\n\"https://api.openai.com/v1/traces/ingest\"\n,\nmax_retries\n:\nint\n=\n3\n,\nbase_delay\n:\nfloat\n=\n1.0\n,\nmax_delay\n:\nfloat\n=\n30.0\n,\n)\nParameters:\nName\nType\nDescription\nDefault\napi_key\nstr\n| None\nThe API key for the \"Authorization\" header. Defaults to\nos.environ[\"OPENAI_API_KEY\"]\nif not provided.\nNone\norganization\nstr\n| None\nThe OpenAI organization to use. Defaults to\nos.environ[\"OPENAI_ORG_ID\"]\nif not provided.\nNone\nproject\nstr\n| None\nThe OpenAI project to use. Defaults to\nos.environ[\"OPENAI_PROJECT_ID\"]\nif not provided.\nNone\nendpoint\nstr\nThe HTTP endpoint to which traces/spans are posted.\n'https://api.openai.com/v1/traces/ingest'\nmax_retries\nint\nMaximum number of retries upon failures.\n3\nbase_delay\nfloat\nBase delay (in seconds) for the first backoff.\n1.0\nmax_delay\nfloat\nMaximum delay (in seconds) for backoff growth.\n30.0\nSource code in\nsrc/agents/tracing/processors.py\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\ndef\n__init__\n(\nself\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\nendpoint\n:\nstr\n=\n\"https://api.openai.com/v1/traces/ingest\"\n,\nmax_retries\n:\nint\n=\n3\n,\nbase_delay\n:\nfloat\n=\n1.0\n,\nmax_delay\n:\nfloat\n=\n30.0\n,\n):\n\"\"\"\nArgs:\napi_key: The API key for the \"Authorization\" header. Defaults to\n`os.environ[\"OPENAI_API_KEY\"]` if not provided.\norganization: The OpenAI organization to use. Defaults to\n`os.environ[\"OPENAI_ORG_ID\"]` if not provided.\nproject: The OpenAI project to use. Defaults to\n`os.environ[\"OPENAI_PROJECT_ID\"]` if not provided.\nendpoint: The HTTP endpoint to which traces/spans are posted.\nmax_retries: Maximum number of retries upon failures.\nbase_delay: Base delay (in seconds) for the first backoff.\nmax_delay: Maximum delay (in seconds) for backoff growth.\n\"\"\"\nself\n.\n_api_key\n=\napi_key\nself\n.\n_organization\n=\norganization\nself\n.\n_project\n=\nproject\nself\n.\nendpoint\n=\nendpoint\nself\n.\nmax_retries\n=\nmax_retries\nself\n.\nbase_delay\n=\nbase_delay\nself\n.\nmax_delay\n=\nmax_delay\n# Keep a client open for connection pooling across multiple export calls\nself\n.\n_client\n=\nhttpx\n.\nClient\n(\ntimeout\n=\nhttpx\n.\nTimeout\n(\ntimeout\n=\n60\n,\nconnect\n=\n5.0\n))\nset_api_key\nset_api_key\n(\napi_key\n:\nstr\n)\nSet the OpenAI API key for the exporter.\nParameters:\nName\nType\nDescription\nDefault\napi_key\nstr\nThe OpenAI API key to use. This is the same key used by the OpenAI Python\nclient.\nrequired\nSource code in\nsrc/agents/tracing/processors.py\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\ndef\nset_api_key\n(\nself\n,\napi_key\n:\nstr\n):\n\"\"\"Set the OpenAI API key for the exporter.\nArgs:\napi_key: The OpenAI API key to use. This is the same key used by the OpenAI Python\nclient.\n\"\"\"\n# We're specifically setting the underlying cached property as well\nself\n.\n_api_key\n=\napi_key\nself\n.\napi_key\n=\napi_key\nclose\nclose\n()\nClose the underlying HTTP client.\nSource code in\nsrc/agents/tracing/processors.py\n143\n144\n145\ndef\nclose\n(\nself\n):\n\"\"\"Close the underlying HTTP client.\"\"\"\nself\n.\n_client\n.\nclose\n()\nBatchTraceProcessor\nBases:\nTracingProcessor\nSome implementation notes:\n1. Using Queue, which is thread-safe.\n2. Using a background thread to export spans, to minimize any performance issues.\n3. Spans are stored in memory until they are exported.\nSource code in\nsrc/agents/tracing/processors.py\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\nclass\nBatchTraceProcessor\n(\nTracingProcessor\n):\n\"\"\"Some implementation notes:\n1. Using Queue, which is thread-safe.\n2. Using a background thread to export spans, to minimize any performance issues.\n3. Spans are stored in memory until they are exported.\n\"\"\"\ndef\n__init__\n(\nself\n,\nexporter\n:\nTracingExporter\n,\nmax_queue_size\n:\nint\n=\n8192\n,\nmax_batch_size\n:\nint\n=\n128\n,\nschedule_delay\n:\nfloat\n=\n5.0\n,\nexport_trigger_ratio\n:\nfloat\n=\n0.7\n,\n):\n\"\"\"\nArgs:\nexporter: The exporter to use.\nmax_queue_size: The maximum number of spans to store in the queue. After this, we will\nstart dropping spans.\nmax_batch_size: The maximum number of spans to export in a single batch.\nschedule_delay: The delay between checks for new spans to export.\nexport_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n\"\"\"\nself\n.\n_exporter\n=\nexporter\nself\n.\n_queue\n:\nqueue\n.\nQueue\n[\nTrace\n|\nSpan\n[\nAny\n]]\n=\nqueue\n.\nQueue\n(\nmaxsize\n=\nmax_queue_size\n)\nself\n.\n_max_queue_size\n=\nmax_queue_size\nself\n.\n_max_batch_size\n=\nmax_batch_size\nself\n.\n_schedule_delay\n=\nschedule_delay\nself\n.\n_shutdown_event\n=\nthreading\n.\nEvent\n()\n# The queue size threshold at which we export immediately.\nself\n.\n_export_trigger_size\n=\nint\n(\nmax_queue_size\n*\nexport_trigger_ratio\n)\n# Track when we next *must* perform a scheduled export\nself\n.\n_next_export_time\n=\ntime\n.\ntime\n()\n+\nself\n.\n_schedule_delay\nself\n.\n_worker_thread\n=\nthreading\n.\nThread\n(\ntarget\n=\nself\n.\n_run\n,\ndaemon\n=\nTrue\n)\nself\n.\n_worker_thread\n.\nstart\n()\ndef\non_trace_start\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\ntry\n:\nself\n.\n_queue\n.\nput_nowait\n(\ntrace\n)\nexcept\nqueue\n.\nFull\n:\nlogger\n.\nwarning\n(\n\"Queue is full, dropping trace.\"\n)\ndef\non_trace_end\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\n# We send traces via on_trace_start, so we don't need to do anything here.\npass\ndef\non_span_start\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\n# We send spans via on_span_end, so we don't need to do anything here.\npass\ndef\non_span_end\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\ntry\n:\nself\n.\n_queue\n.\nput_nowait\n(\nspan\n)\nexcept\nqueue\n.\nFull\n:\nlogger\n.\nwarning\n(\n\"Queue is full, dropping span.\"\n)\ndef\nshutdown\n(\nself\n,\ntimeout\n:\nfloat\n|\nNone\n=\nNone\n):\n\"\"\"\nCalled when the application stops. We signal our thread to stop, then join it.\n\"\"\"\nself\n.\n_shutdown_event\n.\nset\n()\nself\n.\n_worker_thread\n.\njoin\n(\ntimeout\n=\ntimeout\n)\ndef\nforce_flush\n(\nself\n):\n\"\"\"\nForces an immediate flush of all queued spans.\n\"\"\"\nself\n.\n_export_batches\n(\nforce\n=\nTrue\n)\ndef\n_run\n(\nself\n):\nwhile\nnot\nself\n.\n_shutdown_event\n.\nis_set\n():\ncurrent_time\n=\ntime\n.\ntime\n()\nqueue_size\n=\nself\n.\n_queue\n.\nqsize\n()\n# If it's time for a scheduled flush or queue is above the trigger threshold\nif\ncurrent_time\n>=\nself\n.\n_next_export_time\nor\nqueue_size\n>=\nself\n.\n_export_trigger_size\n:\nself\n.\n_export_batches\n(\nforce\n=\nFalse\n)\n# Reset the next scheduled flush time\nself\n.\n_next_export_time\n=\ntime\n.\ntime\n()\n+\nself\n.\n_schedule_delay\nelse\n:\n# Sleep a short interval so we don't busy-wait.\ntime\n.\nsleep\n(\n0.2\n)\n# Final drain after shutdown\nself\n.\n_export_batches\n(\nforce\n=\nTrue\n)\ndef\n_export_batches\n(\nself\n,\nforce\n:\nbool\n=\nFalse\n):\n\"\"\"Drains the queue and exports in batches. If force=True, export everything.\nOtherwise, export up to `max_batch_size` repeatedly until the queue is empty or below a\ncertain threshold.\n\"\"\"\nwhile\nTrue\n:\nitems_to_export\n:\nlist\n[\nSpan\n[\nAny\n]\n|\nTrace\n]\n=\n[]\n# Gather a batch of spans up to max_batch_size\nwhile\nnot\nself\n.\n_queue\n.\nempty\n()\nand\n(\nforce\nor\nlen\n(\nitems_to_export\n)\n<\nself\n.\n_max_batch_size\n):\ntry\n:\nitems_to_export\n.\nappend\n(\nself\n.\n_queue\n.\nget_nowait\n())\nexcept\nqueue\n.\nEmpty\n:\n# Another thread might have emptied the queue between checks\nbreak\n# If we collected nothing, we're done\nif\nnot\nitems_to_export\n:\nbreak\n# Export the batch\nself\n.\n_exporter\n.\nexport\n(\nitems_to_export\n)\n__init__\n__init__\n(\nexporter\n:\nTracingExporter\n,\nmax_queue_size\n:\nint\n=\n8192\n,\nmax_batch_size\n:\nint\n=\n128\n,\nschedule_delay\n:\nfloat\n=\n5.0\n,\nexport_trigger_ratio\n:\nfloat\n=\n0.7\n,\n)\nParameters:\nName\nType\nDescription\nDefault\nexporter\nTracingExporter\nThe exporter to use.\nrequired\nmax_queue_size\nint\nThe maximum number of spans to store in the queue. After this, we will\nstart dropping spans.\n8192\nmax_batch_size\nint\nThe maximum number of spans to export in a single batch.\n128\nschedule_delay\nfloat\nThe delay between checks for new spans to export.\n5.0\nexport_trigger_ratio\nfloat\nThe ratio of the queue size at which we will trigger an export.\n0.7\nSource code in\nsrc/agents/tracing/processors.py\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\ndef\n__init__\n(\nself\n,\nexporter\n:\nTracingExporter\n,\nmax_queue_size\n:\nint\n=\n8192\n,\nmax_batch_size\n:\nint\n=\n128\n,\nschedule_delay\n:\nfloat\n=\n5.0\n,\nexport_trigger_ratio\n:\nfloat\n=\n0.7\n,\n):\n\"\"\"\nArgs:\nexporter: The exporter to use.\nmax_queue_size: The maximum number of spans to store in the queue. After this, we will\nstart dropping spans.\nmax_batch_size: The maximum number of spans to export in a single batch.\nschedule_delay: The delay between checks for new spans to export.\nexport_trigger_ratio: The ratio of the queue size at which we will trigger an export.\n\"\"\"\nself\n.\n_exporter\n=\nexporter\nself\n.\n_queue\n:\nqueue\n.\nQueue\n[\nTrace\n|\nSpan\n[\nAny\n]]\n=\nqueue\n.\nQueue\n(\nmaxsize\n=\nmax_queue_size\n)\nself\n.\n_max_queue_size\n=\nmax_queue_size\nself\n.\n_max_batch_size\n=\nmax_batch_size\nself\n.\n_schedule_delay\n=\nschedule_delay\nself\n.\n_shutdown_event\n=\nthreading\n.\nEvent\n()\n# The queue size threshold at which we export immediately.\nself\n.\n_export_trigger_size\n=\nint\n(\nmax_queue_size\n*\nexport_trigger_ratio\n)\n# Track when we next *must* perform a scheduled export\nself\n.\n_next_export_time\n=\ntime\n.\ntime\n()\n+\nself\n.\n_schedule_delay\nself\n.\n_worker_thread\n=\nthreading\n.\nThread\n(\ntarget\n=\nself\n.\n_run\n,\ndaemon\n=\nTrue\n)\nself\n.\n_worker_thread\n.\nstart\n()\nshutdown\nshutdown\n(\ntimeout\n:\nfloat\n|\nNone\n=\nNone\n)\nCalled when the application stops. We signal our thread to stop, then join it.\nSource code in\nsrc/agents/tracing/processors.py\n208\n209\n210\n211\n212\n213\ndef\nshutdown\n(\nself\n,\ntimeout\n:\nfloat\n|\nNone\n=\nNone\n):\n\"\"\"\nCalled when the application stops. We signal our thread to stop, then join it.\n\"\"\"\nself\n.\n_shutdown_event\n.\nset\n()\nself\n.\n_worker_thread\n.\njoin\n(\ntimeout\n=\ntimeout\n)\nforce_flush\nforce_flush\n()\nForces an immediate flush of all queued spans.\nSource code in\nsrc/agents/tracing/processors.py\n215\n216\n217\n218\n219\ndef\nforce_flush\n(\nself\n):\n\"\"\"\nForces an immediate flush of all queued spans.\n\"\"\"\nself\n.\n_export_batches\n(\nforce\n=\nTrue\n)\ndefault_exporter\ndefault_exporter\n()\n->\nBackendSpanExporter\nThe default exporter, which exports traces and spans to the backend in batches.\nSource code in\nsrc/agents/tracing/processors.py\n269\n270\n271\ndef\ndefault_exporter\n()\n->\nBackendSpanExporter\n:\n\"\"\"The default exporter, which exports traces and spans to the backend in batches.\"\"\"\nreturn\n_global_exporter\ndefault_processor\ndefault_processor\n()\n->\nBatchTraceProcessor\nThe default processor, which exports traces and spans to the backend in batches.\nSource code in\nsrc/agents/tracing/processors.py\n274\n275\n276\ndef\ndefault_processor\n()\n->\nBatchTraceProcessor\n:\n\"\"\"The default processor, which exports traces and spans to the backend in batches.\"\"\"\nreturn\n_global_processor",
  "Scope": "Scope\nScope\nManages the current span and trace in the context.\nSource code in\nsrc/agents/tracing/scope.py\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nclass\nScope\n:\n\"\"\"\nManages the current span and trace in the context.\n\"\"\"\n@classmethod\ndef\nget_current_span\n(\ncls\n)\n->\n\"Span[Any] | None\"\n:\nreturn\n_current_span\n.\nget\n()\n@classmethod\ndef\nset_current_span\n(\ncls\n,\nspan\n:\n\"Span[Any] | None\"\n)\n->\n\"contextvars.Token[Span[Any] | None]\"\n:\nreturn\n_current_span\n.\nset\n(\nspan\n)\n@classmethod\ndef\nreset_current_span\n(\ncls\n,\ntoken\n:\n\"contextvars.Token[Span[Any] | None]\"\n)\n->\nNone\n:\n_current_span\n.\nreset\n(\ntoken\n)\n@classmethod\ndef\nget_current_trace\n(\ncls\n)\n->\n\"Trace | None\"\n:\nreturn\n_current_trace\n.\nget\n()\n@classmethod\ndef\nset_current_trace\n(\ncls\n,\ntrace\n:\n\"Trace | None\"\n)\n->\n\"contextvars.Token[Trace | None]\"\n:\nlogger\n.\ndebug\n(\nf\n\"Setting current trace:\n{\ntrace\n.\ntrace_id\nif\ntrace\nelse\nNone\n}\n\"\n)\nreturn\n_current_trace\n.\nset\n(\ntrace\n)\n@classmethod\ndef\nreset_current_trace\n(\ncls\n,\ntoken\n:\n\"contextvars.Token[Trace | None]\"\n)\n->\nNone\n:\nlogger\n.\ndebug\n(\n\"Resetting current trace\"\n)\n_current_trace\n.\nreset\n(\ntoken\n)",
  "Setup": "Setup\nSynchronousMultiTracingProcessor\nBases:\nTracingProcessor\nForwards all calls to a list of TracingProcessors, in order of registration.\nSource code in\nsrc/agents/tracing/setup.py\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\nclass\nSynchronousMultiTracingProcessor\n(\nTracingProcessor\n):\n\"\"\"\nForwards all calls to a list of TracingProcessors, in order of registration.\n\"\"\"\ndef\n__init__\n(\nself\n):\n# Using a tuple to avoid race conditions when iterating over processors\nself\n.\n_processors\n:\ntuple\n[\nTracingProcessor\n,\n...\n]\n=\n()\nself\n.\n_lock\n=\nthreading\n.\nLock\n()\ndef\nadd_tracing_processor\n(\nself\n,\ntracing_processor\n:\nTracingProcessor\n):\n\"\"\"\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\n\"\"\"\nwith\nself\n.\n_lock\n:\nself\n.\n_processors\n+=\n(\ntracing_processor\n,)\ndef\nset_processors\n(\nself\n,\nprocessors\n:\nlist\n[\nTracingProcessor\n]):\n\"\"\"\nSet the list of processors. This will replace the current list of processors.\n\"\"\"\nwith\nself\n.\n_lock\n:\nself\n.\n_processors\n=\ntuple\n(\nprocessors\n)\ndef\non_trace_start\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\n\"\"\"\nCalled when a trace is started.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_trace_start\n(\ntrace\n)\ndef\non_trace_end\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\n\"\"\"\nCalled when a trace is finished.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_trace_end\n(\ntrace\n)\ndef\non_span_start\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\n\"\"\"\nCalled when a span is started.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_span_start\n(\nspan\n)\ndef\non_span_end\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\n\"\"\"\nCalled when a span is finished.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_span_end\n(\nspan\n)\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"\nCalled when the application stops.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nlogger\n.\ndebug\n(\nf\n\"Shutting down trace processor\n{\nprocessor\n}\n\"\n)\nprocessor\n.\nshutdown\n()\ndef\nforce_flush\n(\nself\n):\n\"\"\"\nForce the processors to flush their buffers.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\nforce_flush\n()\nadd_tracing_processor\nadd_tracing_processor\n(\ntracing_processor\n:\nTracingProcessor\n)\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\nSource code in\nsrc/agents/tracing/setup.py\n25\n26\n27\n28\n29\n30\ndef\nadd_tracing_processor\n(\nself\n,\ntracing_processor\n:\nTracingProcessor\n):\n\"\"\"\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\n\"\"\"\nwith\nself\n.\n_lock\n:\nself\n.\n_processors\n+=\n(\ntracing_processor\n,)\nset_processors\nset_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n])\nSet the list of processors. This will replace the current list of processors.\nSource code in\nsrc/agents/tracing/setup.py\n32\n33\n34\n35\n36\n37\ndef\nset_processors\n(\nself\n,\nprocessors\n:\nlist\n[\nTracingProcessor\n]):\n\"\"\"\nSet the list of processors. This will replace the current list of processors.\n\"\"\"\nwith\nself\n.\n_lock\n:\nself\n.\n_processors\n=\ntuple\n(\nprocessors\n)\non_trace_start\non_trace_start\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is started.\nSource code in\nsrc/agents/tracing/setup.py\n39\n40\n41\n42\n43\n44\ndef\non_trace_start\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\n\"\"\"\nCalled when a trace is started.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_trace_start\n(\ntrace\n)\non_trace_end\non_trace_end\n(\ntrace\n:\nTrace\n)\n->\nNone\nCalled when a trace is finished.\nSource code in\nsrc/agents/tracing/setup.py\n46\n47\n48\n49\n50\n51\ndef\non_trace_end\n(\nself\n,\ntrace\n:\nTrace\n)\n->\nNone\n:\n\"\"\"\nCalled when a trace is finished.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_trace_end\n(\ntrace\n)\non_span_start\non_span_start\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is started.\nSource code in\nsrc/agents/tracing/setup.py\n53\n54\n55\n56\n57\n58\ndef\non_span_start\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\n\"\"\"\nCalled when a span is started.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_span_start\n(\nspan\n)\non_span_end\non_span_end\n(\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\nCalled when a span is finished.\nSource code in\nsrc/agents/tracing/setup.py\n60\n61\n62\n63\n64\n65\ndef\non_span_end\n(\nself\n,\nspan\n:\nSpan\n[\nAny\n])\n->\nNone\n:\n\"\"\"\nCalled when a span is finished.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\non_span_end\n(\nspan\n)\nshutdown\nshutdown\n()\n->\nNone\nCalled when the application stops.\nSource code in\nsrc/agents/tracing/setup.py\n67\n68\n69\n70\n71\n72\n73\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\n\"\"\"\nCalled when the application stops.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nlogger\n.\ndebug\n(\nf\n\"Shutting down trace processor\n{\nprocessor\n}\n\"\n)\nprocessor\n.\nshutdown\n()\nforce_flush\nforce_flush\n()\nForce the processors to flush their buffers.\nSource code in\nsrc/agents/tracing/setup.py\n75\n76\n77\n78\n79\n80\ndef\nforce_flush\n(\nself\n):\n\"\"\"\nForce the processors to flush their buffers.\n\"\"\"\nfor\nprocessor\nin\nself\n.\n_processors\n:\nprocessor\n.\nforce_flush\n()\nTraceProvider\nSource code in\nsrc/agents/tracing/setup.py\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\nclass\nTraceProvider\n:\ndef\n__init__\n(\nself\n):\nself\n.\n_multi_processor\n=\nSynchronousMultiTracingProcessor\n()\nself\n.\n_disabled\n=\nos\n.\nenviron\n.\nget\n(\n\"OPENAI_AGENTS_DISABLE_TRACING\"\n,\n\"false\"\n)\n.\nlower\n()\nin\n(\n\"true\"\n,\n\"1\"\n,\n)\ndef\nregister_processor\n(\nself\n,\nprocessor\n:\nTracingProcessor\n):\n\"\"\"\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\n\"\"\"\nself\n.\n_multi_processor\n.\nadd_tracing_processor\n(\nprocessor\n)\ndef\nset_processors\n(\nself\n,\nprocessors\n:\nlist\n[\nTracingProcessor\n]):\n\"\"\"\nSet the list of processors. This will replace the current list of processors.\n\"\"\"\nself\n.\n_multi_processor\n.\nset_processors\n(\nprocessors\n)\ndef\nget_current_trace\n(\nself\n)\n->\nTrace\n|\nNone\n:\n\"\"\"\nReturns the currently active trace, if any.\n\"\"\"\nreturn\nScope\n.\nget_current_trace\n()\ndef\nget_current_span\n(\nself\n)\n->\nSpan\n[\nAny\n]\n|\nNone\n:\n\"\"\"\nReturns the currently active span, if any.\n\"\"\"\nreturn\nScope\n.\nget_current_span\n()\ndef\nset_disabled\n(\nself\n,\ndisabled\n:\nbool\n)\n->\nNone\n:\n\"\"\"\nSet whether tracing is disabled.\n\"\"\"\nself\n.\n_disabled\n=\ndisabled\ndef\ncreate_trace\n(\nself\n,\nname\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\n:\n\"\"\"\nCreate a new trace.\n\"\"\"\nif\nself\n.\n_disabled\nor\ndisabled\n:\nlogger\n.\ndebug\n(\nf\n\"Tracing is disabled. Not creating trace\n{\nname\n}\n\"\n)\nreturn\nNoOpTrace\n()\ntrace_id\n=\ntrace_id\nor\nutil\n.\ngen_trace_id\n()\nlogger\n.\ndebug\n(\nf\n\"Creating trace\n{\nname\n}\nwith id\n{\ntrace_id\n}\n\"\n)\nreturn\nTraceImpl\n(\nname\n=\nname\n,\ntrace_id\n=\ntrace_id\n,\ngroup_id\n=\ngroup_id\n,\nmetadata\n=\nmetadata\n,\nprocessor\n=\nself\n.\n_multi_processor\n,\n)\ndef\ncreate_span\n(\nself\n,\nspan_data\n:\nTSpanData\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTSpanData\n]:\n\"\"\"\nCreate a new span.\n\"\"\"\nif\nself\n.\n_disabled\nor\ndisabled\n:\nlogger\n.\ndebug\n(\nf\n\"Tracing is disabled. Not creating span\n{\nspan_data\n}\n\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nif\nnot\nparent\n:\ncurrent_span\n=\nScope\n.\nget_current_span\n()\ncurrent_trace\n=\nScope\n.\nget_current_trace\n()\nif\ncurrent_trace\nis\nNone\n:\nlogger\n.\nerror\n(\n\"No active trace. Make sure to start a trace with `trace()` first\"\n\"Returning NoOpSpan.\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nelif\nisinstance\n(\ncurrent_trace\n,\nNoOpTrace\n)\nor\nisinstance\n(\ncurrent_span\n,\nNoOpSpan\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\ncurrent_span\n}\nor\n{\ncurrent_trace\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nparent_id\n=\ncurrent_span\n.\nspan_id\nif\ncurrent_span\nelse\nNone\ntrace_id\n=\ncurrent_trace\n.\ntrace_id\nelif\nisinstance\n(\nparent\n,\nTrace\n):\nif\nisinstance\n(\nparent\n,\nNoOpTrace\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\nparent\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\ntrace_id\n=\nparent\n.\ntrace_id\nparent_id\n=\nNone\nelif\nisinstance\n(\nparent\n,\nSpan\n):\nif\nisinstance\n(\nparent\n,\nNoOpSpan\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\nparent\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nparent_id\n=\nparent\n.\nspan_id\ntrace_id\n=\nparent\n.\ntrace_id\nlogger\n.\ndebug\n(\nf\n\"Creating span\n{\nspan_data\n}\nwith id\n{\nspan_id\n}\n\"\n)\nreturn\nSpanImpl\n(\ntrace_id\n=\ntrace_id\n,\nspan_id\n=\nspan_id\n,\nparent_id\n=\nparent_id\n,\nprocessor\n=\nself\n.\n_multi_processor\n,\nspan_data\n=\nspan_data\n,\n)\ndef\nshutdown\n(\nself\n)\n->\nNone\n:\nif\nself\n.\n_disabled\n:\nreturn\ntry\n:\nlogger\n.\ndebug\n(\n\"Shutting down trace provider\"\n)\nself\n.\n_multi_processor\n.\nshutdown\n()\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error shutting down trace provider:\n{\ne\n}\n\"\n)\nregister_processor\nregister_processor\n(\nprocessor\n:\nTracingProcessor\n)\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\nSource code in\nsrc/agents/tracing/setup.py\n91\n92\n93\n94\n95\ndef\nregister_processor\n(\nself\n,\nprocessor\n:\nTracingProcessor\n):\n\"\"\"\nAdd a processor to the list of processors. Each processor will receive all traces/spans.\n\"\"\"\nself\n.\n_multi_processor\n.\nadd_tracing_processor\n(\nprocessor\n)\nset_processors\nset_processors\n(\nprocessors\n:\nlist\n[\nTracingProcessor\n])\nSet the list of processors. This will replace the current list of processors.\nSource code in\nsrc/agents/tracing/setup.py\n97\n98\n99\n100\n101\ndef\nset_processors\n(\nself\n,\nprocessors\n:\nlist\n[\nTracingProcessor\n]):\n\"\"\"\nSet the list of processors. This will replace the current list of processors.\n\"\"\"\nself\n.\n_multi_processor\n.\nset_processors\n(\nprocessors\n)\nget_current_trace\nget_current_trace\n()\n->\nTrace\n|\nNone\nReturns the currently active trace, if any.\nSource code in\nsrc/agents/tracing/setup.py\n103\n104\n105\n106\n107\ndef\nget_current_trace\n(\nself\n)\n->\nTrace\n|\nNone\n:\n\"\"\"\nReturns the currently active trace, if any.\n\"\"\"\nreturn\nScope\n.\nget_current_trace\n()\nget_current_span\nget_current_span\n()\n->\nSpan\n[\nAny\n]\n|\nNone\nReturns the currently active span, if any.\nSource code in\nsrc/agents/tracing/setup.py\n109\n110\n111\n112\n113\ndef\nget_current_span\n(\nself\n)\n->\nSpan\n[\nAny\n]\n|\nNone\n:\n\"\"\"\nReturns the currently active span, if any.\n\"\"\"\nreturn\nScope\n.\nget_current_span\n()\nset_disabled\nset_disabled\n(\ndisabled\n:\nbool\n)\n->\nNone\nSet whether tracing is disabled.\nSource code in\nsrc/agents/tracing/setup.py\n115\n116\n117\n118\n119\ndef\nset_disabled\n(\nself\n,\ndisabled\n:\nbool\n)\n->\nNone\n:\n\"\"\"\nSet whether tracing is disabled.\n\"\"\"\nself\n.\n_disabled\n=\ndisabled\ncreate_trace\ncreate_trace\n(\nname\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\nCreate a new trace.\nSource code in\nsrc/agents/tracing/setup.py\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\ndef\ncreate_trace\n(\nself\n,\nname\n:\nstr\n,\ntrace_id\n:\nstr\n|\nNone\n=\nNone\n,\ngroup_id\n:\nstr\n|\nNone\n=\nNone\n,\nmetadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nTrace\n:\n\"\"\"\nCreate a new trace.\n\"\"\"\nif\nself\n.\n_disabled\nor\ndisabled\n:\nlogger\n.\ndebug\n(\nf\n\"Tracing is disabled. Not creating trace\n{\nname\n}\n\"\n)\nreturn\nNoOpTrace\n()\ntrace_id\n=\ntrace_id\nor\nutil\n.\ngen_trace_id\n()\nlogger\n.\ndebug\n(\nf\n\"Creating trace\n{\nname\n}\nwith id\n{\ntrace_id\n}\n\"\n)\nreturn\nTraceImpl\n(\nname\n=\nname\n,\ntrace_id\n=\ntrace_id\n,\ngroup_id\n=\ngroup_id\n,\nmetadata\n=\nmetadata\n,\nprocessor\n=\nself\n.\n_multi_processor\n,\n)\ncreate_span\ncreate_span\n(\nspan_data\n:\nTSpanData\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTSpanData\n]\nCreate a new span.\nSource code in\nsrc/agents/tracing/setup.py\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\ndef\ncreate_span\n(\nself\n,\nspan_data\n:\nTSpanData\n,\nspan_id\n:\nstr\n|\nNone\n=\nNone\n,\nparent\n:\nTrace\n|\nSpan\n[\nAny\n]\n|\nNone\n=\nNone\n,\ndisabled\n:\nbool\n=\nFalse\n,\n)\n->\nSpan\n[\nTSpanData\n]:\n\"\"\"\nCreate a new span.\n\"\"\"\nif\nself\n.\n_disabled\nor\ndisabled\n:\nlogger\n.\ndebug\n(\nf\n\"Tracing is disabled. Not creating span\n{\nspan_data\n}\n\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nif\nnot\nparent\n:\ncurrent_span\n=\nScope\n.\nget_current_span\n()\ncurrent_trace\n=\nScope\n.\nget_current_trace\n()\nif\ncurrent_trace\nis\nNone\n:\nlogger\n.\nerror\n(\n\"No active trace. Make sure to start a trace with `trace()` first\"\n\"Returning NoOpSpan.\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nelif\nisinstance\n(\ncurrent_trace\n,\nNoOpTrace\n)\nor\nisinstance\n(\ncurrent_span\n,\nNoOpSpan\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\ncurrent_span\n}\nor\n{\ncurrent_trace\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nparent_id\n=\ncurrent_span\n.\nspan_id\nif\ncurrent_span\nelse\nNone\ntrace_id\n=\ncurrent_trace\n.\ntrace_id\nelif\nisinstance\n(\nparent\n,\nTrace\n):\nif\nisinstance\n(\nparent\n,\nNoOpTrace\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\nparent\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\ntrace_id\n=\nparent\n.\ntrace_id\nparent_id\n=\nNone\nelif\nisinstance\n(\nparent\n,\nSpan\n):\nif\nisinstance\n(\nparent\n,\nNoOpSpan\n):\nlogger\n.\ndebug\n(\nf\n\"Parent\n{\nparent\n}\nis no-op, returning NoOpSpan\"\n)\nreturn\nNoOpSpan\n(\nspan_data\n)\nparent_id\n=\nparent\n.\nspan_id\ntrace_id\n=\nparent\n.\ntrace_id\nlogger\n.\ndebug\n(\nf\n\"Creating span\n{\nspan_data\n}\nwith id\n{\nspan_id\n}\n\"\n)\nreturn\nSpanImpl\n(\ntrace_id\n=\ntrace_id\n,\nspan_id\n=\nspan_id\n,\nparent_id\n=\nparent_id\n,\nprocessor\n=\nself\n.\n_multi_processor\n,\nspan_data\n=\nspan_data\n,\n)",
  "Span data": "Span data\nSpanData\nBases:\nABC\nRepresents span data in the trace.\nSource code in\nsrc/agents/tracing/span_data.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nclass\nSpanData\n(\nabc\n.\nABC\n):\n\"\"\"\nRepresents span data in the trace.\n\"\"\"\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Export the span data as a dictionary.\"\"\"\npass\n@property\n@abc\n.\nabstractmethod\ndef\ntype\n(\nself\n)\n->\nstr\n:\n\"\"\"Return the type of the span.\"\"\"\npass\ntype\nabstractmethod\nproperty\ntype\n:\nstr\nReturn the type of the span.\nexport\nabstractmethod\nexport\n()\n->\ndict\n[\nstr\n,\nAny\n]\nExport the span data as a dictionary.\nSource code in\nsrc/agents/tracing/span_data.py\n16\n17\n18\n19\n@abc\n.\nabstractmethod\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n\"\"\"Export the span data as a dictionary.\"\"\"\npass\nAgentSpanData\nBases:\nSpanData\nRepresents an Agent Span in the trace.\nIncludes name, handoffs, tools, and output type.\nSource code in\nsrc/agents/tracing/span_data.py\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\nclass\nAgentSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents an Agent Span in the trace.\nIncludes name, handoffs, tools, and output type.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"handoffs\"\n,\n\"tools\"\n,\n\"output_type\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n,\noutput_type\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\nname\n=\nname\nself\n.\nhandoffs\n:\nlist\n[\nstr\n]\n|\nNone\n=\nhandoffs\nself\n.\ntools\n:\nlist\n[\nstr\n]\n|\nNone\n=\ntools\nself\n.\noutput_type\n:\nstr\n|\nNone\n=\noutput_type\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"agent\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"handoffs\"\n:\nself\n.\nhandoffs\n,\n\"tools\"\n:\nself\n.\ntools\n,\n\"output_type\"\n:\nself\n.\noutput_type\n,\n}\nFunctionSpanData\nBases:\nSpanData\nRepresents a Function Span in the trace.\nIncludes input, output and MCP data (if applicable).\nSource code in\nsrc/agents/tracing/span_data.py\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nclass\nFunctionSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Function Span in the trace.\nIncludes input, output and MCP data (if applicable).\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"input\"\n,\n\"output\"\n,\n\"mcp_data\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ninput\n:\nstr\n|\nNone\n,\noutput\n:\nAny\n|\nNone\n,\nmcp_data\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\nname\n=\nname\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\nmcp_data\n=\nmcp_data\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"function\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\nstr\n(\nself\n.\noutput\n)\nif\nself\n.\noutput\nelse\nNone\n,\n\"mcp_data\"\n:\nself\n.\nmcp_data\n,\n}\nGenerationSpanData\nBases:\nSpanData\nRepresents a Generation Span in the trace.\nIncludes input, output, model, model configuration, and usage.\nSource code in\nsrc/agents/tracing/span_data.py\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\nclass\nGenerationSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Generation Span in the trace.\nIncludes input, output, model, model configuration, and usage.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n\"usage\"\n,\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\noutput\n:\nSequence\n[\nMapping\n[\nstr\n,\nAny\n]]\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nusage\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\nself\n.\nusage\n=\nusage\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"generation\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\nself\n.\noutput\n,\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n\"usage\"\n:\nself\n.\nusage\n,\n}\nResponseSpanData\nBases:\nSpanData\nRepresents a Response Span in the trace.\nIncludes response and input.\nSource code in\nsrc/agents/tracing/span_data.py\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\nclass\nResponseSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Response Span in the trace.\nIncludes response and input.\n\"\"\"\n__slots__\n=\n(\n\"response\"\n,\n\"input\"\n)\ndef\n__init__\n(\nself\n,\nresponse\n:\nResponse\n|\nNone\n=\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nResponseInputItemParam\n]\n|\nNone\n=\nNone\n,\n)\n->\nNone\n:\nself\n.\nresponse\n=\nresponse\n# This is not used by the OpenAI trace processors, but is useful for other tracing\n# processor implementations\nself\n.\ninput\n=\ninput\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"response\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"response_id\"\n:\nself\n.\nresponse\n.\nid\nif\nself\n.\nresponse\nelse\nNone\n,\n}\nHandoffSpanData\nBases:\nSpanData\nRepresents a Handoff Span in the trace.\nIncludes source and destination agents.\nSource code in\nsrc/agents/tracing/span_data.py\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\nclass\nHandoffSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Handoff Span in the trace.\nIncludes source and destination agents.\n\"\"\"\n__slots__\n=\n(\n\"from_agent\"\n,\n\"to_agent\"\n)\ndef\n__init__\n(\nself\n,\nfrom_agent\n:\nstr\n|\nNone\n,\nto_agent\n:\nstr\n|\nNone\n):\nself\n.\nfrom_agent\n=\nfrom_agent\nself\n.\nto_agent\n=\nto_agent\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"handoff\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"from_agent\"\n:\nself\n.\nfrom_agent\n,\n\"to_agent\"\n:\nself\n.\nto_agent\n,\n}\nCustomSpanData\nBases:\nSpanData\nRepresents a Custom Span in the trace.\nIncludes name and data property bag.\nSource code in\nsrc/agents/tracing/span_data.py\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\nclass\nCustomSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Custom Span in the trace.\nIncludes name and data property bag.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"data\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ndata\n:\ndict\n[\nstr\n,\nAny\n]):\nself\n.\nname\n=\nname\nself\n.\ndata\n=\ndata\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"custom\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"data\"\n:\nself\n.\ndata\n,\n}\nGuardrailSpanData\nBases:\nSpanData\nRepresents a Guardrail Span in the trace.\nIncludes name and triggered status.\nSource code in\nsrc/agents/tracing/span_data.py\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\nclass\nGuardrailSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Guardrail Span in the trace.\nIncludes name and triggered status.\n\"\"\"\n__slots__\n=\n(\n\"name\"\n,\n\"triggered\"\n)\ndef\n__init__\n(\nself\n,\nname\n:\nstr\n,\ntriggered\n:\nbool\n=\nFalse\n):\nself\n.\nname\n=\nname\nself\n.\ntriggered\n=\ntriggered\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"guardrail\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"name\"\n:\nself\n.\nname\n,\n\"triggered\"\n:\nself\n.\ntriggered\n,\n}\nTranscriptionSpanData\nBases:\nSpanData\nRepresents a Transcription Span in the trace.\nIncludes input, output, model, and model configuration.\nSource code in\nsrc/agents/tracing/span_data.py\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\nclass\nTranscriptionSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Transcription Span in the trace.\nIncludes input, output, model, and model configuration.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\ninput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\ninput_format\n=\ninput_format\nself\n.\noutput\n=\noutput\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"transcription\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\n{\n\"data\"\n:\nself\n.\ninput\nor\n\"\"\n,\n\"format\"\n:\nself\n.\ninput_format\n,\n},\n\"output\"\n:\nself\n.\noutput\n,\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n}\nSpeechSpanData\nBases:\nSpanData\nRepresents a Speech Span in the trace.\nIncludes input, output, model, model configuration, and first content timestamp.\nSource code in\nsrc/agents/tracing/span_data.py\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\nclass\nSpeechSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Speech Span in the trace.\nIncludes input, output, model, model configuration, and first content timestamp.\n\"\"\"\n__slots__\n=\n(\n\"input\"\n,\n\"output\"\n,\n\"model\"\n,\n\"model_config\"\n,\n\"first_content_at\"\n)\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\noutput\n:\nstr\n|\nNone\n=\nNone\n,\noutput_format\n:\nstr\n|\nNone\n=\n\"pcm\"\n,\nmodel\n:\nstr\n|\nNone\n=\nNone\n,\nmodel_config\n:\nMapping\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n,\nfirst_content_at\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\nself\n.\noutput\n=\noutput\nself\n.\noutput_format\n=\noutput_format\nself\n.\nmodel\n=\nmodel\nself\n.\nmodel_config\n=\nmodel_config\nself\n.\nfirst_content_at\n=\nfirst_content_at\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"speech\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n\"output\"\n:\n{\n\"data\"\n:\nself\n.\noutput\nor\n\"\"\n,\n\"format\"\n:\nself\n.\noutput_format\n,\n},\n\"model\"\n:\nself\n.\nmodel\n,\n\"model_config\"\n:\nself\n.\nmodel_config\n,\n\"first_content_at\"\n:\nself\n.\nfirst_content_at\n,\n}\nSpeechGroupSpanData\nBases:\nSpanData\nRepresents a Speech Group Span in the trace.\nSource code in\nsrc/agents/tracing/span_data.py\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\nclass\nSpeechGroupSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents a Speech Group Span in the trace.\n\"\"\"\n__slots__\n=\n\"input\"\ndef\n__init__\n(\nself\n,\ninput\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\ninput\n=\ninput\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"speech-group\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"input\"\n:\nself\n.\ninput\n,\n}\nMCPListToolsSpanData\nBases:\nSpanData\nRepresents an MCP List Tools Span in the trace.\nIncludes server and result.\nSource code in\nsrc/agents/tracing/span_data.py\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\nclass\nMCPListToolsSpanData\n(\nSpanData\n):\n\"\"\"\nRepresents an MCP List Tools Span in the trace.\nIncludes server and result.\n\"\"\"\n__slots__\n=\n(\n\"server\"\n,\n\"result\"\n,\n)\ndef\n__init__\n(\nself\n,\nserver\n:\nstr\n|\nNone\n=\nNone\n,\nresult\n:\nlist\n[\nstr\n]\n|\nNone\n=\nNone\n):\nself\n.\nserver\n=\nserver\nself\n.\nresult\n=\nresult\n@property\ndef\ntype\n(\nself\n)\n->\nstr\n:\nreturn\n\"mcp_tools\"\ndef\nexport\n(\nself\n)\n->\ndict\n[\nstr\n,\nAny\n]:\nreturn\n{\n\"type\"\n:\nself\n.\ntype\n,\n\"server\"\n:\nself\n.\nserver\n,\n\"result\"\n:\nself\n.\nresult\n,\n}",
  "Util": "Util\ntime_iso\ntime_iso\n()\n->\nstr\nReturns the current time in ISO 8601 format.\nSource code in\nsrc/agents/tracing/util.py\n5\n6\n7\ndef\ntime_iso\n()\n->\nstr\n:\n\"\"\"Returns the current time in ISO 8601 format.\"\"\"\nreturn\ndatetime\n.\nnow\n(\ntimezone\n.\nutc\n)\n.\nisoformat\n()\ngen_trace_id\ngen_trace_id\n()\n->\nstr\nGenerates a new trace ID.\nSource code in\nsrc/agents/tracing/util.py\n10\n11\n12\ndef\ngen_trace_id\n()\n->\nstr\n:\n\"\"\"Generates a new trace ID.\"\"\"\nreturn\nf\n\"trace_\n{\nuuid\n.\nuuid4\n()\n.\nhex\n}\n\"\ngen_span_id\ngen_span_id\n()\n->\nstr\nGenerates a new span ID.\nSource code in\nsrc/agents/tracing/util.py\n15\n16\n17\ndef\ngen_span_id\n()\n->\nstr\n:\n\"\"\"Generates a new span ID.\"\"\"\nreturn\nf\n\"span_\n{\nuuid\n.\nuuid4\n()\n.\nhex\n[:\n24\n]\n}\n\"\ngen_group_id\ngen_group_id\n()\n->\nstr\nGenerates a new group ID.\nSource code in\nsrc/agents/tracing/util.py\n20\n21\n22\ndef\ngen_group_id\n()\n->\nstr\n:\n\"\"\"Generates a new group ID.\"\"\"\nreturn\nf\n\"group_\n{\nuuid\n.\nuuid4\n()\n.\nhex\n[:\n24\n]\n}\n\"",
  "Pipeline": "Pipeline\nVoicePipeline\nAn opinionated voice agent pipeline. It works in three steps:\n1. Transcribe audio input into text.\n2. Run the provided\nworkflow\n, which produces a sequence of text responses.\n3. Convert the text responses into streaming audio output.\nSource code in\nsrc/agents/voice/pipeline.py\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\nclass\nVoicePipeline\n:\n\"\"\"An opinionated voice agent pipeline. It works in three steps:\n1. Transcribe audio input into text.\n2. Run the provided `workflow`, which produces a sequence of text responses.\n3. Convert the text responses into streaming audio output.\n\"\"\"\ndef\n__init__\n(\nself\n,\n*\n,\nworkflow\n:\nVoiceWorkflowBase\n,\nstt_model\n:\nSTTModel\n|\nstr\n|\nNone\n=\nNone\n,\ntts_model\n:\nTTSModel\n|\nstr\n|\nNone\n=\nNone\n,\nconfig\n:\nVoicePipelineConfig\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new voice pipeline.\nArgs:\nworkflow: The workflow to run. See `VoiceWorkflowBase`.\nstt_model: The speech-to-text model to use. If not provided, a default OpenAI\nmodel will be used.\ntts_model: The text-to-speech model to use. If not provided, a default OpenAI\nmodel will be used.\nconfig: The pipeline configuration. If not provided, a default configuration will be\nused.\n\"\"\"\nself\n.\nworkflow\n=\nworkflow\nself\n.\nstt_model\n=\nstt_model\nif\nisinstance\n(\nstt_model\n,\nSTTModel\n)\nelse\nNone\nself\n.\ntts_model\n=\ntts_model\nif\nisinstance\n(\ntts_model\n,\nTTSModel\n)\nelse\nNone\nself\n.\n_stt_model_name\n=\nstt_model\nif\nisinstance\n(\nstt_model\n,\nstr\n)\nelse\nNone\nself\n.\n_tts_model_name\n=\ntts_model\nif\nisinstance\n(\ntts_model\n,\nstr\n)\nelse\nNone\nself\n.\nconfig\n=\nconfig\nor\nVoicePipelineConfig\n()\nasync\ndef\nrun\n(\nself\n,\naudio_input\n:\nAudioInput\n|\nStreamedAudioInput\n)\n->\nStreamedAudioResult\n:\n\"\"\"Run the voice pipeline.\nArgs:\naudio_input: The audio input to process. This can either be an `AudioInput` instance,\nwhich is a single static buffer, or a `StreamedAudioInput` instance, which is a\nstream of audio data that you can append to.\nReturns:\nA `StreamedAudioResult` instance. You can use this object to stream audio events and\nplay them out.\n\"\"\"\nif\nisinstance\n(\naudio_input\n,\nAudioInput\n):\nreturn\nawait\nself\n.\n_run_single_turn\n(\naudio_input\n)\nelif\nisinstance\n(\naudio_input\n,\nStreamedAudioInput\n):\nreturn\nawait\nself\n.\n_run_multi_turn\n(\naudio_input\n)\nelse\n:\nraise\nUserError\n(\nf\n\"Unsupported audio input type:\n{\ntype\n(\naudio_input\n)\n}\n\"\n)\ndef\n_get_tts_model\n(\nself\n)\n->\nTTSModel\n:\nif\nnot\nself\n.\ntts_model\n:\nself\n.\ntts_model\n=\nself\n.\nconfig\n.\nmodel_provider\n.\nget_tts_model\n(\nself\n.\n_tts_model_name\n)\nreturn\nself\n.\ntts_model\ndef\n_get_stt_model\n(\nself\n)\n->\nSTTModel\n:\nif\nnot\nself\n.\nstt_model\n:\nself\n.\nstt_model\n=\nself\n.\nconfig\n.\nmodel_provider\n.\nget_stt_model\n(\nself\n.\n_stt_model_name\n)\nreturn\nself\n.\nstt_model\nasync\ndef\n_process_audio_input\n(\nself\n,\naudio_input\n:\nAudioInput\n)\n->\nstr\n:\nmodel\n=\nself\n.\n_get_stt_model\n()\nreturn\nawait\nmodel\n.\ntranscribe\n(\naudio_input\n,\nself\n.\nconfig\n.\nstt_settings\n,\nself\n.\nconfig\n.\ntrace_include_sensitive_data\n,\nself\n.\nconfig\n.\ntrace_include_sensitive_audio_data\n,\n)\nasync\ndef\n_run_single_turn\n(\nself\n,\naudio_input\n:\nAudioInput\n)\n->\nStreamedAudioResult\n:\n# Since this is single turn, we can use the TraceCtxManager to manage starting/ending the\n# trace\nwith\nTraceCtxManager\n(\nworkflow_name\n=\nself\n.\nconfig\n.\nworkflow_name\nor\n\"Voice Agent\"\n,\ntrace_id\n=\nNone\n,\n# Automatically generated\ngroup_id\n=\nself\n.\nconfig\n.\ngroup_id\n,\nmetadata\n=\nself\n.\nconfig\n.\ntrace_metadata\n,\ndisabled\n=\nself\n.\nconfig\n.\ntracing_disabled\n,\n):\ninput_text\n=\nawait\nself\n.\n_process_audio_input\n(\naudio_input\n)\noutput\n=\nStreamedAudioResult\n(\nself\n.\n_get_tts_model\n(),\nself\n.\nconfig\n.\ntts_settings\n,\nself\n.\nconfig\n)\nasync\ndef\nstream_events\n():\ntry\n:\nasync\nfor\ntext_event\nin\nself\n.\nworkflow\n.\nrun\n(\ninput_text\n):\nawait\noutput\n.\n_add_text\n(\ntext_event\n)\nawait\noutput\n.\n_turn_done\n()\nawait\noutput\n.\n_done\n()\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error processing single turn:\n{\ne\n}\n\"\n)\nawait\noutput\n.\n_add_error\n(\ne\n)\nraise\ne\noutput\n.\n_set_task\n(\nasyncio\n.\ncreate_task\n(\nstream_events\n()))\nreturn\noutput\nasync\ndef\n_run_multi_turn\n(\nself\n,\naudio_input\n:\nStreamedAudioInput\n)\n->\nStreamedAudioResult\n:\nwith\nTraceCtxManager\n(\nworkflow_name\n=\nself\n.\nconfig\n.\nworkflow_name\nor\n\"Voice Agent\"\n,\ntrace_id\n=\nNone\n,\ngroup_id\n=\nself\n.\nconfig\n.\ngroup_id\n,\nmetadata\n=\nself\n.\nconfig\n.\ntrace_metadata\n,\ndisabled\n=\nself\n.\nconfig\n.\ntracing_disabled\n,\n):\noutput\n=\nStreamedAudioResult\n(\nself\n.\n_get_tts_model\n(),\nself\n.\nconfig\n.\ntts_settings\n,\nself\n.\nconfig\n)\ntranscription_session\n=\nawait\nself\n.\n_get_stt_model\n()\n.\ncreate_session\n(\naudio_input\n,\nself\n.\nconfig\n.\nstt_settings\n,\nself\n.\nconfig\n.\ntrace_include_sensitive_data\n,\nself\n.\nconfig\n.\ntrace_include_sensitive_audio_data\n,\n)\nasync\ndef\nprocess_turns\n():\ntry\n:\nasync\nfor\ninput_text\nin\ntranscription_session\n.\ntranscribe_turns\n():\nresult\n=\nself\n.\nworkflow\n.\nrun\n(\ninput_text\n)\nasync\nfor\ntext_event\nin\nresult\n:\nawait\noutput\n.\n_add_text\n(\ntext_event\n)\nawait\noutput\n.\n_turn_done\n()\nexcept\nException\nas\ne\n:\nlogger\n.\nerror\n(\nf\n\"Error processing turns:\n{\ne\n}\n\"\n)\nawait\noutput\n.\n_add_error\n(\ne\n)\nraise\ne\nfinally\n:\nawait\ntranscription_session\n.\nclose\n()\nawait\noutput\n.\n_done\n()\noutput\n.\n_set_task\n(\nasyncio\n.\ncreate_task\n(\nprocess_turns\n()))\nreturn\noutput\n__init__\n__init__\n(\n*\n,\nworkflow\n:\nVoiceWorkflowBase\n,\nstt_model\n:\nSTTModel\n|\nstr\n|\nNone\n=\nNone\n,\ntts_model\n:\nTTSModel\n|\nstr\n|\nNone\n=\nNone\n,\nconfig\n:\nVoicePipelineConfig\n|\nNone\n=\nNone\n,\n)\nCreate a new voice pipeline.\nParameters:\nName\nType\nDescription\nDefault\nworkflow\nVoiceWorkflowBase\nThe workflow to run. See\nVoiceWorkflowBase\n.\nrequired\nstt_model\nSTTModel\n|\nstr\n| None\nThe speech-to-text model to use. If not provided, a default OpenAI\nmodel will be used.\nNone\ntts_model\nTTSModel\n|\nstr\n| None\nThe text-to-speech model to use. If not provided, a default OpenAI\nmodel will be used.\nNone\nconfig\nVoicePipelineConfig\n| None\nThe pipeline configuration. If not provided, a default configuration will be\nused.\nNone\nSource code in\nsrc/agents/voice/pipeline.py\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\ndef\n__init__\n(\nself\n,\n*\n,\nworkflow\n:\nVoiceWorkflowBase\n,\nstt_model\n:\nSTTModel\n|\nstr\n|\nNone\n=\nNone\n,\ntts_model\n:\nTTSModel\n|\nstr\n|\nNone\n=\nNone\n,\nconfig\n:\nVoicePipelineConfig\n|\nNone\n=\nNone\n,\n):\n\"\"\"Create a new voice pipeline.\nArgs:\nworkflow: The workflow to run. See `VoiceWorkflowBase`.\nstt_model: The speech-to-text model to use. If not provided, a default OpenAI\nmodel will be used.\ntts_model: The text-to-speech model to use. If not provided, a default OpenAI\nmodel will be used.\nconfig: The pipeline configuration. If not provided, a default configuration will be\nused.\n\"\"\"\nself\n.\nworkflow\n=\nworkflow\nself\n.\nstt_model\n=\nstt_model\nif\nisinstance\n(\nstt_model\n,\nSTTModel\n)\nelse\nNone\nself\n.\ntts_model\n=\ntts_model\nif\nisinstance\n(\ntts_model\n,\nTTSModel\n)\nelse\nNone\nself\n.\n_stt_model_name\n=\nstt_model\nif\nisinstance\n(\nstt_model\n,\nstr\n)\nelse\nNone\nself\n.\n_tts_model_name\n=\ntts_model\nif\nisinstance\n(\ntts_model\n,\nstr\n)\nelse\nNone\nself\n.\nconfig\n=\nconfig\nor\nVoicePipelineConfig\n()\nrun\nasync\nrun\n(\naudio_input\n:\nAudioInput\n|\nStreamedAudioInput\n,\n)\n->\nStreamedAudioResult\nRun the voice pipeline.\nParameters:\nName\nType\nDescription\nDefault\naudio_input\nAudioInput\n|\nStreamedAudioInput\nThe audio input to process. This can either be an\nAudioInput\ninstance,\nwhich is a single static buffer, or a\nStreamedAudioInput\ninstance, which is a\nstream of audio data that you can append to.\nrequired\nReturns:\nType\nDescription\nStreamedAudioResult\nA\nStreamedAudioResult\ninstance. You can use this object to stream audio events and\nStreamedAudioResult\nplay them out.\nSource code in\nsrc/agents/voice/pipeline.py\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\nasync\ndef\nrun\n(\nself\n,\naudio_input\n:\nAudioInput\n|\nStreamedAudioInput\n)\n->\nStreamedAudioResult\n:\n\"\"\"Run the voice pipeline.\nArgs:\naudio_input: The audio input to process. This can either be an `AudioInput` instance,\nwhich is a single static buffer, or a `StreamedAudioInput` instance, which is a\nstream of audio data that you can append to.\nReturns:\nA `StreamedAudioResult` instance. You can use this object to stream audio events and\nplay them out.\n\"\"\"\nif\nisinstance\n(\naudio_input\n,\nAudioInput\n):\nreturn\nawait\nself\n.\n_run_single_turn\n(\naudio_input\n)\nelif\nisinstance\n(\naudio_input\n,\nStreamedAudioInput\n):\nreturn\nawait\nself\n.\n_run_multi_turn\n(\naudio_input\n)\nelse\n:\nraise\nUserError\n(\nf\n\"Unsupported audio input type:\n{\ntype\n(\naudio_input\n)\n}\n\"\n)",
  "Workflow": "Workflow\nVoiceWorkflowBase\nBases:\nABC\nA base class for a voice workflow. You must implement the\nrun\nmethod. A \"workflow\" is any\ncode you want, that receives a transcription and yields text that will be turned into speech\nby a text-to-speech model.\nIn most cases, you'll create\nAgent\ns and use\nRunner.run_streamed()\nto run them, returning\nsome or all of the text events from the stream. You can use the\nVoiceWorkflowHelper\nclass to\nhelp with extracting text events from the stream.\nIf you have a simple workflow that has a single starting agent and no custom logic, you can\nuse\nSingleAgentVoiceWorkflow\ndirectly.\nSource code in\nsrc/agents/voice/workflow.py\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\nclass\nVoiceWorkflowBase\n(\nabc\n.\nABC\n):\n\"\"\"\nA base class for a voice workflow. You must implement the `run` method. A \"workflow\" is any\ncode you want, that receives a transcription and yields text that will be turned into speech\nby a text-to-speech model.\nIn most cases, you'll create `Agent`s and use `Runner.run_streamed()` to run them, returning\nsome or all of the text events from the stream. You can use the `VoiceWorkflowHelper` class to\nhelp with extracting text events from the stream.\nIf you have a simple workflow that has a single starting agent and no custom logic, you can\nuse `SingleAgentVoiceWorkflow` directly.\n\"\"\"\n@abc\n.\nabstractmethod\ndef\nrun\n(\nself\n,\ntranscription\n:\nstr\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"\nRun the voice workflow. You will receive an input transcription, and must yield text that\nwill be spoken to the user. You can run whatever logic you want here. In most cases, the\nfinal logic will involve calling `Runner.run_streamed()` and yielding any text events from\nthe stream.\n\"\"\"\npass\nrun\nabstractmethod\nrun\n(\ntranscription\n:\nstr\n)\n->\nAsyncIterator\n[\nstr\n]\nRun the voice workflow. You will receive an input transcription, and must yield text that\nwill be spoken to the user. You can run whatever logic you want here. In most cases, the\nfinal logic will involve calling\nRunner.run_streamed()\nand yielding any text events from\nthe stream.\nSource code in\nsrc/agents/voice/workflow.py\n25\n26\n27\n28\n29\n30\n31\n32\n33\n@abc\n.\nabstractmethod\ndef\nrun\n(\nself\n,\ntranscription\n:\nstr\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"\nRun the voice workflow. You will receive an input transcription, and must yield text that\nwill be spoken to the user. You can run whatever logic you want here. In most cases, the\nfinal logic will involve calling `Runner.run_streamed()` and yielding any text events from\nthe stream.\n\"\"\"\npass\nVoiceWorkflowHelper\nSource code in\nsrc/agents/voice/workflow.py\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\nclass\nVoiceWorkflowHelper\n:\n@classmethod\nasync\ndef\nstream_text_from\n(\ncls\n,\nresult\n:\nRunResultStreaming\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\nasync\nfor\nevent\nin\nresult\n.\nstream_events\n():\nif\n(\nevent\n.\ntype\n==\n\"raw_response_event\"\nand\nevent\n.\ndata\n.\ntype\n==\n\"response.output_text.delta\"\n):\nyield\nevent\n.\ndata\n.\ndelta\nstream_text_from\nasync\nclassmethod\nstream_text_from\n(\nresult\n:\nRunResultStreaming\n,\n)\n->\nAsyncIterator\n[\nstr\n]\nWraps a\nRunResultStreaming\nobject and yields text events from the stream.\nSource code in\nsrc/agents/voice/workflow.py\n37\n38\n39\n40\n41\n42\n43\n44\n45\n@classmethod\nasync\ndef\nstream_text_from\n(\ncls\n,\nresult\n:\nRunResultStreaming\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"Wraps a `RunResultStreaming` object and yields text events from the stream.\"\"\"\nasync\nfor\nevent\nin\nresult\n.\nstream_events\n():\nif\n(\nevent\n.\ntype\n==\n\"raw_response_event\"\nand\nevent\n.\ndata\n.\ntype\n==\n\"response.output_text.delta\"\n):\nyield\nevent\n.\ndata\n.\ndelta\nSingleAgentWorkflowCallbacks\nSource code in\nsrc/agents/voice/workflow.py\n48\n49\n50\n51\nclass\nSingleAgentWorkflowCallbacks\n:\ndef\non_run\n(\nself\n,\nworkflow\n:\nSingleAgentVoiceWorkflow\n,\ntranscription\n:\nstr\n)\n->\nNone\n:\n\"\"\"Called when the workflow is run.\"\"\"\npass\non_run\non_run\n(\nworkflow\n:\nSingleAgentVoiceWorkflow\n,\ntranscription\n:\nstr\n)\n->\nNone\nCalled when the workflow is run.\nSource code in\nsrc/agents/voice/workflow.py\n49\n50\n51\ndef\non_run\n(\nself\n,\nworkflow\n:\nSingleAgentVoiceWorkflow\n,\ntranscription\n:\nstr\n)\n->\nNone\n:\n\"\"\"Called when the workflow is run.\"\"\"\npass\nSingleAgentVoiceWorkflow\nBases:\nVoiceWorkflowBase\nA simple voice workflow that runs a single agent. Each transcription and result is added to\nthe input history.\nFor more complex workflows (e.g. multiple Runner calls, custom message history, custom logic,\ncustom configs), subclass\nVoiceWorkflowBase\nand implement your own logic.\nSource code in\nsrc/agents/voice/workflow.py\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\nclass\nSingleAgentVoiceWorkflow\n(\nVoiceWorkflowBase\n):\n\"\"\"A simple voice workflow that runs a single agent. Each transcription and result is added to\nthe input history.\nFor more complex workflows (e.g. multiple Runner calls, custom message history, custom logic,\ncustom configs), subclass `VoiceWorkflowBase` and implement your own logic.\n\"\"\"\ndef\n__init__\n(\nself\n,\nagent\n:\nAgent\n[\nAny\n],\ncallbacks\n:\nSingleAgentWorkflowCallbacks\n|\nNone\n=\nNone\n):\n\"\"\"Create a new single agent voice workflow.\nArgs:\nagent: The agent to run.\ncallbacks: Optional callbacks to call during the workflow.\n\"\"\"\nself\n.\n_input_history\n:\nlist\n[\nTResponseInputItem\n]\n=\n[]\nself\n.\n_current_agent\n=\nagent\nself\n.\n_callbacks\n=\ncallbacks\nasync\ndef\nrun\n(\nself\n,\ntranscription\n:\nstr\n)\n->\nAsyncIterator\n[\nstr\n]:\nif\nself\n.\n_callbacks\n:\nself\n.\n_callbacks\n.\non_run\n(\nself\n,\ntranscription\n)\n# Add the transcription to the input history\nself\n.\n_input_history\n.\nappend\n(\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\ntranscription\n,\n}\n)\n# Run the agent\nresult\n=\nRunner\n.\nrun_streamed\n(\nself\n.\n_current_agent\n,\nself\n.\n_input_history\n)\n# Stream the text from the result\nasync\nfor\nchunk\nin\nVoiceWorkflowHelper\n.\nstream_text_from\n(\nresult\n):\nyield\nchunk\n# Update the input history and current agent\nself\n.\n_input_history\n=\nresult\n.\nto_input_list\n()\nself\n.\n_current_agent\n=\nresult\n.\nlast_agent\n__init__\n__init__\n(\nagent\n:\nAgent\n[\nAny\n],\ncallbacks\n:\nSingleAgentWorkflowCallbacks\n|\nNone\n=\nNone\n,\n)\nCreate a new single agent voice workflow.\nParameters:\nName\nType\nDescription\nDefault\nagent\nAgent\n[\nAny\n]\nThe agent to run.\nrequired\ncallbacks\nSingleAgentWorkflowCallbacks\n| None\nOptional callbacks to call during the workflow.\nNone\nSource code in\nsrc/agents/voice/workflow.py\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\ndef\n__init__\n(\nself\n,\nagent\n:\nAgent\n[\nAny\n],\ncallbacks\n:\nSingleAgentWorkflowCallbacks\n|\nNone\n=\nNone\n):\n\"\"\"Create a new single agent voice workflow.\nArgs:\nagent: The agent to run.\ncallbacks: Optional callbacks to call during the workflow.\n\"\"\"\nself\n.\n_input_history\n:\nlist\n[\nTResponseInputItem\n]\n=\n[]\nself\n.\n_current_agent\n=\nagent\nself\n.\n_callbacks\n=\ncallbacks",
  "Input": "Input\nAudioInput\ndataclass\nStatic audio to be used as input for the VoicePipeline.\nSource code in\nsrc/agents/voice/input.py\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n@dataclass\nclass\nAudioInput\n:\n\"\"\"Static audio to be used as input for the VoicePipeline.\"\"\"\nbuffer\n:\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]\n\"\"\"\nA buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.\n\"\"\"\nframe_rate\n:\nint\n=\nDEFAULT_SAMPLE_RATE\n\"\"\"The sample rate of the audio data. Defaults to 24000.\"\"\"\nsample_width\n:\nint\n=\n2\n\"\"\"The sample width of the audio data. Defaults to 2.\"\"\"\nchannels\n:\nint\n=\n1\n\"\"\"The number of channels in the audio data. Defaults to 1.\"\"\"\ndef\nto_audio_file\n(\nself\n)\n->\ntuple\n[\nstr\n,\nio\n.\nBytesIO\n,\nstr\n]:\n\"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\nreturn\n_buffer_to_audio_file\n(\nself\n.\nbuffer\n,\nself\n.\nframe_rate\n,\nself\n.\nsample_width\n,\nself\n.\nchannels\n)\ndef\nto_base64\n(\nself\n)\n->\nstr\n:\n\"\"\"Returns the audio data as a base64 encoded string.\"\"\"\nif\nself\n.\nbuffer\n.\ndtype\n==\nnp\n.\nfloat32\n:\n# convert to int16\nself\n.\nbuffer\n=\nnp\n.\nclip\n(\nself\n.\nbuffer\n,\n-\n1.0\n,\n1.0\n)\nself\n.\nbuffer\n=\n(\nself\n.\nbuffer\n*\n32767\n)\n.\nastype\n(\nnp\n.\nint16\n)\nelif\nself\n.\nbuffer\n.\ndtype\n!=\nnp\n.\nint16\n:\nraise\nUserError\n(\n\"Buffer must be a numpy array of int16 or float32\"\n)\nreturn\nbase64\n.\nb64encode\n(\nself\n.\nbuffer\n.\ntobytes\n())\n.\ndecode\n(\n\"utf-8\"\n)\nbuffer\ninstance-attribute\nbuffer\n:\nNDArray\n[\nint16\n|\nfloat32\n]\nA buffer containing the audio data for the agent. Must be a numpy array of int16 or float32.\nframe_rate\nclass-attribute\ninstance-attribute\nframe_rate\n:\nint\n=\nDEFAULT_SAMPLE_RATE\nThe sample rate of the audio data. Defaults to 24000.\nsample_width\nclass-attribute\ninstance-attribute\nsample_width\n:\nint\n=\n2\nThe sample width of the audio data. Defaults to 2.\nchannels\nclass-attribute\ninstance-attribute\nchannels\n:\nint\n=\n1\nThe number of channels in the audio data. Defaults to 1.\nto_audio_file\nto_audio_file\n()\n->\ntuple\n[\nstr\n,\nBytesIO\n,\nstr\n]\nReturns a tuple of (filename, bytes, content_type)\nSource code in\nsrc/agents/voice/input.py\n58\n59\n60\ndef\nto_audio_file\n(\nself\n)\n->\ntuple\n[\nstr\n,\nio\n.\nBytesIO\n,\nstr\n]:\n\"\"\"Returns a tuple of (filename, bytes, content_type)\"\"\"\nreturn\n_buffer_to_audio_file\n(\nself\n.\nbuffer\n,\nself\n.\nframe_rate\n,\nself\n.\nsample_width\n,\nself\n.\nchannels\n)\nto_base64\nto_base64\n()\n->\nstr\nReturns the audio data as a base64 encoded string.\nSource code in\nsrc/agents/voice/input.py\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\ndef\nto_base64\n(\nself\n)\n->\nstr\n:\n\"\"\"Returns the audio data as a base64 encoded string.\"\"\"\nif\nself\n.\nbuffer\n.\ndtype\n==\nnp\n.\nfloat32\n:\n# convert to int16\nself\n.\nbuffer\n=\nnp\n.\nclip\n(\nself\n.\nbuffer\n,\n-\n1.0\n,\n1.0\n)\nself\n.\nbuffer\n=\n(\nself\n.\nbuffer\n*\n32767\n)\n.\nastype\n(\nnp\n.\nint16\n)\nelif\nself\n.\nbuffer\n.\ndtype\n!=\nnp\n.\nint16\n:\nraise\nUserError\n(\n\"Buffer must be a numpy array of int16 or float32\"\n)\nreturn\nbase64\n.\nb64encode\n(\nself\n.\nbuffer\n.\ntobytes\n())\n.\ndecode\n(\n\"utf-8\"\n)\nStreamedAudioInput\nAudio input represented as a stream of audio data. You can pass this to the\nVoicePipeline\nand then push audio data into the queue using the\nadd_audio\nmethod.\nSource code in\nsrc/agents/voice/input.py\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\nclass\nStreamedAudioInput\n:\n\"\"\"Audio input represented as a stream of audio data. You can pass this to the `VoicePipeline`\nand then push audio data into the queue using the `add_audio` method.\n\"\"\"\ndef\n__init__\n(\nself\n):\nself\n.\nqueue\n:\nasyncio\n.\nQueue\n[\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]]\n=\nasyncio\n.\nQueue\n()\nasync\ndef\nadd_audio\n(\nself\n,\naudio\n:\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]):\n\"\"\"Adds more audio data to the stream.\nArgs:\naudio: The audio data to add. Must be a numpy array of int16 or float32.\n\"\"\"\nawait\nself\n.\nqueue\n.\nput\n(\naudio\n)\nadd_audio\nasync\nadd_audio\n(\naudio\n:\nNDArray\n[\nint16\n|\nfloat32\n])\nAdds more audio data to the stream.\nParameters:\nName\nType\nDescription\nDefault\naudio\nNDArray\n[\nint16\n|\nfloat32\n]\nThe audio data to add. Must be a numpy array of int16 or float32.\nrequired\nSource code in\nsrc/agents/voice/input.py\n82\n83\n84\n85\n86\n87\n88\nasync\ndef\nadd_audio\n(\nself\n,\naudio\n:\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]):\n\"\"\"Adds more audio data to the stream.\nArgs:\naudio: The audio data to add. Must be a numpy array of int16 or float32.\n\"\"\"\nawait\nself\n.\nqueue\n.\nput\n(\naudio\n)",
  "Result": "Result\nStreamedAudioResult\nThe output of a\nVoicePipeline\n. Streams events and audio data as they're generated.\nSource code in\nsrc/agents/voice/result.py\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\nclass\nStreamedAudioResult\n:\n\"\"\"The output of a `VoicePipeline`. Streams events and audio data as they're generated.\"\"\"\ndef\n__init__\n(\nself\n,\ntts_model\n:\nTTSModel\n,\ntts_settings\n:\nTTSModelSettings\n,\nvoice_pipeline_config\n:\nVoicePipelineConfig\n,\n):\n\"\"\"Create a new `StreamedAudioResult` instance.\nArgs:\ntts_model: The TTS model to use.\ntts_settings: The TTS settings to use.\nvoice_pipeline_config: The voice pipeline config to use.\n\"\"\"\nself\n.\ntts_model\n=\ntts_model\nself\n.\ntts_settings\n=\ntts_settings\nself\n.\ntotal_output_text\n=\n\"\"\nself\n.\ninstructions\n=\ntts_settings\n.\ninstructions\nself\n.\ntext_generation_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_voice_pipeline_config\n=\nvoice_pipeline_config\nself\n.\n_text_buffer\n=\n\"\"\nself\n.\n_turn_text_buffer\n=\n\"\"\nself\n.\n_queue\n:\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_tasks\n:\nlist\n[\nasyncio\n.\nTask\n[\nAny\n]]\n=\n[]\nself\n.\n_ordered_tasks\n:\nlist\n[\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n|\nNone\n]\n]\n=\n[]\n# New: list to hold local queues for each text segment\nself\n.\n_dispatcher_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\n(\nNone\n# Task to dispatch audio chunks in order\n)\nself\n.\n_done_processing\n=\nFalse\nself\n.\n_buffer_size\n=\ntts_settings\n.\nbuffer_size\nself\n.\n_started_processing_turn\n=\nFalse\nself\n.\n_first_byte_received\n=\nFalse\nself\n.\n_generation_start_time\n:\nstr\n|\nNone\n=\nNone\nself\n.\n_completed_session\n=\nFalse\nself\n.\n_stored_exception\n:\nBaseException\n|\nNone\n=\nNone\nself\n.\n_tracing_span\n:\nSpan\n[\nSpeechGroupSpanData\n]\n|\nNone\n=\nNone\nasync\ndef\n_start_turn\n(\nself\n):\nif\nself\n.\n_started_processing_turn\n:\nreturn\nself\n.\n_tracing_span\n=\nspeech_group_span\n()\nself\n.\n_tracing_span\n.\nstart\n()\nself\n.\n_started_processing_turn\n=\nTrue\nself\n.\n_first_byte_received\n=\nFalse\nself\n.\n_generation_start_time\n=\ntime_iso\n()\nawait\nself\n.\n_queue\n.\nput\n(\nVoiceStreamEventLifecycle\n(\nevent\n=\n\"turn_started\"\n))\ndef\n_set_task\n(\nself\n,\ntask\n:\nasyncio\n.\nTask\n[\nAny\n]):\nself\n.\ntext_generation_task\n=\ntask\nasync\ndef\n_add_error\n(\nself\n,\nerror\n:\nException\n):\nawait\nself\n.\n_queue\n.\nput\n(\nVoiceStreamEventError\n(\nerror\n))\ndef\n_transform_audio_buffer\n(\nself\n,\nbuffer\n:\nlist\n[\nbytes\n],\noutput_dtype\n:\nnpt\n.\nDTypeLike\n)\n->\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]:\nnp_array\n=\nnp\n.\nfrombuffer\n(\nb\n\"\"\n.\njoin\n(\nbuffer\n),\ndtype\n=\nnp\n.\nint16\n)\nif\noutput_dtype\n==\nnp\n.\nint16\n:\nreturn\nnp_array\nelif\noutput_dtype\n==\nnp\n.\nfloat32\n:\nreturn\n(\nnp_array\n.\nastype\n(\nnp\n.\nfloat32\n)\n/\n32767.0\n)\n.\nreshape\n(\n-\n1\n,\n1\n)\nelse\n:\nraise\nUserError\n(\n\"Invalid output dtype\"\n)\nasync\ndef\n_stream_audio\n(\nself\n,\ntext\n:\nstr\n,\nlocal_queue\n:\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n|\nNone\n],\nfinish_turn\n:\nbool\n=\nFalse\n,\n):\nwith\nspeech_span\n(\nmodel\n=\nself\n.\ntts_model\n.\nmodel_name\n,\ninput\n=\ntext\nif\nself\n.\n_voice_pipeline_config\n.\ntrace_include_sensitive_data\nelse\n\"\"\n,\nmodel_config\n=\n{\n\"voice\"\n:\nself\n.\ntts_settings\n.\nvoice\n,\n\"instructions\"\n:\nself\n.\ninstructions\n,\n\"speed\"\n:\nself\n.\ntts_settings\n.\nspeed\n,\n},\noutput_format\n=\n\"pcm\"\n,\nparent\n=\nself\n.\n_tracing_span\n,\n)\nas\ntts_span\n:\ntry\n:\nfirst_byte_received\n=\nFalse\nbuffer\n:\nlist\n[\nbytes\n]\n=\n[]\nfull_audio_data\n:\nlist\n[\nbytes\n]\n=\n[]\nasync\nfor\nchunk\nin\nself\n.\ntts_model\n.\nrun\n(\ntext\n,\nself\n.\ntts_settings\n):\nif\nnot\nfirst_byte_received\n:\nfirst_byte_received\n=\nTrue\ntts_span\n.\nspan_data\n.\nfirst_content_at\n=\ntime_iso\n()\nif\nchunk\n:\nbuffer\n.\nappend\n(\nchunk\n)\nfull_audio_data\n.\nappend\n(\nchunk\n)\nif\nlen\n(\nbuffer\n)\n>=\nself\n.\n_buffer_size\n:\naudio_np\n=\nself\n.\n_transform_audio_buffer\n(\nbuffer\n,\nself\n.\ntts_settings\n.\ndtype\n)\nif\nself\n.\ntts_settings\n.\ntransform_data\n:\naudio_np\n=\nself\n.\ntts_settings\n.\ntransform_data\n(\naudio_np\n)\nawait\nlocal_queue\n.\nput\n(\nVoiceStreamEventAudio\n(\ndata\n=\naudio_np\n)\n)\n# Use local queue\nbuffer\n=\n[]\nif\nbuffer\n:\naudio_np\n=\nself\n.\n_transform_audio_buffer\n(\nbuffer\n,\nself\n.\ntts_settings\n.\ndtype\n)\nif\nself\n.\ntts_settings\n.\ntransform_data\n:\naudio_np\n=\nself\n.\ntts_settings\n.\ntransform_data\n(\naudio_np\n)\nawait\nlocal_queue\n.\nput\n(\nVoiceStreamEventAudio\n(\ndata\n=\naudio_np\n))\n# Use local queue\nif\nself\n.\n_voice_pipeline_config\n.\ntrace_include_sensitive_audio_data\n:\ntts_span\n.\nspan_data\n.\noutput\n=\n_audio_to_base64\n(\nfull_audio_data\n)\nelse\n:\ntts_span\n.\nspan_data\n.\noutput\n=\n\"\"\nif\nfinish_turn\n:\nawait\nlocal_queue\n.\nput\n(\nVoiceStreamEventLifecycle\n(\nevent\n=\n\"turn_ended\"\n))\nelse\n:\nawait\nlocal_queue\n.\nput\n(\nNone\n)\n# Signal completion for this segment\nexcept\nException\nas\ne\n:\ntts_span\n.\nset_error\n(\n{\n\"message\"\n:\nstr\n(\ne\n),\n\"data\"\n:\n{\n\"text\"\n:\ntext\nif\nself\n.\n_voice_pipeline_config\n.\ntrace_include_sensitive_data\nelse\n\"\"\n,\n},\n}\n)\nlogger\n.\nerror\n(\nf\n\"Error streaming audio:\n{\ne\n}\n\"\n)\n# Signal completion for whole session because of error\nawait\nlocal_queue\n.\nput\n(\nVoiceStreamEventLifecycle\n(\nevent\n=\n\"session_ended\"\n))\nraise\ne\nasync\ndef\n_add_text\n(\nself\n,\ntext\n:\nstr\n):\nawait\nself\n.\n_start_turn\n()\nself\n.\n_text_buffer\n+=\ntext\nself\n.\ntotal_output_text\n+=\ntext\nself\n.\n_turn_text_buffer\n+=\ntext\ncombined_sentences\n,\nself\n.\n_text_buffer\n=\nself\n.\ntts_settings\n.\ntext_splitter\n(\nself\n.\n_text_buffer\n)\nif\nlen\n(\ncombined_sentences\n)\n>=\n20\n:\nlocal_queue\n:\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n|\nNone\n]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_ordered_tasks\n.\nappend\n(\nlocal_queue\n)\nself\n.\n_tasks\n.\nappend\n(\nasyncio\n.\ncreate_task\n(\nself\n.\n_stream_audio\n(\ncombined_sentences\n,\nlocal_queue\n))\n)\nif\nself\n.\n_dispatcher_task\nis\nNone\n:\nself\n.\n_dispatcher_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_dispatch_audio\n())\nasync\ndef\n_turn_done\n(\nself\n):\nif\nself\n.\n_text_buffer\n:\nlocal_queue\n:\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n|\nNone\n]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_ordered_tasks\n.\nappend\n(\nlocal_queue\n)\n# Append the local queue for the final segment\nself\n.\n_tasks\n.\nappend\n(\nasyncio\n.\ncreate_task\n(\nself\n.\n_stream_audio\n(\nself\n.\n_text_buffer\n,\nlocal_queue\n,\nfinish_turn\n=\nTrue\n)\n)\n)\nself\n.\n_text_buffer\n=\n\"\"\nself\n.\n_done_processing\n=\nTrue\nif\nself\n.\n_dispatcher_task\nis\nNone\n:\nself\n.\n_dispatcher_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_dispatch_audio\n())\nawait\nasyncio\n.\ngather\n(\n*\nself\n.\n_tasks\n)\ndef\n_finish_turn\n(\nself\n):\nif\nself\n.\n_tracing_span\n:\nif\nself\n.\n_voice_pipeline_config\n.\ntrace_include_sensitive_data\n:\nself\n.\n_tracing_span\n.\nspan_data\n.\ninput\n=\nself\n.\n_turn_text_buffer\nelse\n:\nself\n.\n_tracing_span\n.\nspan_data\n.\ninput\n=\n\"\"\nself\n.\n_tracing_span\n.\nfinish\n()\nself\n.\n_tracing_span\n=\nNone\nself\n.\n_turn_text_buffer\n=\n\"\"\nself\n.\n_started_processing_turn\n=\nFalse\nasync\ndef\n_done\n(\nself\n):\nself\n.\n_completed_session\n=\nTrue\nawait\nself\n.\n_wait_for_completion\n()\nasync\ndef\n_dispatch_audio\n(\nself\n):\n# Dispatch audio chunks from each segment in the order they were added\nwhile\nTrue\n:\nif\nlen\n(\nself\n.\n_ordered_tasks\n)\n==\n0\n:\nif\nself\n.\n_completed_session\n:\nbreak\nawait\nasyncio\n.\nsleep\n(\n0\n)\ncontinue\nlocal_queue\n=\nself\n.\n_ordered_tasks\n.\npop\n(\n0\n)\nwhile\nTrue\n:\nchunk\n=\nawait\nlocal_queue\n.\nget\n()\nif\nchunk\nis\nNone\n:\nbreak\nawait\nself\n.\n_queue\n.\nput\n(\nchunk\n)\nif\nisinstance\n(\nchunk\n,\nVoiceStreamEventLifecycle\n):\nlocal_queue\n.\ntask_done\n()\nif\nchunk\n.\nevent\n==\n\"turn_ended\"\n:\nself\n.\n_finish_turn\n()\nbreak\nawait\nself\n.\n_queue\n.\nput\n(\nVoiceStreamEventLifecycle\n(\nevent\n=\n\"session_ended\"\n))\nasync\ndef\n_wait_for_completion\n(\nself\n):\ntasks\n:\nlist\n[\nasyncio\n.\nTask\n[\nAny\n]]\n=\nself\n.\n_tasks\nif\nself\n.\n_dispatcher_task\nis\nnot\nNone\n:\ntasks\n.\nappend\n(\nself\n.\n_dispatcher_task\n)\nawait\nasyncio\n.\ngather\n(\n*\ntasks\n)\ndef\n_cleanup_tasks\n(\nself\n):\nself\n.\n_finish_turn\n()\nfor\ntask\nin\nself\n.\n_tasks\n:\nif\nnot\ntask\n.\ndone\n():\ntask\n.\ncancel\n()\nif\nself\n.\n_dispatcher_task\nand\nnot\nself\n.\n_dispatcher_task\n.\ndone\n():\nself\n.\n_dispatcher_task\n.\ncancel\n()\nif\nself\n.\ntext_generation_task\nand\nnot\nself\n.\ntext_generation_task\n.\ndone\n():\nself\n.\ntext_generation_task\n.\ncancel\n()\ndef\n_check_errors\n(\nself\n):\nfor\ntask\nin\nself\n.\n_tasks\n:\nif\ntask\n.\ndone\n():\nif\ntask\n.\nexception\n():\nself\n.\n_stored_exception\n=\ntask\n.\nexception\n()\nbreak\nasync\ndef\nstream\n(\nself\n)\n->\nAsyncIterator\n[\nVoiceStreamEvent\n]:\n\"\"\"Stream the events and audio data as they're generated.\"\"\"\nwhile\nTrue\n:\ntry\n:\nevent\n=\nawait\nself\n.\n_queue\n.\nget\n()\nexcept\nasyncio\n.\nCancelledError\n:\nbreak\nif\nisinstance\n(\nevent\n,\nVoiceStreamEventError\n):\nself\n.\n_stored_exception\n=\nevent\n.\nerror\nlogger\n.\nerror\n(\nf\n\"Error processing output:\n{\nevent\n.\nerror\n}\n\"\n)\nbreak\nif\nevent\nis\nNone\n:\nbreak\nyield\nevent\nif\nevent\n.\ntype\n==\n\"voice_stream_event_lifecycle\"\nand\nevent\n.\nevent\n==\n\"session_ended\"\n:\nbreak\nself\n.\n_check_errors\n()\nself\n.\n_cleanup_tasks\n()\nif\nself\n.\n_stored_exception\n:\nraise\nself\n.\n_stored_exception\n__init__\n__init__\n(\ntts_model\n:\nTTSModel\n,\ntts_settings\n:\nTTSModelSettings\n,\nvoice_pipeline_config\n:\nVoicePipelineConfig\n,\n)\nCreate a new\nStreamedAudioResult\ninstance.\nParameters:\nName\nType\nDescription\nDefault\ntts_model\nTTSModel\nThe TTS model to use.\nrequired\ntts_settings\nTTSModelSettings\nThe TTS settings to use.\nrequired\nvoice_pipeline_config\nVoicePipelineConfig\nThe voice pipeline config to use.\nrequired\nSource code in\nsrc/agents/voice/result.py\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\ndef\n__init__\n(\nself\n,\ntts_model\n:\nTTSModel\n,\ntts_settings\n:\nTTSModelSettings\n,\nvoice_pipeline_config\n:\nVoicePipelineConfig\n,\n):\n\"\"\"Create a new `StreamedAudioResult` instance.\nArgs:\ntts_model: The TTS model to use.\ntts_settings: The TTS settings to use.\nvoice_pipeline_config: The voice pipeline config to use.\n\"\"\"\nself\n.\ntts_model\n=\ntts_model\nself\n.\ntts_settings\n=\ntts_settings\nself\n.\ntotal_output_text\n=\n\"\"\nself\n.\ninstructions\n=\ntts_settings\n.\ninstructions\nself\n.\ntext_generation_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_voice_pipeline_config\n=\nvoice_pipeline_config\nself\n.\n_text_buffer\n=\n\"\"\nself\n.\n_turn_text_buffer\n=\n\"\"\nself\n.\n_queue\n:\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_tasks\n:\nlist\n[\nasyncio\n.\nTask\n[\nAny\n]]\n=\n[]\nself\n.\n_ordered_tasks\n:\nlist\n[\nasyncio\n.\nQueue\n[\nVoiceStreamEvent\n|\nNone\n]\n]\n=\n[]\n# New: list to hold local queues for each text segment\nself\n.\n_dispatcher_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\n(\nNone\n# Task to dispatch audio chunks in order\n)\nself\n.\n_done_processing\n=\nFalse\nself\n.\n_buffer_size\n=\ntts_settings\n.\nbuffer_size\nself\n.\n_started_processing_turn\n=\nFalse\nself\n.\n_first_byte_received\n=\nFalse\nself\n.\n_generation_start_time\n:\nstr\n|\nNone\n=\nNone\nself\n.\n_completed_session\n=\nFalse\nself\n.\n_stored_exception\n:\nBaseException\n|\nNone\n=\nNone\nself\n.\n_tracing_span\n:\nSpan\n[\nSpeechGroupSpanData\n]\n|\nNone\n=\nNone\nstream\nasync\nstream\n()\n->\nAsyncIterator\n[\nVoiceStreamEvent\n]\nStream the events and audio data as they're generated.\nSource code in\nsrc/agents/voice/result.py\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\nasync\ndef\nstream\n(\nself\n)\n->\nAsyncIterator\n[\nVoiceStreamEvent\n]:\n\"\"\"Stream the events and audio data as they're generated.\"\"\"\nwhile\nTrue\n:\ntry\n:\nevent\n=\nawait\nself\n.\n_queue\n.\nget\n()\nexcept\nasyncio\n.\nCancelledError\n:\nbreak\nif\nisinstance\n(\nevent\n,\nVoiceStreamEventError\n):\nself\n.\n_stored_exception\n=\nevent\n.\nerror\nlogger\n.\nerror\n(\nf\n\"Error processing output:\n{\nevent\n.\nerror\n}\n\"\n)\nbreak\nif\nevent\nis\nNone\n:\nbreak\nyield\nevent\nif\nevent\n.\ntype\n==\n\"voice_stream_event_lifecycle\"\nand\nevent\n.\nevent\n==\n\"session_ended\"\n:\nbreak\nself\n.\n_check_errors\n()\nself\n.\n_cleanup_tasks\n()\nif\nself\n.\n_stored_exception\n:\nraise\nself\n.\n_stored_exception",
  "Pipeline Config": "Pipeline Config\nVoicePipelineConfig\ndataclass\nConfiguration for a\nVoicePipeline\n.\nSource code in\nsrc/agents/voice/pipeline_config.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n@dataclass\nclass\nVoicePipelineConfig\n:\n\"\"\"Configuration for a `VoicePipeline`.\"\"\"\nmodel_provider\n:\nVoiceModelProvider\n=\nfield\n(\ndefault_factory\n=\nOpenAIVoiceModelProvider\n)\n\"\"\"The voice model provider to use for the pipeline. Defaults to OpenAI.\"\"\"\ntracing_disabled\n:\nbool\n=\nFalse\n\"\"\"Whether to disable tracing of the pipeline. Defaults to `False`.\"\"\"\ntrace_include_sensitive_data\n:\nbool\n=\nTrue\n\"\"\"Whether to include sensitive data in traces. Defaults to `True`. This is specifically for the\nvoice pipeline, and not for anything that goes on inside your Workflow.\"\"\"\ntrace_include_sensitive_audio_data\n:\nbool\n=\nTrue\n\"\"\"Whether to include audio data in traces. Defaults to `True`.\"\"\"\nworkflow_name\n:\nstr\n=\n\"Voice Agent\"\n\"\"\"The name of the workflow to use for tracing. Defaults to `Voice Agent`.\"\"\"\ngroup_id\n:\nstr\n=\nfield\n(\ndefault_factory\n=\ngen_group_id\n)\n\"\"\"\nA grouping identifier to use for tracing, to link multiple traces from the same conversation\nor process. If not provided, we will create a random group ID.\n\"\"\"\ntrace_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n\"\"\"\nAn optional dictionary of additional metadata to include with the trace.\n\"\"\"\nstt_settings\n:\nSTTModelSettings\n=\nfield\n(\ndefault_factory\n=\nSTTModelSettings\n)\n\"\"\"The settings to use for the STT model.\"\"\"\ntts_settings\n:\nTTSModelSettings\n=\nfield\n(\ndefault_factory\n=\nTTSModelSettings\n)\n\"\"\"The settings to use for the TTS model.\"\"\"\nmodel_provider\nclass-attribute\ninstance-attribute\nmodel_provider\n:\nVoiceModelProvider\n=\nfield\n(\ndefault_factory\n=\nOpenAIVoiceModelProvider\n)\nThe voice model provider to use for the pipeline. Defaults to OpenAI.\ntracing_disabled\nclass-attribute\ninstance-attribute\ntracing_disabled\n:\nbool\n=\nFalse\nWhether to disable tracing of the pipeline. Defaults to\nFalse\n.\ntrace_include_sensitive_data\nclass-attribute\ninstance-attribute\ntrace_include_sensitive_data\n:\nbool\n=\nTrue\nWhether to include sensitive data in traces. Defaults to\nTrue\n. This is specifically for the\nvoice pipeline, and not for anything that goes on inside your Workflow.\ntrace_include_sensitive_audio_data\nclass-attribute\ninstance-attribute\ntrace_include_sensitive_audio_data\n:\nbool\n=\nTrue\nWhether to include audio data in traces. Defaults to\nTrue\n.\nworkflow_name\nclass-attribute\ninstance-attribute\nworkflow_name\n:\nstr\n=\n'Voice Agent'\nThe name of the workflow to use for tracing. Defaults to\nVoice Agent\n.\ngroup_id\nclass-attribute\ninstance-attribute\ngroup_id\n:\nstr\n=\nfield\n(\ndefault_factory\n=\ngen_group_id\n)\nA grouping identifier to use for tracing, to link multiple traces from the same conversation\nor process. If not provided, we will create a random group ID.\ntrace_metadata\nclass-attribute\ninstance-attribute\ntrace_metadata\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nAn optional dictionary of additional metadata to include with the trace.\nstt_settings\nclass-attribute\ninstance-attribute\nstt_settings\n:\nSTTModelSettings\n=\nfield\n(\ndefault_factory\n=\nSTTModelSettings\n)\nThe settings to use for the STT model.\ntts_settings\nclass-attribute\ninstance-attribute\ntts_settings\n:\nTTSModelSettings\n=\nfield\n(\ndefault_factory\n=\nTTSModelSettings\n)\nThe settings to use for the TTS model.",
  "Events": "Events\nVoiceStreamEvent\nmodule-attribute\nVoiceStreamEvent\n:\nTypeAlias\n=\nUnion\n[\nVoiceStreamEventAudio\n,\nVoiceStreamEventLifecycle\n,\nVoiceStreamEventError\n,\n]\nAn event from the\nVoicePipeline\n, streamed via\nStreamedAudioResult.stream()\n.\nVoiceStreamEventAudio\ndataclass\nStreaming event from the VoicePipeline\nSource code in\nsrc/agents/voice/events.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n@dataclass\nclass\nVoiceStreamEventAudio\n:\n\"\"\"Streaming event from the VoicePipeline\"\"\"\ndata\n:\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]\n|\nNone\n\"\"\"The audio data.\"\"\"\ntype\n:\nLiteral\n[\n\"voice_stream_event_audio\"\n]\n=\n\"voice_stream_event_audio\"\n\"\"\"The type of event.\"\"\"\ndata\ninstance-attribute\ndata\n:\nNDArray\n[\nint16\n|\nfloat32\n]\n|\nNone\nThe audio data.\ntype\nclass-attribute\ninstance-attribute\ntype\n:\nLiteral\n[\n\"voice_stream_event_audio\"\n]\n=\n(\n\"voice_stream_event_audio\"\n)\nThe type of event.\nVoiceStreamEventLifecycle\ndataclass\nStreaming event from the VoicePipeline\nSource code in\nsrc/agents/voice/events.py\n22\n23\n24\n25\n26\n27\n28\n29\n30\n@dataclass\nclass\nVoiceStreamEventLifecycle\n:\n\"\"\"Streaming event from the VoicePipeline\"\"\"\nevent\n:\nLiteral\n[\n\"turn_started\"\n,\n\"turn_ended\"\n,\n\"session_ended\"\n]\n\"\"\"The event that occurred.\"\"\"\ntype\n:\nLiteral\n[\n\"voice_stream_event_lifecycle\"\n]\n=\n\"voice_stream_event_lifecycle\"\n\"\"\"The type of event.\"\"\"\nevent\ninstance-attribute\nevent\n:\nLiteral\n[\n\"turn_started\"\n,\n\"turn_ended\"\n,\n\"session_ended\"\n]\nThe event that occurred.\ntype\nclass-attribute\ninstance-attribute\ntype\n:\nLiteral\n[\n\"voice_stream_event_lifecycle\"\n]\n=\n(\n\"voice_stream_event_lifecycle\"\n)\nThe type of event.\nVoiceStreamEventError\ndataclass\nStreaming event from the VoicePipeline\nSource code in\nsrc/agents/voice/events.py\n33\n34\n35\n36\n37\n38\n39\n40\n41\n@dataclass\nclass\nVoiceStreamEventError\n:\n\"\"\"Streaming event from the VoicePipeline\"\"\"\nerror\n:\nException\n\"\"\"The error that occurred.\"\"\"\ntype\n:\nLiteral\n[\n\"voice_stream_event_error\"\n]\n=\n\"voice_stream_event_error\"\n\"\"\"The type of event.\"\"\"\nerror\ninstance-attribute\nerror\n:\nException\nThe error that occurred.\ntype\nclass-attribute\ninstance-attribute\ntype\n:\nLiteral\n[\n\"voice_stream_event_error\"\n]\n=\n(\n\"voice_stream_event_error\"\n)\nThe type of event.",
  "Model": "Model\nTTSModelSettings\ndataclass\nSettings for a TTS model.\nSource code in\nsrc/agents/voice/model.py\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n@dataclass\nclass\nTTSModelSettings\n:\n\"\"\"Settings for a TTS model.\"\"\"\nvoice\n:\n(\nLiteral\n[\n\"alloy\"\n,\n\"ash\"\n,\n\"coral\"\n,\n\"echo\"\n,\n\"fable\"\n,\n\"onyx\"\n,\n\"nova\"\n,\n\"sage\"\n,\n\"shimmer\"\n]\n|\nNone\n)\n=\nNone\n\"\"\"\nThe voice to use for the TTS model. If not provided, the default voice for the respective model\nwill be used.\n\"\"\"\nbuffer_size\n:\nint\n=\n120\n\"\"\"The minimal size of the chunks of audio data that are being streamed out.\"\"\"\ndtype\n:\nnpt\n.\nDTypeLike\n=\nnp\n.\nint16\n\"\"\"The data type for the audio data to be returned in.\"\"\"\ntransform_data\n:\n(\nCallable\n[[\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]],\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]]\n|\nNone\n)\n=\nNone\n\"\"\"\nA function to transform the data from the TTS model. This is useful if you want the resulting\naudio stream to have the data in a specific shape already.\n\"\"\"\ninstructions\n:\nstr\n=\n(\n\"You will receive partial sentences. Do not complete the sentence just read out the text.\"\n)\n\"\"\"\nThe instructions to use for the TTS model. This is useful if you want to control the tone of the\naudio output.\n\"\"\"\ntext_splitter\n:\nCallable\n[[\nstr\n],\ntuple\n[\nstr\n,\nstr\n]]\n=\nget_sentence_based_splitter\n()\n\"\"\"\nA function to split the text into chunks. This is useful if you want to split the text into\nchunks before sending it to the TTS model rather than waiting for the whole text to be\nprocessed.\n\"\"\"\nspeed\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The speed with which the TTS model will read the text. Between 0.25 and 4.0.\"\"\"\nvoice\nclass-attribute\ninstance-attribute\nvoice\n:\n(\nLiteral\n[\n\"alloy\"\n,\n\"ash\"\n,\n\"coral\"\n,\n\"echo\"\n,\n\"fable\"\n,\n\"onyx\"\n,\n\"nova\"\n,\n\"sage\"\n,\n\"shimmer\"\n,\n]\n|\nNone\n)\n=\nNone\nThe voice to use for the TTS model. If not provided, the default voice for the respective model\nwill be used.\nbuffer_size\nclass-attribute\ninstance-attribute\nbuffer_size\n:\nint\n=\n120\nThe minimal size of the chunks of audio data that are being streamed out.\ndtype\nclass-attribute\ninstance-attribute\ndtype\n:\nDTypeLike\n=\nint16\nThe data type for the audio data to be returned in.\ntransform_data\nclass-attribute\ninstance-attribute\ntransform_data\n:\n(\nCallable\n[\n[\nNDArray\n[\nint16\n|\nfloat32\n]],\nNDArray\n[\nint16\n|\nfloat32\n]\n]\n|\nNone\n)\n=\nNone\nA function to transform the data from the TTS model. This is useful if you want the resulting\naudio stream to have the data in a specific shape already.\ninstructions\nclass-attribute\ninstance-attribute\ninstructions\n:\nstr\n=\n\"You will receive partial sentences. Do not complete the sentence just read out the text.\"\nThe instructions to use for the TTS model. This is useful if you want to control the tone of the\naudio output.\ntext_splitter\nclass-attribute\ninstance-attribute\ntext_splitter\n:\nCallable\n[[\nstr\n],\ntuple\n[\nstr\n,\nstr\n]]\n=\n(\nget_sentence_based_splitter\n()\n)\nA function to split the text into chunks. This is useful if you want to split the text into\nchunks before sending it to the TTS model rather than waiting for the whole text to be\nprocessed.\nspeed\nclass-attribute\ninstance-attribute\nspeed\n:\nfloat\n|\nNone\n=\nNone\nThe speed with which the TTS model will read the text. Between 0.25 and 4.0.\nTTSModel\nBases:\nABC\nA text-to-speech model that can convert text into audio output.\nSource code in\nsrc/agents/voice/model.py\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\nclass\nTTSModel\n(\nabc\n.\nABC\n):\n\"\"\"A text-to-speech model that can convert text into audio output.\"\"\"\n@property\n@abc\n.\nabstractmethod\ndef\nmodel_name\n(\nself\n)\n->\nstr\n:\n\"\"\"The name of the TTS model.\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nrun\n(\nself\n,\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]:\n\"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\nArgs:\ntext: The text to convert to audio.\nReturns:\nAn async iterator of audio bytes, in PCM format.\n\"\"\"\npass\nmodel_name\nabstractmethod\nproperty\nmodel_name\n:\nstr\nThe name of the TTS model.\nrun\nabstractmethod\nrun\n(\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]\nGiven a text string, produces a stream of audio bytes, in PCM format.\nParameters:\nName\nType\nDescription\nDefault\ntext\nstr\nThe text to convert to audio.\nrequired\nReturns:\nType\nDescription\nAsyncIterator\n[\nbytes\n]\nAn async iterator of audio bytes, in PCM format.\nSource code in\nsrc/agents/voice/model.py\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n@abc\n.\nabstractmethod\ndef\nrun\n(\nself\n,\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]:\n\"\"\"Given a text string, produces a stream of audio bytes, in PCM format.\nArgs:\ntext: The text to convert to audio.\nReturns:\nAn async iterator of audio bytes, in PCM format.\n\"\"\"\npass\nStreamedTranscriptionSession\nBases:\nABC\nA streamed transcription of audio input.\nSource code in\nsrc/agents/voice/model.py\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\nclass\nStreamedTranscriptionSession\n(\nabc\n.\nABC\n):\n\"\"\"A streamed transcription of audio input.\"\"\"\n@abc\n.\nabstractmethod\ndef\ntranscribe_turns\n(\nself\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\nThis method is expected to return only after `close()` is called.\n\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\nclose\n(\nself\n)\n->\nNone\n:\n\"\"\"Closes the session.\"\"\"\npass\ntranscribe_turns\nabstractmethod\ntranscribe_turns\n()\n->\nAsyncIterator\n[\nstr\n]\nYields a stream of text transcriptions. Each transcription is a turn in the conversation.\nThis method is expected to return only after\nclose()\nis called.\nSource code in\nsrc/agents/voice/model.py\n88\n89\n90\n91\n92\n93\n94\n@abc\n.\nabstractmethod\ndef\ntranscribe_turns\n(\nself\n)\n->\nAsyncIterator\n[\nstr\n]:\n\"\"\"Yields a stream of text transcriptions. Each transcription is a turn in the conversation.\nThis method is expected to return only after `close()` is called.\n\"\"\"\npass\nclose\nabstractmethod\nasync\nclose\n()\n->\nNone\nCloses the session.\nSource code in\nsrc/agents/voice/model.py\n96\n97\n98\n99\n@abc\n.\nabstractmethod\nasync\ndef\nclose\n(\nself\n)\n->\nNone\n:\n\"\"\"Closes the session.\"\"\"\npass\nSTTModelSettings\ndataclass\nSettings for a speech-to-text model.\nSource code in\nsrc/agents/voice/model.py\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n@dataclass\nclass\nSTTModelSettings\n:\n\"\"\"Settings for a speech-to-text model.\"\"\"\nprompt\n:\nstr\n|\nNone\n=\nNone\n\"\"\"Instructions for the model to follow.\"\"\"\nlanguage\n:\nstr\n|\nNone\n=\nNone\n\"\"\"The language of the audio input.\"\"\"\ntemperature\n:\nfloat\n|\nNone\n=\nNone\n\"\"\"The temperature of the model.\"\"\"\nturn_detection\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\n\"\"\"The turn detection settings for the model when using streamed audio input.\"\"\"\nprompt\nclass-attribute\ninstance-attribute\nprompt\n:\nstr\n|\nNone\n=\nNone\nInstructions for the model to follow.\nlanguage\nclass-attribute\ninstance-attribute\nlanguage\n:\nstr\n|\nNone\n=\nNone\nThe language of the audio input.\ntemperature\nclass-attribute\ninstance-attribute\ntemperature\n:\nfloat\n|\nNone\n=\nNone\nThe temperature of the model.\nturn_detection\nclass-attribute\ninstance-attribute\nturn_detection\n:\ndict\n[\nstr\n,\nAny\n]\n|\nNone\n=\nNone\nThe turn detection settings for the model when using streamed audio input.\nSTTModel\nBases:\nABC\nA speech-to-text model that can convert audio input into text.\nSource code in\nsrc/agents/voice/model.py\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\nclass\nSTTModel\n(\nabc\n.\nABC\n):\n\"\"\"A speech-to-text model that can convert audio input into text.\"\"\"\n@property\n@abc\n.\nabstractmethod\ndef\nmodel_name\n(\nself\n)\n->\nstr\n:\n\"\"\"The name of the STT model.\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\ntranscribe\n(\nself\n,\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\n:\n\"\"\"Given an audio input, produces a text transcription.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nThe text transcription of the audio input.\n\"\"\"\npass\n@abc\n.\nabstractmethod\nasync\ndef\ncreate_session\n(\nself\n,\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\n:\n\"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\nof text transcriptions.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nA new transcription session.\n\"\"\"\npass\nmodel_name\nabstractmethod\nproperty\nmodel_name\n:\nstr\nThe name of the STT model.\ntranscribe\nabstractmethod\nasync\ntranscribe\n(\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\nGiven an audio input, produces a text transcription.\nParameters:\nName\nType\nDescription\nDefault\ninput\nAudioInput\nThe audio input to transcribe.\nrequired\nsettings\nSTTModelSettings\nThe settings to use for the transcription.\nrequired\ntrace_include_sensitive_data\nbool\nWhether to include sensitive data in traces.\nrequired\ntrace_include_sensitive_audio_data\nbool\nWhether to include sensitive audio data in traces.\nrequired\nReturns:\nType\nDescription\nstr\nThe text transcription of the audio input.\nSource code in\nsrc/agents/voice/model.py\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n@abc\n.\nabstractmethod\nasync\ndef\ntranscribe\n(\nself\n,\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\n:\n\"\"\"Given an audio input, produces a text transcription.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nThe text transcription of the audio input.\n\"\"\"\npass\ncreate_session\nabstractmethod\nasync\ncreate_session\n(\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\nCreates a new transcription session, which you can push audio to, and receive a stream\nof text transcriptions.\nParameters:\nName\nType\nDescription\nDefault\ninput\nStreamedAudioInput\nThe audio input to transcribe.\nrequired\nsettings\nSTTModelSettings\nThe settings to use for the transcription.\nrequired\ntrace_include_sensitive_data\nbool\nWhether to include sensitive data in traces.\nrequired\ntrace_include_sensitive_audio_data\nbool\nWhether to include sensitive audio data in traces.\nrequired\nReturns:\nType\nDescription\nStreamedTranscriptionSession\nA new transcription session.\nSource code in\nsrc/agents/voice/model.py\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n@abc\n.\nabstractmethod\nasync\ndef\ncreate_session\n(\nself\n,\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\n:\n\"\"\"Creates a new transcription session, which you can push audio to, and receive a stream\nof text transcriptions.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nA new transcription session.\n\"\"\"\npass\nVoiceModelProvider\nBases:\nABC\nThe base interface for a voice model provider.\nA model provider is responsible for creating speech-to-text and text-to-speech models, given a\nname.\nSource code in\nsrc/agents/voice/model.py\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\nclass\nVoiceModelProvider\n(\nabc\n.\nABC\n):\n\"\"\"The base interface for a voice model provider.\nA model provider is responsible for creating speech-to-text and text-to-speech models, given a\nname.\n\"\"\"\n@abc\n.\nabstractmethod\ndef\nget_stt_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\n:\n\"\"\"Get a speech-to-text model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe speech-to-text model.\n\"\"\"\npass\n@abc\n.\nabstractmethod\ndef\nget_tts_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\n:\n\"\"\"Get a text-to-speech model by name.\"\"\"\nget_stt_model\nabstractmethod\nget_stt_model\n(\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\nGet a speech-to-text model by name.\nParameters:\nName\nType\nDescription\nDefault\nmodel_name\nstr\n| None\nThe name of the model to get.\nrequired\nReturns:\nType\nDescription\nSTTModel\nThe speech-to-text model.\nSource code in\nsrc/agents/voice/model.py\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n@abc\n.\nabstractmethod\ndef\nget_stt_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\n:\n\"\"\"Get a speech-to-text model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe speech-to-text model.\n\"\"\"\npass\nget_tts_model\nabstractmethod\nget_tts_model\n(\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\nGet a text-to-speech model by name.\nSource code in\nsrc/agents/voice/model.py\n191\n192\n193\n@abc\n.\nabstractmethod\ndef\nget_tts_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\n:\n\"\"\"Get a text-to-speech model by name.\"\"\"",
  "Utils": "Utils\nget_sentence_based_splitter\nget_sentence_based_splitter\n(\nmin_sentence_length\n:\nint\n=\n20\n,\n)\n->\nCallable\n[[\nstr\n],\ntuple\n[\nstr\n,\nstr\n]]\nReturns a function that splits text into chunks based on sentence boundaries.\nParameters:\nName\nType\nDescription\nDefault\nmin_sentence_length\nint\nThe minimum length of a sentence to be included in a chunk.\n20\nReturns:\nType\nDescription\nCallable\n[[\nstr\n],\ntuple\n[\nstr\n,\nstr\n]]\nA function that splits text into chunks based on sentence boundaries.\nSource code in\nsrc/agents/voice/utils.py\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\ndef\nget_sentence_based_splitter\n(\nmin_sentence_length\n:\nint\n=\n20\n,\n)\n->\nCallable\n[[\nstr\n],\ntuple\n[\nstr\n,\nstr\n]]:\n\"\"\"Returns a function that splits text into chunks based on sentence boundaries.\nArgs:\nmin_sentence_length: The minimum length of a sentence to be included in a chunk.\nReturns:\nA function that splits text into chunks based on sentence boundaries.\n\"\"\"\ndef\nsentence_based_text_splitter\n(\ntext_buffer\n:\nstr\n)\n->\ntuple\n[\nstr\n,\nstr\n]:\n\"\"\"\nA function to split the text into chunks. This is useful if you want to split the text into\nchunks before sending it to the TTS model rather than waiting for the whole text to be\nprocessed.\nArgs:\ntext_buffer: The text to split.\nReturns:\nA tuple of the text to process and the remaining text buffer.\n\"\"\"\nsentences\n=\nre\n.\nsplit\n(\nr\n\"(?<=[.!?])\\s+\"\n,\ntext_buffer\n.\nstrip\n())\nif\nlen\n(\nsentences\n)\n>=\n1\n:\ncombined_sentences\n=\n\" \"\n.\njoin\n(\nsentences\n[:\n-\n1\n])\nif\nlen\n(\ncombined_sentences\n)\n>=\nmin_sentence_length\n:\nremaining_text_buffer\n=\nsentences\n[\n-\n1\n]\nreturn\ncombined_sentences\n,\nremaining_text_buffer\nreturn\n\"\"\n,\ntext_buffer\nreturn\nsentence_based_text_splitter",
  "OpenAIVoiceModelProvider": "OpenAIVoiceModelProvider\nOpenAIVoiceModelProvider\nBases:\nVoiceModelProvider\nA voice model provider that uses OpenAI models.\nSource code in\nsrc/agents/voice/models/openai_model_provider.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\nclass\nOpenAIVoiceModelProvider\n(\nVoiceModelProvider\n):\n\"\"\"A voice model provider that uses OpenAI models.\"\"\"\ndef\n__init__\n(\nself\n,\n*\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\nbase_url\n:\nstr\n|\nNone\n=\nNone\n,\nopenai_client\n:\nAsyncOpenAI\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\n:\n\"\"\"Create a new OpenAI voice model provider.\nArgs:\napi_key: The API key to use for the OpenAI client. If not provided, we will use the\ndefault API key.\nbase_url: The base URL to use for the OpenAI client. If not provided, we will use the\ndefault base URL.\nopenai_client: An optional OpenAI client to use. If not provided, we will create a new\nOpenAI client using the api_key and base_url.\norganization: The organization to use for the OpenAI client.\nproject: The project to use for the OpenAI client.\n\"\"\"\nif\nopenai_client\nis\nnot\nNone\n:\nassert\napi_key\nis\nNone\nand\nbase_url\nis\nNone\n,\n(\n\"Don't provide api_key or base_url if you provide openai_client\"\n)\nself\n.\n_client\n:\nAsyncOpenAI\n|\nNone\n=\nopenai_client\nelse\n:\nself\n.\n_client\n=\nNone\nself\n.\n_stored_api_key\n=\napi_key\nself\n.\n_stored_base_url\n=\nbase_url\nself\n.\n_stored_organization\n=\norganization\nself\n.\n_stored_project\n=\nproject\n# We lazy load the client in case you never actually use OpenAIProvider(). Otherwise\n# AsyncOpenAI() raises an error if you don't have an API key set.\ndef\n_get_client\n(\nself\n)\n->\nAsyncOpenAI\n:\nif\nself\n.\n_client\nis\nNone\n:\nself\n.\n_client\n=\n_openai_shared\n.\nget_default_openai_client\n()\nor\nAsyncOpenAI\n(\napi_key\n=\nself\n.\n_stored_api_key\nor\n_openai_shared\n.\nget_default_openai_key\n(),\nbase_url\n=\nself\n.\n_stored_base_url\n,\norganization\n=\nself\n.\n_stored_organization\n,\nproject\n=\nself\n.\n_stored_project\n,\nhttp_client\n=\nshared_http_client\n(),\n)\nreturn\nself\n.\n_client\ndef\nget_stt_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\n:\n\"\"\"Get a speech-to-text model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe speech-to-text model.\n\"\"\"\nreturn\nOpenAISTTModel\n(\nmodel_name\nor\nDEFAULT_STT_MODEL\n,\nself\n.\n_get_client\n())\ndef\nget_tts_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\n:\n\"\"\"Get a text-to-speech model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe text-to-speech model.\n\"\"\"\nreturn\nOpenAITTSModel\n(\nmodel_name\nor\nDEFAULT_TTS_MODEL\n,\nself\n.\n_get_client\n())\n__init__\n__init__\n(\n*\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\nbase_url\n:\nstr\n|\nNone\n=\nNone\n,\nopenai_client\n:\nAsyncOpenAI\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\nCreate a new OpenAI voice model provider.\nParameters:\nName\nType\nDescription\nDefault\napi_key\nstr\n| None\nThe API key to use for the OpenAI client. If not provided, we will use the\ndefault API key.\nNone\nbase_url\nstr\n| None\nThe base URL to use for the OpenAI client. If not provided, we will use the\ndefault base URL.\nNone\nopenai_client\nAsyncOpenAI\n| None\nAn optional OpenAI client to use. If not provided, we will create a new\nOpenAI client using the api_key and base_url.\nNone\norganization\nstr\n| None\nThe organization to use for the OpenAI client.\nNone\nproject\nstr\n| None\nThe project to use for the OpenAI client.\nNone\nSource code in\nsrc/agents/voice/models/openai_model_provider.py\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\ndef\n__init__\n(\nself\n,\n*\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\nbase_url\n:\nstr\n|\nNone\n=\nNone\n,\nopenai_client\n:\nAsyncOpenAI\n|\nNone\n=\nNone\n,\norganization\n:\nstr\n|\nNone\n=\nNone\n,\nproject\n:\nstr\n|\nNone\n=\nNone\n,\n)\n->\nNone\n:\n\"\"\"Create a new OpenAI voice model provider.\nArgs:\napi_key: The API key to use for the OpenAI client. If not provided, we will use the\ndefault API key.\nbase_url: The base URL to use for the OpenAI client. If not provided, we will use the\ndefault base URL.\nopenai_client: An optional OpenAI client to use. If not provided, we will create a new\nOpenAI client using the api_key and base_url.\norganization: The organization to use for the OpenAI client.\nproject: The project to use for the OpenAI client.\n\"\"\"\nif\nopenai_client\nis\nnot\nNone\n:\nassert\napi_key\nis\nNone\nand\nbase_url\nis\nNone\n,\n(\n\"Don't provide api_key or base_url if you provide openai_client\"\n)\nself\n.\n_client\n:\nAsyncOpenAI\n|\nNone\n=\nopenai_client\nelse\n:\nself\n.\n_client\n=\nNone\nself\n.\n_stored_api_key\n=\napi_key\nself\n.\n_stored_base_url\n=\nbase_url\nself\n.\n_stored_organization\n=\norganization\nself\n.\n_stored_project\n=\nproject\nget_stt_model\nget_stt_model\n(\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\nGet a speech-to-text model by name.\nParameters:\nName\nType\nDescription\nDefault\nmodel_name\nstr\n| None\nThe name of the model to get.\nrequired\nReturns:\nType\nDescription\nSTTModel\nThe speech-to-text model.\nSource code in\nsrc/agents/voice/models/openai_model_provider.py\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\ndef\nget_stt_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nSTTModel\n:\n\"\"\"Get a speech-to-text model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe speech-to-text model.\n\"\"\"\nreturn\nOpenAISTTModel\n(\nmodel_name\nor\nDEFAULT_STT_MODEL\n,\nself\n.\n_get_client\n())\nget_tts_model\nget_tts_model\n(\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\nGet a text-to-speech model by name.\nParameters:\nName\nType\nDescription\nDefault\nmodel_name\nstr\n| None\nThe name of the model to get.\nrequired\nReturns:\nType\nDescription\nTTSModel\nThe text-to-speech model.\nSource code in\nsrc/agents/voice/models/openai_model_provider.py\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\ndef\nget_tts_model\n(\nself\n,\nmodel_name\n:\nstr\n|\nNone\n)\n->\nTTSModel\n:\n\"\"\"Get a text-to-speech model by name.\nArgs:\nmodel_name: The name of the model to get.\nReturns:\nThe text-to-speech model.\n\"\"\"\nreturn\nOpenAITTSModel\n(\nmodel_name\nor\nDEFAULT_TTS_MODEL\n,\nself\n.\n_get_client\n())",
  "OpenAI STT": "OpenAI STT\nOpenAISTTTranscriptionSession\nBases:\nStreamedTranscriptionSession\nA transcription session for OpenAI's STT model.\nSource code in\nsrc/agents/voice/models/openai_stt.py\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\nclass\nOpenAISTTTranscriptionSession\n(\nStreamedTranscriptionSession\n):\n\"\"\"A transcription session for OpenAI's STT model.\"\"\"\ndef\n__init__\n(\nself\n,\ninput\n:\nStreamedAudioInput\n,\nclient\n:\nAsyncOpenAI\n,\nmodel\n:\nstr\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n):\nself\n.\nconnected\n:\nbool\n=\nFalse\nself\n.\n_client\n=\nclient\nself\n.\n_model\n=\nmodel\nself\n.\n_settings\n=\nsettings\nself\n.\n_turn_detection\n=\nsettings\n.\nturn_detection\nor\nDEFAULT_TURN_DETECTION\nself\n.\n_trace_include_sensitive_data\n=\ntrace_include_sensitive_data\nself\n.\n_trace_include_sensitive_audio_data\n=\ntrace_include_sensitive_audio_data\nself\n.\n_input_queue\n:\nasyncio\n.\nQueue\n[\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]]\n=\ninput\n.\nqueue\nself\n.\n_output_queue\n:\nasyncio\n.\nQueue\n[\nstr\n|\nErrorSentinel\n|\nSessionCompleteSentinel\n]\n=\n(\nasyncio\n.\nQueue\n()\n)\nself\n.\n_websocket\n:\nwebsockets\n.\nClientConnection\n|\nNone\n=\nNone\nself\n.\n_event_queue\n:\nasyncio\n.\nQueue\n[\ndict\n[\nstr\n,\nAny\n]\n|\nWebsocketDoneSentinel\n]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_state_queue\n:\nasyncio\n.\nQueue\n[\ndict\n[\nstr\n,\nAny\n]]\n=\nasyncio\n.\nQueue\n()\nself\n.\n_turn_audio_buffer\n:\nlist\n[\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]]\n=\n[]\nself\n.\n_tracing_span\n:\nSpan\n[\nTranscriptionSpanData\n]\n|\nNone\n=\nNone\n# tasks\nself\n.\n_listener_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_process_events_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_stream_audio_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_connection_task\n:\nasyncio\n.\nTask\n[\nAny\n]\n|\nNone\n=\nNone\nself\n.\n_stored_exception\n:\nException\n|\nNone\n=\nNone\ndef\n_start_turn\n(\nself\n)\n->\nNone\n:\nself\n.\n_tracing_span\n=\ntranscription_span\n(\nmodel\n=\nself\n.\n_model\n,\nmodel_config\n=\n{\n\"temperature\"\n:\nself\n.\n_settings\n.\ntemperature\n,\n\"language\"\n:\nself\n.\n_settings\n.\nlanguage\n,\n\"prompt\"\n:\nself\n.\n_settings\n.\nprompt\n,\n\"turn_detection\"\n:\nself\n.\n_turn_detection\n,\n},\n)\nself\n.\n_tracing_span\n.\nstart\n()\ndef\n_end_turn\n(\nself\n,\n_transcript\n:\nstr\n)\n->\nNone\n:\nif\nlen\n(\n_transcript\n)\n<\n1\n:\nreturn\nif\nself\n.\n_tracing_span\n:\nif\nself\n.\n_trace_include_sensitive_audio_data\n:\nself\n.\n_tracing_span\n.\nspan_data\n.\ninput\n=\n_audio_to_base64\n(\nself\n.\n_turn_audio_buffer\n)\nself\n.\n_tracing_span\n.\nspan_data\n.\ninput_format\n=\n\"pcm\"\nif\nself\n.\n_trace_include_sensitive_data\n:\nself\n.\n_tracing_span\n.\nspan_data\n.\noutput\n=\n_transcript\nself\n.\n_tracing_span\n.\nfinish\n()\nself\n.\n_turn_audio_buffer\n=\n[]\nself\n.\n_tracing_span\n=\nNone\nasync\ndef\n_event_listener\n(\nself\n)\n->\nNone\n:\nassert\nself\n.\n_websocket\nis\nnot\nNone\n,\n\"Websocket not initialized\"\nasync\nfor\nmessage\nin\nself\n.\n_websocket\n:\ntry\n:\nevent\n=\njson\n.\nloads\n(\nmessage\n)\nif\nevent\n.\nget\n(\n\"type\"\n)\n==\n\"error\"\n:\nraise\nSTTWebsocketConnectionError\n(\nf\n\"Error event:\n{\nevent\n.\nget\n(\n'error'\n)\n}\n\"\n)\nif\nevent\n.\nget\n(\n\"type\"\n)\nin\n[\n\"session.updated\"\n,\n\"transcription_session.updated\"\n,\n\"session.created\"\n,\n\"transcription_session.created\"\n,\n]:\nawait\nself\n.\n_state_queue\n.\nput\n(\nevent\n)\nawait\nself\n.\n_event_queue\n.\nput\n(\nevent\n)\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\nSTTWebsocketConnectionError\n(\n\"Error parsing events\"\n)\nfrom\ne\nawait\nself\n.\n_event_queue\n.\nput\n(\nWebsocketDoneSentinel\n())\nasync\ndef\n_configure_session\n(\nself\n)\n->\nNone\n:\nassert\nself\n.\n_websocket\nis\nnot\nNone\n,\n\"Websocket not initialized\"\nawait\nself\n.\n_websocket\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"type\"\n:\n\"transcription_session.update\"\n,\n\"session\"\n:\n{\n\"input_audio_format\"\n:\n\"pcm16\"\n,\n\"input_audio_transcription\"\n:\n{\n\"model\"\n:\nself\n.\n_model\n},\n\"turn_detection\"\n:\nself\n.\n_turn_detection\n,\n},\n}\n)\n)\nasync\ndef\n_setup_connection\n(\nself\n,\nws\n:\nwebsockets\n.\nClientConnection\n)\n->\nNone\n:\nself\n.\n_websocket\n=\nws\nself\n.\n_listener_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_event_listener\n())\ntry\n:\nevent\n=\nawait\n_wait_for_event\n(\nself\n.\n_state_queue\n,\n[\n\"session.created\"\n,\n\"transcription_session.created\"\n],\nSESSION_CREATION_TIMEOUT\n,\n)\nexcept\nTimeoutError\nas\ne\n:\nwrapped_err\n=\nSTTWebsocketConnectionError\n(\n\"Timeout waiting for transcription_session.created event\"\n)\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\nwrapped_err\n))\nraise\nwrapped_err\nfrom\ne\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\ne\nawait\nself\n.\n_configure_session\n()\ntry\n:\nevent\n=\nawait\n_wait_for_event\n(\nself\n.\n_state_queue\n,\n[\n\"session.updated\"\n,\n\"transcription_session.updated\"\n],\nSESSION_UPDATE_TIMEOUT\n,\n)\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Session updated\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Session updated:\n{\nevent\n}\n\"\n)\nexcept\nTimeoutError\nas\ne\n:\nwrapped_err\n=\nSTTWebsocketConnectionError\n(\n\"Timeout waiting for transcription_session.updated event\"\n)\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\nwrapped_err\n))\nraise\nwrapped_err\nfrom\ne\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\nasync\ndef\n_handle_events\n(\nself\n)\n->\nNone\n:\nwhile\nTrue\n:\ntry\n:\nevent\n=\nawait\nasyncio\n.\nwait_for\n(\nself\n.\n_event_queue\n.\nget\n(),\ntimeout\n=\nEVENT_INACTIVITY_TIMEOUT\n)\nif\nisinstance\n(\nevent\n,\nWebsocketDoneSentinel\n):\n# processed all events and websocket is done\nbreak\nevent_type\n=\nevent\n.\nget\n(\n\"type\"\n,\n\"unknown\"\n)\nif\nevent_type\n==\n\"conversation.item.input_audio_transcription.completed\"\n:\ntranscript\n=\ncast\n(\nstr\n,\nevent\n.\nget\n(\n\"transcript\"\n,\n\"\"\n))\nif\nlen\n(\ntranscript\n)\n>\n0\n:\nself\n.\n_end_turn\n(\ntranscript\n)\nself\n.\n_start_turn\n()\nawait\nself\n.\n_output_queue\n.\nput\n(\ntranscript\n)\nawait\nasyncio\n.\nsleep\n(\n0\n)\n# yield control\nexcept\nasyncio\n.\nTimeoutError\n:\n# No new events for a while. Assume the session is done.\nbreak\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\ne\nawait\nself\n.\n_output_queue\n.\nput\n(\nSessionCompleteSentinel\n())\nasync\ndef\n_stream_audio\n(\nself\n,\naudio_queue\n:\nasyncio\n.\nQueue\n[\nnpt\n.\nNDArray\n[\nnp\n.\nint16\n|\nnp\n.\nfloat32\n]]\n)\n->\nNone\n:\nassert\nself\n.\n_websocket\nis\nnot\nNone\n,\n\"Websocket not initialized\"\nself\n.\n_start_turn\n()\nwhile\nTrue\n:\nbuffer\n=\nawait\naudio_queue\n.\nget\n()\nif\nbuffer\nis\nNone\n:\nbreak\nself\n.\n_turn_audio_buffer\n.\nappend\n(\nbuffer\n)\ntry\n:\nawait\nself\n.\n_websocket\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"type\"\n:\n\"input_audio_buffer.append\"\n,\n\"audio\"\n:\nbase64\n.\nb64encode\n(\nbuffer\n.\ntobytes\n())\n.\ndecode\n(\n\"utf-8\"\n),\n}\n)\n)\nexcept\nwebsockets\n.\nConnectionClosed\n:\nbreak\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\ne\nawait\nasyncio\n.\nsleep\n(\n0\n)\n# yield control\nasync\ndef\n_process_websocket_connection\n(\nself\n)\n->\nNone\n:\ntry\n:\nasync\nwith\nwebsockets\n.\nconnect\n(\n\"wss://api.openai.com/v1/realtime?intent=transcription\"\n,\nadditional_headers\n=\n{\n\"Authorization\"\n:\nf\n\"Bearer\n{\nself\n.\n_client\n.\napi_key\n}\n\"\n,\n\"OpenAI-Beta\"\n:\n\"realtime=v1\"\n,\n\"OpenAI-Log-Session\"\n:\n\"1\"\n,\n},\n)\nas\nws\n:\nawait\nself\n.\n_setup_connection\n(\nws\n)\nself\n.\n_process_events_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_handle_events\n())\nself\n.\n_stream_audio_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_stream_audio\n(\nself\n.\n_input_queue\n))\nself\n.\nconnected\n=\nTrue\nif\nself\n.\n_listener_task\n:\nawait\nself\n.\n_listener_task\nelse\n:\nlogger\n.\nerror\n(\n\"Listener task not initialized\"\n)\nraise\nAgentsException\n(\n\"Listener task not initialized\"\n)\nexcept\nException\nas\ne\n:\nawait\nself\n.\n_output_queue\n.\nput\n(\nErrorSentinel\n(\ne\n))\nraise\ne\ndef\n_check_errors\n(\nself\n)\n->\nNone\n:\nif\nself\n.\n_connection_task\nand\nself\n.\n_connection_task\n.\ndone\n():\nexc\n=\nself\n.\n_connection_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\nif\nself\n.\n_process_events_task\nand\nself\n.\n_process_events_task\n.\ndone\n():\nexc\n=\nself\n.\n_process_events_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\nif\nself\n.\n_stream_audio_task\nand\nself\n.\n_stream_audio_task\n.\ndone\n():\nexc\n=\nself\n.\n_stream_audio_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\nif\nself\n.\n_listener_task\nand\nself\n.\n_listener_task\n.\ndone\n():\nexc\n=\nself\n.\n_listener_task\n.\nexception\n()\nif\nexc\nand\nisinstance\n(\nexc\n,\nException\n):\nself\n.\n_stored_exception\n=\nexc\ndef\n_cleanup_tasks\n(\nself\n)\n->\nNone\n:\nif\nself\n.\n_listener_task\nand\nnot\nself\n.\n_listener_task\n.\ndone\n():\nself\n.\n_listener_task\n.\ncancel\n()\nif\nself\n.\n_process_events_task\nand\nnot\nself\n.\n_process_events_task\n.\ndone\n():\nself\n.\n_process_events_task\n.\ncancel\n()\nif\nself\n.\n_stream_audio_task\nand\nnot\nself\n.\n_stream_audio_task\n.\ndone\n():\nself\n.\n_stream_audio_task\n.\ncancel\n()\nif\nself\n.\n_connection_task\nand\nnot\nself\n.\n_connection_task\n.\ndone\n():\nself\n.\n_connection_task\n.\ncancel\n()\nasync\ndef\ntranscribe_turns\n(\nself\n)\n->\nAsyncIterator\n[\nstr\n]:\nself\n.\n_connection_task\n=\nasyncio\n.\ncreate_task\n(\nself\n.\n_process_websocket_connection\n())\nwhile\nTrue\n:\ntry\n:\nturn\n=\nawait\nself\n.\n_output_queue\n.\nget\n()\nexcept\nasyncio\n.\nCancelledError\n:\nbreak\nif\n(\nturn\nis\nNone\nor\nisinstance\n(\nturn\n,\nErrorSentinel\n)\nor\nisinstance\n(\nturn\n,\nSessionCompleteSentinel\n)\n):\nself\n.\n_output_queue\n.\ntask_done\n()\nbreak\nyield\nturn\nself\n.\n_output_queue\n.\ntask_done\n()\nif\nself\n.\n_tracing_span\n:\nself\n.\n_end_turn\n(\n\"\"\n)\nif\nself\n.\n_websocket\n:\nawait\nself\n.\n_websocket\n.\nclose\n()\nself\n.\n_check_errors\n()\nif\nself\n.\n_stored_exception\n:\nraise\nself\n.\n_stored_exception\nasync\ndef\nclose\n(\nself\n)\n->\nNone\n:\nif\nself\n.\n_websocket\n:\nawait\nself\n.\n_websocket\n.\nclose\n()\nself\n.\n_cleanup_tasks\n()\nOpenAISTTModel\nBases:\nSTTModel\nA speech-to-text model for OpenAI.\nSource code in\nsrc/agents/voice/models/openai_stt.py\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\nclass\nOpenAISTTModel\n(\nSTTModel\n):\n\"\"\"A speech-to-text model for OpenAI.\"\"\"\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n,\n):\n\"\"\"Create a new OpenAI speech-to-text model.\nArgs:\nmodel: The name of the model to use.\nopenai_client: The OpenAI client to use.\n\"\"\"\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\n@property\ndef\nmodel_name\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\nmodel\ndef\n_non_null_or_not_given\n(\nself\n,\nvalue\n:\nAny\n)\n->\nAny\n:\nreturn\nvalue\nif\nvalue\nis\nnot\nNone\nelse\nNone\n# NOT_GIVEN\nasync\ndef\ntranscribe\n(\nself\n,\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\n:\n\"\"\"Transcribe an audio input.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\nReturns:\nThe transcribed text.\n\"\"\"\nwith\ntranscription_span\n(\nmodel\n=\nself\n.\nmodel\n,\ninput\n=\ninput\n.\nto_base64\n()\nif\ntrace_include_sensitive_audio_data\nelse\n\"\"\n,\ninput_format\n=\n\"pcm\"\n,\nmodel_config\n=\n{\n\"temperature\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\ntemperature\n),\n\"language\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nlanguage\n),\n\"prompt\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nprompt\n),\n},\n)\nas\nspan\n:\ntry\n:\nresponse\n=\nawait\nself\n.\n_client\n.\naudio\n.\ntranscriptions\n.\ncreate\n(\nmodel\n=\nself\n.\nmodel\n,\nfile\n=\ninput\n.\nto_audio_file\n(),\nprompt\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nprompt\n),\nlanguage\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nlanguage\n),\ntemperature\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\ntemperature\n),\n)\nif\ntrace_include_sensitive_data\n:\nspan\n.\nspan_data\n.\noutput\n=\nresponse\n.\ntext\nreturn\nresponse\n.\ntext\nexcept\nException\nas\ne\n:\nspan\n.\nspan_data\n.\noutput\n=\n\"\"\nspan\n.\nset_error\n(\nSpanError\n(\nmessage\n=\nstr\n(\ne\n),\ndata\n=\n{}))\nraise\ne\nasync\ndef\ncreate_session\n(\nself\n,\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\n:\n\"\"\"Create a new transcription session.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nA new transcription session.\n\"\"\"\nreturn\nOpenAISTTTranscriptionSession\n(\ninput\n,\nself\n.\n_client\n,\nself\n.\nmodel\n,\nsettings\n,\ntrace_include_sensitive_data\n,\ntrace_include_sensitive_audio_data\n,\n)\n__init__\n__init__\n(\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n)\nCreate a new OpenAI speech-to-text model.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\nThe name of the model to use.\nrequired\nopenai_client\nAsyncOpenAI\nThe OpenAI client to use.\nrequired\nSource code in\nsrc/agents/voice/models/openai_stt.py\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n,\n):\n\"\"\"Create a new OpenAI speech-to-text model.\nArgs:\nmodel: The name of the model to use.\nopenai_client: The OpenAI client to use.\n\"\"\"\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\ntranscribe\nasync\ntranscribe\n(\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\nTranscribe an audio input.\nParameters:\nName\nType\nDescription\nDefault\ninput\nAudioInput\nThe audio input to transcribe.\nrequired\nsettings\nSTTModelSettings\nThe settings to use for the transcription.\nrequired\nReturns:\nType\nDescription\nstr\nThe transcribed text.\nSource code in\nsrc/agents/voice/models/openai_stt.py\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\nasync\ndef\ntranscribe\n(\nself\n,\ninput\n:\nAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nstr\n:\n\"\"\"Transcribe an audio input.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\nReturns:\nThe transcribed text.\n\"\"\"\nwith\ntranscription_span\n(\nmodel\n=\nself\n.\nmodel\n,\ninput\n=\ninput\n.\nto_base64\n()\nif\ntrace_include_sensitive_audio_data\nelse\n\"\"\n,\ninput_format\n=\n\"pcm\"\n,\nmodel_config\n=\n{\n\"temperature\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\ntemperature\n),\n\"language\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nlanguage\n),\n\"prompt\"\n:\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nprompt\n),\n},\n)\nas\nspan\n:\ntry\n:\nresponse\n=\nawait\nself\n.\n_client\n.\naudio\n.\ntranscriptions\n.\ncreate\n(\nmodel\n=\nself\n.\nmodel\n,\nfile\n=\ninput\n.\nto_audio_file\n(),\nprompt\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nprompt\n),\nlanguage\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\nlanguage\n),\ntemperature\n=\nself\n.\n_non_null_or_not_given\n(\nsettings\n.\ntemperature\n),\n)\nif\ntrace_include_sensitive_data\n:\nspan\n.\nspan_data\n.\noutput\n=\nresponse\n.\ntext\nreturn\nresponse\n.\ntext\nexcept\nException\nas\ne\n:\nspan\n.\nspan_data\n.\noutput\n=\n\"\"\nspan\n.\nset_error\n(\nSpanError\n(\nmessage\n=\nstr\n(\ne\n),\ndata\n=\n{}))\nraise\ne\ncreate_session\nasync\ncreate_session\n(\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\nCreate a new transcription session.\nParameters:\nName\nType\nDescription\nDefault\ninput\nStreamedAudioInput\nThe audio input to transcribe.\nrequired\nsettings\nSTTModelSettings\nThe settings to use for the transcription.\nrequired\ntrace_include_sensitive_data\nbool\nWhether to include sensitive data in traces.\nrequired\ntrace_include_sensitive_audio_data\nbool\nWhether to include sensitive audio data in traces.\nrequired\nReturns:\nType\nDescription\nStreamedTranscriptionSession\nA new transcription session.\nSource code in\nsrc/agents/voice/models/openai_stt.py\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\nasync\ndef\ncreate_session\n(\nself\n,\ninput\n:\nStreamedAudioInput\n,\nsettings\n:\nSTTModelSettings\n,\ntrace_include_sensitive_data\n:\nbool\n,\ntrace_include_sensitive_audio_data\n:\nbool\n,\n)\n->\nStreamedTranscriptionSession\n:\n\"\"\"Create a new transcription session.\nArgs:\ninput: The audio input to transcribe.\nsettings: The settings to use for the transcription.\ntrace_include_sensitive_data: Whether to include sensitive data in traces.\ntrace_include_sensitive_audio_data: Whether to include sensitive audio data in traces.\nReturns:\nA new transcription session.\n\"\"\"\nreturn\nOpenAISTTTranscriptionSession\n(\ninput\n,\nself\n.\n_client\n,\nself\n.\nmodel\n,\nsettings\n,\ntrace_include_sensitive_data\n,\ntrace_include_sensitive_audio_data\n,\n)",
  "OpenAI TTS": "OpenAI TTS\nOpenAITTSModel\nBases:\nTTSModel\nA text-to-speech model for OpenAI.\nSource code in\nsrc/agents/voice/models/openai_tts.py\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\nclass\nOpenAITTSModel\n(\nTTSModel\n):\n\"\"\"A text-to-speech model for OpenAI.\"\"\"\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n,\n):\n\"\"\"Create a new OpenAI text-to-speech model.\nArgs:\nmodel: The name of the model to use.\nopenai_client: The OpenAI client to use.\n\"\"\"\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\n@property\ndef\nmodel_name\n(\nself\n)\n->\nstr\n:\nreturn\nself\n.\nmodel\nasync\ndef\nrun\n(\nself\n,\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]:\n\"\"\"Run the text-to-speech model.\nArgs:\ntext: The text to convert to speech.\nsettings: The settings to use for the text-to-speech model.\nReturns:\nAn iterator of audio chunks.\n\"\"\"\nresponse\n=\nself\n.\n_client\n.\naudio\n.\nspeech\n.\nwith_streaming_response\n.\ncreate\n(\nmodel\n=\nself\n.\nmodel\n,\nvoice\n=\nsettings\n.\nvoice\nor\nDEFAULT_VOICE\n,\ninput\n=\ntext\n,\nresponse_format\n=\n\"pcm\"\n,\nextra_body\n=\n{\n\"instructions\"\n:\nsettings\n.\ninstructions\n,\n},\n)\nasync\nwith\nresponse\nas\nstream\n:\nasync\nfor\nchunk\nin\nstream\n.\niter_bytes\n(\nchunk_size\n=\n1024\n):\nyield\nchunk\n__init__\n__init__\n(\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n)\nCreate a new OpenAI text-to-speech model.\nParameters:\nName\nType\nDescription\nDefault\nmodel\nstr\nThe name of the model to use.\nrequired\nopenai_client\nAsyncOpenAI\nThe OpenAI client to use.\nrequired\nSource code in\nsrc/agents/voice/models/openai_tts.py\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n,\nopenai_client\n:\nAsyncOpenAI\n,\n):\n\"\"\"Create a new OpenAI text-to-speech model.\nArgs:\nmodel: The name of the model to use.\nopenai_client: The OpenAI client to use.\n\"\"\"\nself\n.\nmodel\n=\nmodel\nself\n.\n_client\n=\nopenai_client\nrun\nasync\nrun\n(\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]\nRun the text-to-speech model.\nParameters:\nName\nType\nDescription\nDefault\ntext\nstr\nThe text to convert to speech.\nrequired\nsettings\nTTSModelSettings\nThe settings to use for the text-to-speech model.\nrequired\nReturns:\nType\nDescription\nAsyncIterator\n[\nbytes\n]\nAn iterator of audio chunks.\nSource code in\nsrc/agents/voice/models/openai_tts.py\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\nasync\ndef\nrun\n(\nself\n,\ntext\n:\nstr\n,\nsettings\n:\nTTSModelSettings\n)\n->\nAsyncIterator\n[\nbytes\n]:\n\"\"\"Run the text-to-speech model.\nArgs:\ntext: The text to convert to speech.\nsettings: The settings to use for the text-to-speech model.\nReturns:\nAn iterator of audio chunks.\n\"\"\"\nresponse\n=\nself\n.\n_client\n.\naudio\n.\nspeech\n.\nwith_streaming_response\n.\ncreate\n(\nmodel\n=\nself\n.\nmodel\n,\nvoice\n=\nsettings\n.\nvoice\nor\nDEFAULT_VOICE\n,\ninput\n=\ntext\n,\nresponse_format\n=\n\"pcm\"\n,\nextra_body\n=\n{\n\"instructions\"\n:\nsettings\n.\ninstructions\n,\n},\n)\nasync\nwith\nresponse\nas\nstream\n:\nasync\nfor\nchunk\nin\nstream\n.\niter_bytes\n(\nchunk_size\n=\n1024\n):\nyield\nchunk",
  "Handoff filters": "Handoff filters\nremove_all_tools\nremove_all_tools\n(\nhandoff_input_data\n:\nHandoffInputData\n,\n)\n->\nHandoffInputData\nFilters out all tool items: file search, web search and function calls+output.\nSource code in\nsrc/agents/extensions/handoff_filters.py\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\ndef\nremove_all_tools\n(\nhandoff_input_data\n:\nHandoffInputData\n)\n->\nHandoffInputData\n:\n\"\"\"Filters out all tool items: file search, web search and function calls+output.\"\"\"\nhistory\n=\nhandoff_input_data\n.\ninput_history\nnew_items\n=\nhandoff_input_data\n.\nnew_items\nfiltered_history\n=\n(\n_remove_tool_types_from_input\n(\nhistory\n)\nif\nisinstance\n(\nhistory\n,\ntuple\n)\nelse\nhistory\n)\nfiltered_pre_handoff_items\n=\n_remove_tools_from_items\n(\nhandoff_input_data\n.\npre_handoff_items\n)\nfiltered_new_items\n=\n_remove_tools_from_items\n(\nnew_items\n)\nreturn\nHandoffInputData\n(\ninput_history\n=\nfiltered_history\n,\npre_handoff_items\n=\nfiltered_pre_handoff_items\n,\nnew_items\n=\nfiltered_new_items\n,\n)",
  "Handoff prompt": "Handoff prompt\nRECOMMENDED_PROMPT_PREFIX\nmodule-attribute\nRECOMMENDED_PROMPT_PREFIX\n=\n\"# System context\n\\n\nYou are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n\\n\n\"\nprompt_with_handoff_instructions\nprompt_with_handoff_instructions\n(\nprompt\n:\nstr\n)\n->\nstr\nAdd recommended instructions to the prompt for agents that use handoffs.\nSource code in\nsrc/agents/extensions/handoff_prompt.py\n15\n16\n17\n18\n19\ndef\nprompt_with_handoff_instructions\n(\nprompt\n:\nstr\n)\n->\nstr\n:\n\"\"\"\nAdd recommended instructions to the prompt for agents that use handoffs.\n\"\"\"\nreturn\nf\n\"\n{\nRECOMMENDED_PROMPT_PREFIX\n}\n\\n\\n\n{\nprompt\n}\n\"",
  "LiteLLM Models": "LiteLLM Models\nLitellmModel\nBases:\nModel\nThis class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\nAnthropic, Gemini, Mistral, and many other models.\nSee supported models here:\nlitellm models\n.\nSource code in\nsrc/agents/extensions/models/litellm_model.py\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\nclass\nLitellmModel\n(\nModel\n):\n\"\"\"This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\nAnthropic, Gemini, Mistral, and many other models.\nSee supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n\"\"\"\ndef\n__init__\n(\nself\n,\nmodel\n:\nstr\n,\nbase_url\n:\nstr\n|\nNone\n=\nNone\n,\napi_key\n:\nstr\n|\nNone\n=\nNone\n,\n):\nself\n.\nmodel\n=\nmodel\nself\n.\nbase_url\n=\nbase_url\nself\n.\napi_key\n=\napi_key\nasync\ndef\nget_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nModelResponse\n:\nwith\ngeneration_span\n(\nmodel\n=\nstr\n(\nself\n.\nmodel\n),\nmodel_config\n=\ndataclasses\n.\nasdict\n(\nmodel_settings\n)\n|\n{\n\"base_url\"\n:\nstr\n(\nself\n.\nbase_url\nor\n\"\"\n),\n\"model_impl\"\n:\n\"litellm\"\n},\ndisabled\n=\ntracing\n.\nis_disabled\n(),\n)\nas\nspan_generation\n:\nresponse\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nspan_generation\n,\ntracing\n,\nstream\n=\nFalse\n,\n)\nassert\nisinstance\n(\nresponse\n.\nchoices\n[\n0\n],\nlitellm\n.\ntypes\n.\nutils\n.\nChoices\n)\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Received model response\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"LLM resp:\n\\n\n{\njson\n.\ndumps\n(\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\nmodel_dump\n(),\nindent\n=\n2\n)\n}\n\\n\n\"\n)\nif\nhasattr\n(\nresponse\n,\n\"usage\"\n):\nresponse_usage\n=\nresponse\n.\nusage\nusage\n=\n(\nUsage\n(\nrequests\n=\n1\n,\ninput_tokens\n=\nresponse_usage\n.\nprompt_tokens\n,\noutput_tokens\n=\nresponse_usage\n.\ncompletion_tokens\n,\ntotal_tokens\n=\nresponse_usage\n.\ntotal_tokens\n,\n)\nif\nresponse\n.\nusage\nelse\nUsage\n()\n)\nelse\n:\nusage\n=\nUsage\n()\nlogger\n.\nwarning\n(\n\"No usage information returned from Litellm\"\n)\nif\ntracing\n.\ninclude_data\n():\nspan_generation\n.\nspan_data\n.\noutput\n=\n[\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\nmodel_dump\n()]\nspan_generation\n.\nspan_data\n.\nusage\n=\n{\n\"input_tokens\"\n:\nusage\n.\ninput_tokens\n,\n\"output_tokens\"\n:\nusage\n.\noutput_tokens\n,\n}\nitems\n=\nConverter\n.\nmessage_to_output_items\n(\nLitellmConverter\n.\nconvert_message_to_openai\n(\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n)\n)\nreturn\nModelResponse\n(\noutput\n=\nitems\n,\nusage\n=\nusage\n,\nresponse_id\n=\nNone\n,\n)\nasync\ndef\nstream_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\ntracing\n:\nModelTracing\n,\n*\n,\nprevious_response_id\n:\nstr\n|\nNone\n,\n)\n->\nAsyncIterator\n[\nTResponseStreamEvent\n]:\nwith\ngeneration_span\n(\nmodel\n=\nstr\n(\nself\n.\nmodel\n),\nmodel_config\n=\ndataclasses\n.\nasdict\n(\nmodel_settings\n)\n|\n{\n\"base_url\"\n:\nstr\n(\nself\n.\nbase_url\nor\n\"\"\n),\n\"model_impl\"\n:\n\"litellm\"\n},\ndisabled\n=\ntracing\n.\nis_disabled\n(),\n)\nas\nspan_generation\n:\nresponse\n,\nstream\n=\nawait\nself\n.\n_fetch_response\n(\nsystem_instructions\n,\ninput\n,\nmodel_settings\n,\ntools\n,\noutput_schema\n,\nhandoffs\n,\nspan_generation\n,\ntracing\n,\nstream\n=\nTrue\n,\n)\nfinal_response\n:\nResponse\n|\nNone\n=\nNone\nasync\nfor\nchunk\nin\nChatCmplStreamHandler\n.\nhandle_stream\n(\nresponse\n,\nstream\n):\nyield\nchunk\nif\nchunk\n.\ntype\n==\n\"response.completed\"\n:\nfinal_response\n=\nchunk\n.\nresponse\nif\ntracing\n.\ninclude_data\n()\nand\nfinal_response\n:\nspan_generation\n.\nspan_data\n.\noutput\n=\n[\nfinal_response\n.\nmodel_dump\n()]\nif\nfinal_response\nand\nfinal_response\n.\nusage\n:\nspan_generation\n.\nspan_data\n.\nusage\n=\n{\n\"input_tokens\"\n:\nfinal_response\n.\nusage\n.\ninput_tokens\n,\n\"output_tokens\"\n:\nfinal_response\n.\nusage\n.\noutput_tokens\n,\n}\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nLiteral\n[\nTrue\n],\n)\n->\ntuple\n[\nResponse\n,\nAsyncStream\n[\nChatCompletionChunk\n]]:\n...\n@overload\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nLiteral\n[\nFalse\n],\n)\n->\nlitellm\n.\ntypes\n.\nutils\n.\nModelResponse\n:\n...\nasync\ndef\n_fetch_response\n(\nself\n,\nsystem_instructions\n:\nstr\n|\nNone\n,\ninput\n:\nstr\n|\nlist\n[\nTResponseInputItem\n],\nmodel_settings\n:\nModelSettings\n,\ntools\n:\nlist\n[\nTool\n],\noutput_schema\n:\nAgentOutputSchema\n|\nNone\n,\nhandoffs\n:\nlist\n[\nHandoff\n],\nspan\n:\nSpan\n[\nGenerationSpanData\n],\ntracing\n:\nModelTracing\n,\nstream\n:\nbool\n=\nFalse\n,\n)\n->\nlitellm\n.\ntypes\n.\nutils\n.\nModelResponse\n|\ntuple\n[\nResponse\n,\nAsyncStream\n[\nChatCompletionChunk\n]]:\nconverted_messages\n=\nConverter\n.\nitems_to_messages\n(\ninput\n)\nif\nsystem_instructions\n:\nconverted_messages\n.\ninsert\n(\n0\n,\n{\n\"content\"\n:\nsystem_instructions\n,\n\"role\"\n:\n\"system\"\n,\n},\n)\nif\ntracing\n.\ninclude_data\n():\nspan\n.\nspan_data\n.\ninput\n=\nconverted_messages\nparallel_tool_calls\n=\n(\nTrue\nif\nmodel_settings\n.\nparallel_tool_calls\nand\ntools\nand\nlen\n(\ntools\n)\n>\n0\nelse\nFalse\nif\nmodel_settings\n.\nparallel_tool_calls\nis\nFalse\nelse\nNone\n)\ntool_choice\n=\nConverter\n.\nconvert_tool_choice\n(\nmodel_settings\n.\ntool_choice\n)\nresponse_format\n=\nConverter\n.\nconvert_response_format\n(\noutput_schema\n)\nconverted_tools\n=\n[\nConverter\n.\ntool_to_openai\n(\ntool\n)\nfor\ntool\nin\ntools\n]\nif\ntools\nelse\n[]\nfor\nhandoff\nin\nhandoffs\n:\nconverted_tools\n.\nappend\n(\nConverter\n.\nconvert_handoff_tool\n(\nhandoff\n))\nif\n_debug\n.\nDONT_LOG_MODEL_DATA\n:\nlogger\n.\ndebug\n(\n\"Calling LLM\"\n)\nelse\n:\nlogger\n.\ndebug\n(\nf\n\"Calling Litellm model:\n{\nself\n.\nmodel\n}\n\\n\n\"\nf\n\"\n{\njson\n.\ndumps\n(\nconverted_messages\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Tools:\n\\n\n{\njson\n.\ndumps\n(\nconverted_tools\n,\nindent\n=\n2\n)\n}\n\\n\n\"\nf\n\"Stream:\n{\nstream\n}\n\\n\n\"\nf\n\"Tool choice:\n{\ntool_choice\n}\n\\n\n\"\nf\n\"Response format:\n{\nresponse_format\n}\n\\n\n\"\n)\nreasoning_effort\n=\nmodel_settings\n.\nreasoning\n.\neffort\nif\nmodel_settings\n.\nreasoning\nelse\nNone\nstream_options\n=\nNone\nif\nstream\nand\nmodel_settings\n.\ninclude_usage\nis\nnot\nNone\n:\nstream_options\n=\n{\n\"include_usage\"\n:\nmodel_settings\n.\ninclude_usage\n}\nextra_kwargs\n=\n{}\nif\nmodel_settings\n.\nextra_query\n:\nextra_kwargs\n[\n\"extra_query\"\n]\n=\nmodel_settings\n.\nextra_query\nif\nmodel_settings\n.\nmetadata\n:\nextra_kwargs\n[\n\"metadata\"\n]\n=\nmodel_settings\n.\nmetadata\nret\n=\nawait\nlitellm\n.\nacompletion\n(\nmodel\n=\nself\n.\nmodel\n,\nmessages\n=\nconverted_messages\n,\ntools\n=\nconverted_tools\nor\nNone\n,\ntemperature\n=\nmodel_settings\n.\ntemperature\n,\ntop_p\n=\nmodel_settings\n.\ntop_p\n,\nfrequency_penalty\n=\nmodel_settings\n.\nfrequency_penalty\n,\npresence_penalty\n=\nmodel_settings\n.\npresence_penalty\n,\nmax_tokens\n=\nmodel_settings\n.\nmax_tokens\n,\ntool_choice\n=\nself\n.\n_remove_not_given\n(\ntool_choice\n),\nresponse_format\n=\nself\n.\n_remove_not_given\n(\nresponse_format\n),\nparallel_tool_calls\n=\nparallel_tool_calls\n,\nstream\n=\nstream\n,\nstream_options\n=\nstream_options\n,\nreasoning_effort\n=\nreasoning_effort\n,\nextra_headers\n=\nHEADERS\n,\napi_key\n=\nself\n.\napi_key\n,\nbase_url\n=\nself\n.\nbase_url\n,\n**\nextra_kwargs\n,\n)\nif\nisinstance\n(\nret\n,\nlitellm\n.\ntypes\n.\nutils\n.\nModelResponse\n):\nreturn\nret\nresponse\n=\nResponse\n(\nid\n=\nFAKE_RESPONSES_ID\n,\ncreated_at\n=\ntime\n.\ntime\n(),\nmodel\n=\nself\n.\nmodel\n,\nobject\n=\n\"response\"\n,\noutput\n=\n[],\ntool_choice\n=\ncast\n(\nLiteral\n[\n\"auto\"\n,\n\"required\"\n,\n\"none\"\n],\ntool_choice\n)\nif\ntool_choice\n!=\nNOT_GIVEN\nelse\n\"auto\"\n,\ntop_p\n=\nmodel_settings\n.\ntop_p\n,\ntemperature\n=\nmodel_settings\n.\ntemperature\n,\ntools\n=\n[],\nparallel_tool_calls\n=\nparallel_tool_calls\nor\nFalse\n,\nreasoning\n=\nmodel_settings\n.\nreasoning\n,\n)\nreturn\nresponse\n,\nret\ndef\n_remove_not_given\n(\nself\n,\nvalue\n:\nAny\n)\n->\nAny\n:\nif\nisinstance\n(\nvalue\n,\nNotGiven\n):\nreturn\nNone\nreturn\nvalue"
}